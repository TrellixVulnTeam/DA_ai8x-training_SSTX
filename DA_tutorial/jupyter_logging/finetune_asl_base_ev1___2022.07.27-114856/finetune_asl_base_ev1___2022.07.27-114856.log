2022-07-27 11:48:56,375 - Log file for this run: /home/geffencooper/Model_Development/ai8x-training/DA_tutorial/jupyter_logging/finetune_asl_base_ev1___2022.07.27-114856/finetune_asl_base_ev1___2022.07.27-114856.log
2022-07-27 11:48:56,376 - Number of CPUs: 16
2022-07-27 11:48:56,376 - Number of GPUs: 1
2022-07-27 11:48:56,376 - CUDA version: 11.1
2022-07-27 11:48:56,376 - CUDNN version: 8005
2022-07-27 11:48:56,376 - Kernel: 5.4.0-113-generic
2022-07-27 11:48:56,376 - Python: 3.8.11 (default, Jun 14 2022, 10:01:20) 
[GCC 9.4.0]
2022-07-27 11:48:56,376 - pip freeze: {'absl-py': '1.1.0', 'appdirs': '1.4.4', 'argon2-cffi': '21.3.0', 'argon2-cffi-bindings': '21.2.0', 'asttokens': '2.0.5', 'atomicwrites': '1.4.0', 'attrs': '21.4.0', 'audioread': '2.1.9', 'backcall': '0.2.0', 'beautifulsoup4': '4.11.1', 'bleach': '5.0.0', 'bqplot': '0.11.5', 'cachetools': '5.2.0', 'certifi': '2022.5.18.1', 'cffi': '1.15.0', 'charset-normalizer': '2.0.12', 'cloudpickle': '2.1.0', 'cycler': '0.11.0', 'debugpy': '1.6.0', 'decorator': '5.1.1', 'defusedxml': '0.7.1', 'distiller': '0.4.0rc0', 'entrypoints': '0.4', 'executing': '0.8.3', 'fastjsonschema': '2.15.3', 'fonttools': '4.33.3', 'google-auth': '2.7.0', 'google-auth-oauthlib': '0.4.6', 'graphviz': '0.10.1', 'grpcio': '1.46.3', 'gym': '0.12.5', 'h5py': '3.7.0', 'idna': '3.3', 'importlib-metadata': '4.11.4', 'importlib-resources': '5.7.1', 'ipykernel': '6.15.0', 'ipython': '8.4.0', 'ipython-genutils': '0.2.0', 'ipywidgets': '7.4.2', 'jedi': '0.18.1', 'jinja2': '3.1.2', 'joblib': '1.1.0', 'jsonpatch': '1.32', 'jsonpointer': '2.3', 'jsonschema': '4.6.0', 'jupyter': '1.0.0', 'jupyter-client': '7.3.4', 'jupyter-console': '6.4.3', 'jupyter-core': '4.10.0', 'jupyterlab-pygments': '0.2.2', 'kaggle': '1.5.12', 'kiwisolver': '1.4.3', 'librosa': '0.9.1', 'llvmlite': '0.32.1', 'markdown': '3.3.7', 'markupsafe': '2.1.1', 'matplotlib': '3.5.2', 'matplotlib-inline': '0.1.3', 'mistune': '0.8.4', 'more-itertools': '8.13.0', 'munch': '2.5.0', 'nbclient': '0.6.4', 'nbconvert': '6.5.0', 'nbformat': '5.4.0', 'nest-asyncio': '1.5.5', 'notebook': '6.4.12', 'numba': '0.49.1', 'numpy': '1.22.4', 'oauthlib': '3.2.0', 'opencv-python': '4.6.0.66', 'packaging': '21.3', 'pandas': '1.4.2', 'pandocfilters': '1.5.0', 'parso': '0.8.3', 'pexpect': '4.8.0', 'pickleshare': '0.7.5', 'pillow': '9.1.1', 'pip': '22.1.2', 'pluggy': '0.13.1', 'pooch': '1.6.0', 'pretrainedmodels': '0.7.4', 'prometheus-client': '0.14.1', 'prompt-toolkit': '3.0.29', 'protobuf': '3.20.1', 'psutil': '5.9.1', 'ptyprocess': '0.7.0', 'pure-eval': '0.2.2', 'py': '1.11.0', 'pyasn1': '0.4.8', 'pyasn1-modules': '0.2.8', 'pycparser': '2.21', 'pydot': '1.4.1', 'pyglet': '1.5.26', 'pygments': '2.12.0', 'pyparsing': '3.0.9', 'pyrsistent': '0.18.1', 'pytest': '4.6.11', 'python-dateutil': '2.8.2', 'python-slugify': '6.1.2', 'pytsmod': '0.3.5', 'pytz': '2022.1', 'pyyaml': '6.0', 'pyzmq': '23.1.0', 'qgrid': '1.1.1', 'qtconsole': '5.3.1', 'qtpy': '2.1.0', 'requests': '2.28.0', 'requests-oauthlib': '1.3.1', 'resampy': '0.2.2', 'rsa': '4.8', 'scikit-learn': '0.23.2', 'scipy': '1.8.1', 'send2trash': '1.8.0', 'setuptools': '62.4.0', 'shap': '0.40.0', 'six': '1.16.0', 'slicer': '0.0.7', 'soundfile': '0.10.3.post1', 'soupsieve': '2.3.2.post1', 'stack-data': '0.2.0', 'tabulate': '0.8.3', 'tensorboard': '2.9.0', 'tensorboard-data-server': '0.6.1', 'tensorboard-plugin-wit': '1.8.1', 'terminado': '0.15.0', 'text-unidecode': '1.3', 'threadpoolctl': '3.1.0', 'tinycss2': '1.1.1', 'tk': '0.1.0', 'torch': '1.8.1+cu111', 'torchfile': '0.1.0', 'torchnet': '0.0.4', 'torchvision': '0.9.1+cu111', 'tornado': '6.1', 'tqdm': '4.33.0', 'traitlets': '5.2.2.post1', 'traittypes': '0.2.1', 'typing-extensions': '4.2.0', 'urllib3': '1.26.9', 'visdom': '0.1.8.9', 'wcwidth': '0.2.5', 'webencodings': '0.5.1', 'websocket-client': '1.3.2', 'werkzeug': '2.1.2', 'wheel': '0.37.1', 'widgetsnbextension': '3.4.2', 'xlsxwriter': '3.0.3', 'zipp': '3.8.0'}
2022-07-27 11:48:56,376 - Command line: /home/geffencooper/Model_Development/ai8x-training/venv/lib/python3.8/site-packages/ipykernel_launcher.py --ip=127.0.0.1 --stdin=9008 --control=9006 --hb=9005 --Session.signature_scheme="hmac-sha256" --Session.key=b"b01a00b5-368e-48eb-b4bd-3d25ecd4b0e5" --shell=9007 --transport="tcp" --iopub=9009 --f=/home/geffencooper/.local/share/jupyter/runtime/kernel-v2-37716983XU02QIGRjIh.json
2022-07-27 11:48:56,377 - dataset_name:asl
dataset_fn=<function asl_get_datasets at 0x7f6f96f5e160>
num_classes=29
model_name=aslclassifier
dimensions=(3, 128, 128)
batch_size=64
validation_split=0.1
lr=0.001000
num_epochs=12
qat_policy={'start_epoch': 4, 'weight_bits': 8}
2022-07-27 11:49:06,306 - Dataset sizes:
	training=70470
	validation=7830
	test=8700
2022-07-27 11:49:06,306 - Augmentations:Compose(
    Resize(size=(128, 128), interpolation=bilinear)
    ColorJitter(brightness=(0.85, 1.15), contrast=(0.75, 1.25), saturation=(0.75, 1.25), hue=(-0.4, 0.4))
    RandomGrayscale(p=0.15)
    RandomAffine(degrees=[-5.0, 5.0], translate=(0.1, 0.1))
    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 1.5))
    ToTensor()
    <ai8x.normalize object at 0x7f7105bf25b0>
)
Augmentation Seed:786101738
2022-07-27 11:49:13,885 - => loading checkpoint jupyter_logging/SSL___2022.07.15-171757/classifierbackbonenet_qat_checkpoint.pth.tar
2022-07-27 11:49:13,890 - => Checkpoint contents:
+----------------------+-------------+---------------------------+
| Key                  | Type        | Value                     |
|----------------------+-------------+---------------------------|
| arch                 | str         | classifierbackbonenet_qat |
| compression_sched    | dict        |                           |
| epoch                | int         | 99                        |
| extras               | dict        |                           |
| optimizer_state_dict | dict        |                           |
| optimizer_type       | type        | Adam                      |
| state_dict           | OrderedDict |                           |
+----------------------+-------------+---------------------------+

2022-07-27 11:49:13,891 - => Checkpoint['extras'] contents:
+-------+--------+---------+
| Key   | Type   |   Value |
|-------+--------+---------|
| epoch | int    |      99 |
+-------+--------+---------+

2022-07-27 11:49:13,891 - Loaded compression schedule from checkpoint (epoch 99)
2022-07-27 11:49:13,895 - => loaded 'state_dict' from checkpoint 'jupyter_logging/SSL___2022.07.15-171757/classifierbackbonenet_qat_checkpoint.pth.tar'
2022-07-27 11:49:13,906 - model: ASLClassifier(
  (feature_extractor): ClassifierBackbone(
    (conv1): FusedConv2dReLU(
      (activate): ReLU(inplace=True)
      (op): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv2): FusedConv2dReLU(
      (activate): ReLU(inplace=True)
      (op): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv3): FusedMaxPoolConv2dReLU(
      (activate): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (op): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv4): FusedConv2dBNReLU(
      (activate): ReLU(inplace=True)
      (op): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): None
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv5): FusedMaxPoolConv2dBNReLU(
      (activate): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (op): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): None
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv6): FusedConv2dBNReLU(
      (activate): ReLU(inplace=True)
      (op): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): None
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv7): FusedMaxPoolConv2dBNReLU(
      (activate): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (op): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): None
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv8): FusedConv2dBNReLU(
      (activate): ReLU(inplace=True)
      (op): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): None
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv9): FusedMaxPoolConv2dBNReLU(
      (activate): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (op): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): None
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv10): FusedMaxPoolConv2dBNReLU(
      (activate): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (op): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): None
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (fc1): FusedLinearReLU(
      (activate): ReLU(inplace=True)
      (op): Linear(in_features=1024, out_features=128, bias=True)
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (do1): Dropout(p=0.5, inplace=False)
    (fc2): FusedLinearReLU(
      (activate): ReLU(inplace=True)
      (op): Linear(in_features=128, out_features=64, bias=True)
      (calc_out_shift): OutputShiftSqueeze()
      (calc_weight_scale): One()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Empty()
      (quantize_bias): Empty()
      (clamp_weight): Empty()
      (clamp_bias): Empty()
      (quantize): Empty()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (fc3): Linear(
      (activate): Empty()
      (op): Linear(in_features=64, out_features=29, bias=True)
      (calc_out_shift): OutputShiftSqueeze()
      (calc_weight_scale): One()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Empty()
      (quantize_bias): Empty()
      (clamp_weight): Empty()
      (clamp_bias): Empty()
      (quantize): Empty()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
  )
  (do1): Dropout(p=0.5, inplace=False)
)
2022-07-27 11:49:13,954 - Number of Model Params: 288773
2022-07-27 11:49:18,102 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2022-07-27 11:49:18,102 - lr_schedule:base: [0.001] milestones: Counter({4: 1, 8: 1, 20: 1, 100: 1}) gamma: 0.75
2022-07-27 11:49:23,632 - Training epoch: 70470 samples (64 per mini-batch)
2022-07-27 11:49:26,925 - Epoch: [0][   50/ 1102]    objective_loss 3.373875                                        LR 0.001000    
2022-07-27 11:49:29,914 - Epoch: [0][  100/ 1102]    objective_loss 3.373145                                        LR 0.001000    
2022-07-27 11:49:32,890 - Epoch: [0][  150/ 1102]    objective_loss 3.371778                                        LR 0.001000    
2022-07-27 11:49:35,959 - Epoch: [0][  200/ 1102]    objective_loss 3.371369                                        LR 0.001000    
2022-07-27 11:49:38,916 - Epoch: [0][  250/ 1102]    objective_loss 3.371008                                        LR 0.001000    
2022-07-27 11:49:41,894 - Epoch: [0][  300/ 1102]    objective_loss 3.370689                                        LR 0.001000    
2022-07-27 11:49:44,837 - Epoch: [0][  350/ 1102]    objective_loss 3.370196                                        LR 0.001000    
2022-07-27 11:49:47,780 - Epoch: [0][  400/ 1102]    objective_loss 3.369968                                        LR 0.001000    
2022-07-27 11:49:50,803 - Epoch: [0][  450/ 1102]    objective_loss 3.369899                                        LR 0.001000    
2022-07-27 11:49:53,747 - Epoch: [0][  500/ 1102]    objective_loss 3.369753                                        LR 0.001000    
2022-07-27 11:49:56,770 - Epoch: [0][  550/ 1102]    objective_loss 3.369592                                        LR 0.001000    
2022-07-27 11:49:59,719 - Epoch: [0][  600/ 1102]    objective_loss 3.369574                                        LR 0.001000    
2022-07-27 11:50:02,745 - Epoch: [0][  650/ 1102]    objective_loss 3.369466                                        LR 0.001000    
2022-07-27 11:50:05,665 - Epoch: [0][  700/ 1102]    objective_loss 3.369434                                        LR 0.001000    
2022-07-27 11:50:08,677 - Epoch: [0][  750/ 1102]    objective_loss 3.369325                                        LR 0.001000    
2022-07-27 11:50:11,623 - Epoch: [0][  800/ 1102]    objective_loss 3.369339                                        LR 0.001000    
2022-07-27 11:50:14,636 - Epoch: [0][  850/ 1102]    objective_loss 3.369235                                        LR 0.001000    
2022-07-27 11:50:17,608 - Epoch: [0][  900/ 1102]    objective_loss 3.369172                                        LR 0.001000    
2022-07-27 11:50:20,563 - Epoch: [0][  950/ 1102]    objective_loss 3.369089                                        LR 0.001000    
2022-07-27 11:50:23,587 - Epoch: [0][ 1000/ 1102]    objective_loss 3.368990                                        LR 0.001000    
2022-07-27 11:50:26,497 - Epoch: [0][ 1050/ 1102]    objective_loss 3.369007                                        LR 0.001000    
2022-07-27 11:50:29,479 - Epoch: [0][ 1100/ 1102]    objective_loss 3.368911                                        LR 0.001000    
2022-07-27 11:50:29,546 - Epoch: [0][ 1102/ 1102]    objective_loss 3.368914    Top1 12.857143    Top5 22.857143    LR 0.001000    
2022-07-27 11:50:29,601 - --- validate (epoch=0)-----------
2022-07-27 11:50:29,602 - 7830 samples (64 per mini-batch)
2022-07-27 11:50:30,832 - Epoch: [0][   50/  123]    Loss 3.367850    Top1 3.093750    Top5 16.218750    
2022-07-27 11:50:31,813 - Epoch: [0][  100/  123]    Loss 3.367643    Top1 3.593750    Top5 16.734375    
2022-07-27 11:50:32,256 - Epoch: [0][  123/  123]    Loss 3.367557    Top1 3.537676    Top5 16.551724    
2022-07-27 11:50:32,317 - ==> Top1: 3.538    Top5: 16.552    Loss: 3.368

2022-07-27 11:50:32,318 - ==> Confusion:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 261   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 274   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 265   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 283   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 292   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 269   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 256   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 297   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 291   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 275   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 273   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 252   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 284   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 259   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 282   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 277   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 247   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 272   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 276   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 267   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 275   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 264   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 268   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 276   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 248   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 247   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 266   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 261   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 273   0   0
    0   0   0   0   0   0   0   0   0   0   0]]

2022-07-27 11:50:33,099 - ==> Best [Top1: 3.538   Top5: 16.552  on epoch: 0]
2022-07-27 11:50:33,110 - Saving checkpoint to: jupyter_logging/finetune_asl_base_ev1___2022.07.27-114856/aslclassifier_checkpoint.pth.tar
2022-07-27 11:50:33,123 - Training epoch: 70470 samples (64 per mini-batch)
2022-07-27 11:50:36,470 - Epoch: [1][   50/ 1102]    objective_loss 3.366513                                        LR 0.001000    
2022-07-27 11:50:39,445 - Epoch: [1][  100/ 1102]    objective_loss 3.367970                                        LR 0.001000    
2022-07-27 11:50:42,493 - Epoch: [1][  150/ 1102]    objective_loss 3.368061                                        LR 0.001000    
2022-07-27 11:50:45,457 - Epoch: [1][  200/ 1102]    objective_loss 3.367981                                        LR 0.001000    
2022-07-27 11:50:48,517 - Epoch: [1][  250/ 1102]    objective_loss 3.367942                                        LR 0.001000    
2022-07-27 11:50:51,455 - Epoch: [1][  300/ 1102]    objective_loss 3.367906                                        LR 0.001000    
2022-07-27 11:50:54,489 - Epoch: [1][  350/ 1102]    objective_loss 3.367800                                        LR 0.001000    
2022-07-27 11:50:57,427 - Epoch: [1][  400/ 1102]    objective_loss 3.367776                                        LR 0.001000    
2022-07-27 11:51:00,437 - Epoch: [1][  450/ 1102]    objective_loss 3.367862                                        LR 0.001000    
2022-07-27 11:51:03,413 - Epoch: [1][  500/ 1102]    objective_loss 3.367916                                        LR 0.001000    
2022-07-27 11:51:06,457 - Epoch: [1][  550/ 1102]    objective_loss 3.367890                                        LR 0.001000    
2022-07-27 11:51:09,393 - Epoch: [1][  600/ 1102]    objective_loss 3.367869                                        LR 0.001000    
2022-07-27 11:51:12,410 - Epoch: [1][  650/ 1102]    objective_loss 3.367895                                        LR 0.001000    
2022-07-27 11:51:15,329 - Epoch: [1][  700/ 1102]    objective_loss 3.367914                                        LR 0.001000    
2022-07-27 11:51:18,373 - Epoch: [1][  750/ 1102]    objective_loss 3.367877                                        LR 0.001000    
2022-07-27 11:51:21,300 - Epoch: [1][  800/ 1102]    objective_loss 3.367894                                        LR 0.001000    
2022-07-27 11:51:24,304 - Epoch: [1][  850/ 1102]    objective_loss 3.367883                                        LR 0.001000    
2022-07-27 11:51:27,246 - Epoch: [1][  900/ 1102]    objective_loss 3.367880                                        LR 0.001000    
2022-07-27 11:51:30,275 - Epoch: [1][  950/ 1102]    objective_loss 3.367866                                        LR 0.001000    
2022-07-27 11:51:33,217 - Epoch: [1][ 1000/ 1102]    objective_loss 3.367851                                        LR 0.001000    
2022-07-27 11:51:36,182 - Epoch: [1][ 1050/ 1102]    objective_loss 3.367856                                        LR 0.001000    
2022-07-27 11:51:39,196 - Epoch: [1][ 1100/ 1102]    objective_loss 3.367824                                        LR 0.001000    
2022-07-27 11:51:39,262 - Epoch: [1][ 1102/ 1102]    objective_loss 3.367808    Top1 5.714286    Top5 18.571429    LR 0.001000    
2022-07-27 11:51:39,316 - --- validate (epoch=1)-----------
2022-07-27 11:51:39,317 - 7830 samples (64 per mini-batch)
2022-07-27 11:51:40,458 - Epoch: [1][   50/  123]    Loss 3.367496    Top1 3.656250    Top5 17.093750    
2022-07-27 11:51:41,429 - Epoch: [1][  100/  123]    Loss 3.367826    Top1 3.437500    Top5 16.593750    
2022-07-27 11:51:41,813 - Epoch: [1][  123/  123]    Loss 3.367935    Top1 3.422733    Top5 16.411239    
2022-07-27 11:51:41,876 - ==> Top1: 3.423    Top5: 16.411    Loss: 3.368

2022-07-27 11:51:41,878 - ==> Confusion:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 261   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 274   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 265   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 283   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 292   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 269   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 256   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 297   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 291   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 275   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 273   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 252   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 284   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 259   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 282   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 277   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 247   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 272   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 276   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 267   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 275   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 264   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 268   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 276   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 248   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 247   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 266   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 261   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 273   0   0   0   0   0   0]]

2022-07-27 11:51:42,768 - ==> Best [Top1: 3.538   Top5: 16.552  on epoch: 0]
2022-07-27 11:51:42,779 - Saving checkpoint to: jupyter_logging/finetune_asl_base_ev1___2022.07.27-114856/aslclassifier_checkpoint.pth.tar
2022-07-27 11:51:42,801 - Training epoch: 70470 samples (64 per mini-batch)
2022-07-27 11:51:46,159 - Epoch: [2][   50/ 1102]    objective_loss 3.368246                                        LR 0.001000    
2022-07-27 11:51:49,118 - Epoch: [2][  100/ 1102]    objective_loss 3.367671                                        LR 0.001000    
2022-07-27 11:51:52,194 - Epoch: [2][  150/ 1102]    objective_loss 3.367736                                        LR 0.001000    
2022-07-27 11:51:55,162 - Epoch: [2][  200/ 1102]    objective_loss 3.367812                                        LR 0.001000    
2022-07-27 11:51:58,178 - Epoch: [2][  250/ 1102]    objective_loss 3.367689                                        LR 0.001000    
2022-07-27 11:52:01,249 - Epoch: [2][  300/ 1102]    objective_loss 3.367867                                        LR 0.001000    
2022-07-27 11:52:04,235 - Epoch: [2][  350/ 1102]    objective_loss 3.367783                                        LR 0.001000    
2022-07-27 11:52:07,253 - Epoch: [2][  400/ 1102]    objective_loss 3.367837                                        LR 0.001000    
2022-07-27 11:52:10,333 - Epoch: [2][  450/ 1102]    objective_loss 3.367796                                        LR 0.001000    
2022-07-27 11:52:13,344 - Epoch: [2][  500/ 1102]    objective_loss 3.367841                                        LR 0.001000    
2022-07-27 11:52:16,378 - Epoch: [2][  550/ 1102]    objective_loss 3.367838                                        LR 0.001000    
2022-07-27 11:52:19,352 - Epoch: [2][  600/ 1102]    objective_loss 3.367803                                        LR 0.001000    
2022-07-27 11:52:22,390 - Epoch: [2][  650/ 1102]    objective_loss 3.367787                                        LR 0.001000    
2022-07-27 11:52:25,372 - Epoch: [2][  700/ 1102]    objective_loss 3.367811                                        LR 0.001000    
2022-07-27 11:52:28,437 - Epoch: [2][  750/ 1102]    objective_loss 3.367783                                        LR 0.001000    
2022-07-27 11:52:31,384 - Epoch: [2][  800/ 1102]    objective_loss 3.367766                                        LR 0.001000    
2022-07-27 11:52:34,442 - Epoch: [2][  850/ 1102]    objective_loss 3.367741                                        LR 0.001000    
2022-07-27 11:52:37,400 - Epoch: [2][  900/ 1102]    objective_loss 3.367735                                        LR 0.001000    
2022-07-27 11:52:40,469 - Epoch: [2][  950/ 1102]    objective_loss 3.367755                                        LR 0.001000    
2022-07-27 11:52:43,443 - Epoch: [2][ 1000/ 1102]    objective_loss 3.367753                                        LR 0.001000    
2022-07-27 11:52:46,472 - Epoch: [2][ 1050/ 1102]    objective_loss 3.367748                                        LR 0.001000    
2022-07-27 11:52:49,443 - Epoch: [2][ 1100/ 1102]    objective_loss 3.367769                                        LR 0.001000    
2022-07-27 11:52:49,541 - Epoch: [2][ 1102/ 1102]    objective_loss 3.367782    Top1 4.285714    Top5 10.000000    LR 0.001000    
2022-07-27 11:52:49,597 - --- validate (epoch=2)-----------
2022-07-27 11:52:49,598 - 7830 samples (64 per mini-batch)
2022-07-27 11:52:50,776 - Epoch: [2][   50/  123]    Loss 3.367330    Top1 3.218750    Top5 16.593750    
2022-07-27 11:52:51,771 - Epoch: [2][  100/  123]    Loss 3.367469    Top1 3.234375    Top5 16.453125    
2022-07-27 11:52:52,172 - Epoch: [2][  123/  123]    Loss 3.367400    Top1 3.218391    Top5 16.602810    
2022-07-27 11:52:52,228 - ==> Top1: 3.218    Top5: 16.603    Loss: 3.367

2022-07-27 11:52:52,236 - ==> Confusion:
[[  0   0   0   0   0   0   0   0   0   0   0 261   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 274   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 265   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 283   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 292   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 269   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 256   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 297   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 291   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 275   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 273   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 252   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 284   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 259   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 282   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 277   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 247   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 272   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 276   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 267   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 275   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 264   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 268   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 276   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 248   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 247   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 266   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 261   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 273   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]]

2022-07-27 11:52:52,985 - ==> Best [Top1: 3.538   Top5: 16.552  on epoch: 0]
2022-07-27 11:52:52,986 - Saving checkpoint to: jupyter_logging/finetune_asl_base_ev1___2022.07.27-114856/aslclassifier_checkpoint.pth.tar
2022-07-27 11:52:53,017 - Training epoch: 70470 samples (64 per mini-batch)
2022-07-27 11:52:56,303 - Epoch: [3][   50/ 1102]    objective_loss 3.367385                                        LR 0.001000    
2022-07-27 11:52:59,287 - Epoch: [3][  100/ 1102]    objective_loss 3.367403                                        LR 0.001000    
2022-07-27 11:53:02,277 - Epoch: [3][  150/ 1102]    objective_loss 3.367512                                        LR 0.001000    
2022-07-27 11:53:05,354 - Epoch: [3][  200/ 1102]    objective_loss 3.367461                                        LR 0.001000    
2022-07-27 11:53:08,335 - Epoch: [3][  250/ 1102]    objective_loss 3.367574                                        LR 0.001000    
2022-07-27 11:53:11,375 - Epoch: [3][  300/ 1102]    objective_loss 3.367548                                        LR 0.001000    
2022-07-27 11:53:14,333 - Epoch: [3][  350/ 1102]    objective_loss 3.367520                                        LR 0.001000    
2022-07-27 11:53:17,346 - Epoch: [3][  400/ 1102]    objective_loss 3.367522                                        LR 0.001000    
2022-07-27 11:53:20,269 - Epoch: [3][  450/ 1102]    objective_loss 3.367558                                        LR 0.001000    
2022-07-27 11:53:23,296 - Epoch: [3][  500/ 1102]    objective_loss 3.367603                                        LR 0.001000    
2022-07-27 11:53:26,257 - Epoch: [3][  550/ 1102]    objective_loss 3.367606                                        LR 0.001000    
2022-07-27 11:53:29,298 - Epoch: [3][  600/ 1102]    objective_loss 3.367595                                        LR 0.001000    
2022-07-27 11:53:32,252 - Epoch: [3][  650/ 1102]    objective_loss 3.367600                                        LR 0.001000    
2022-07-27 11:53:35,278 - Epoch: [3][  700/ 1102]    objective_loss 3.367608                                        LR 0.001000    
2022-07-27 11:53:38,220 - Epoch: [3][  750/ 1102]    objective_loss 3.367611                                        LR 0.001000    
2022-07-27 11:53:41,270 - Epoch: [3][  800/ 1102]    objective_loss 3.367648                                        LR 0.001000    
2022-07-27 11:53:44,214 - Epoch: [3][  850/ 1102]    objective_loss 3.367653                                        LR 0.001000    
2022-07-27 11:53:47,283 - Epoch: [3][  900/ 1102]    objective_loss 3.367672                                        LR 0.001000    
2022-07-27 11:53:50,226 - Epoch: [3][  950/ 1102]    objective_loss 3.367647                                        LR 0.001000    
2022-07-27 11:53:53,254 - Epoch: [3][ 1000/ 1102]    objective_loss 3.367645                                        LR 0.001000    
2022-07-27 11:53:56,202 - Epoch: [3][ 1050/ 1102]    objective_loss 3.367671                                        LR 0.001000    
2022-07-27 11:53:59,186 - Epoch: [3][ 1100/ 1102]    objective_loss 3.367681                                        LR 0.001000    
2022-07-27 11:53:59,252 - Epoch: [3][ 1102/ 1102]    objective_loss 3.367679    Top1 2.857143    Top5 15.714286    LR 0.001000    
2022-07-27 11:53:59,308 - --- validate (epoch=3)-----------
2022-07-27 11:53:59,308 - 7830 samples (64 per mini-batch)
2022-07-27 11:54:00,461 - Epoch: [3][   50/  123]    Loss 3.367729    Top1 3.125000    Top5 16.937500    
2022-07-27 11:54:01,427 - Epoch: [3][  100/  123]    Loss 3.367714    Top1 3.296875    Top5 16.531250    
2022-07-27 11:54:01,883 - Epoch: [3][  123/  123]    Loss 3.367687    Top1 3.269476    Top5 16.564496    
2022-07-27 11:54:01,939 - ==> Top1: 3.269    Top5: 16.564    Loss: 3.368

2022-07-27 11:54:01,941 - ==> Confusion:
[[  0   0   0   0   0   0 261   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 274   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 265   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 283   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 292   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 269   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 256   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 297   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 291   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 275   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 273   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 252   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 284   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 259   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 282   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 277   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 247   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 272   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 276   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 267   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 275   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 264   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 268   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 276   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 248   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 247   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 266   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 261   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 273   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]]

2022-07-27 11:54:02,848 - ==> Best [Top1: 3.538   Top5: 16.552  on epoch: 0]
2022-07-27 11:54:02,859 - Saving checkpoint to: jupyter_logging/finetune_asl_base_ev1___2022.07.27-114856/aslclassifier_checkpoint.pth.tar
2022-07-27 11:54:02,900 - Training epoch: 70470 samples (64 per mini-batch)
2022-07-27 11:54:06,299 - Epoch: [4][   50/ 1102]    objective_loss 3.367157                                        LR 0.000750    
2022-07-27 11:54:09,348 - Epoch: [4][  100/ 1102]    objective_loss 3.367161                                        LR 0.000750    
2022-07-27 11:54:12,434 - Epoch: [4][  150/ 1102]    objective_loss 3.367270                                        LR 0.000750    
2022-07-27 11:54:15,417 - Epoch: [4][  200/ 1102]    objective_loss 3.367380                                        LR 0.000750    
2022-07-27 11:54:18,544 - Epoch: [4][  250/ 1102]    objective_loss 3.367363                                        LR 0.000750    
2022-07-27 11:54:21,519 - Epoch: [4][  300/ 1102]    objective_loss 3.367354                                        LR 0.000750    
2022-07-27 11:54:24,591 - Epoch: [4][  350/ 1102]    objective_loss 3.367309                                        LR 0.000750    
2022-07-27 11:54:27,585 - Epoch: [4][  400/ 1102]    objective_loss 3.367301                                        LR 0.000750    
2022-07-27 11:54:30,657 - Epoch: [4][  450/ 1102]    objective_loss 3.367361                                        LR 0.000750    
2022-07-27 11:54:33,653 - Epoch: [4][  500/ 1102]    objective_loss 3.367365                                        LR 0.000750    
2022-07-27 11:54:36,714 - Epoch: [4][  550/ 1102]    objective_loss 3.367384                                        LR 0.000750    
2022-07-27 11:54:39,710 - Epoch: [4][  600/ 1102]    objective_loss 3.367386                                        LR 0.000750    
2022-07-27 11:54:42,780 - Epoch: [4][  650/ 1102]    objective_loss 3.367374                                        LR 0.000750    
2022-07-27 11:54:45,755 - Epoch: [4][  700/ 1102]    objective_loss 3.367395                                        LR 0.000750    
2022-07-27 11:54:48,816 - Epoch: [4][  750/ 1102]    objective_loss 3.367420                                        LR 0.000750    
2022-07-27 11:54:51,793 - Epoch: [4][  800/ 1102]    objective_loss 3.367422                                        LR 0.000750    
2022-07-27 11:54:54,859 - Epoch: [4][  850/ 1102]    objective_loss 3.367425                                        LR 0.000750    
2022-07-27 11:54:57,842 - Epoch: [4][  900/ 1102]    objective_loss 3.367394                                        LR 0.000750    
2022-07-27 11:55:00,895 - Epoch: [4][  950/ 1102]    objective_loss 3.367399                                        LR 0.000750    
2022-07-27 11:55:03,882 - Epoch: [4][ 1000/ 1102]    objective_loss 3.367408                                        LR 0.000750    
2022-07-27 11:55:06,949 - Epoch: [4][ 1050/ 1102]    objective_loss 3.367413                                        LR 0.000750    
2022-07-27 11:55:09,917 - Epoch: [4][ 1100/ 1102]    objective_loss 3.367412                                        LR 0.000750    
2022-07-27 11:55:10,057 - Epoch: [4][ 1102/ 1102]    objective_loss 3.367413    Top1 2.857143    Top5 11.428571    LR 0.000750    
2022-07-27 11:55:10,113 - --- validate (epoch=4)-----------
2022-07-27 11:55:10,114 - 7830 samples (64 per mini-batch)
2022-07-27 11:55:11,299 - Epoch: [4][   50/  123]    Loss 3.367661    Top1 2.968750    Top5 15.500000    
2022-07-27 11:55:12,274 - Epoch: [4][  100/  123]    Loss 3.367613    Top1 3.203125    Top5 16.093750    
2022-07-27 11:55:12,694 - Epoch: [4][  123/  123]    Loss 3.367586    Top1 3.167305    Top5 16.309068    
2022-07-27 11:55:12,752 - ==> Top1: 3.167    Top5: 16.309    Loss: 3.368

2022-07-27 11:55:12,754 - ==> Confusion:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 261   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 274   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 265   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 283   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 292   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 269   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 256   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 297   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 291   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 275   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 273   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 252   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 284   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 259   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 282   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 277   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 247   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 272   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 276   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 267   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 275   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 264   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 268   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 276   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 248   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 247   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 266   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 261   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 273   0   0   0   0]]

2022-07-27 11:55:13,492 - ==> Best [Top1: 3.167   Top5: 16.309  on epoch: 4]
2022-07-27 11:55:13,503 - Saving checkpoint to: jupyter_logging/finetune_asl_base_ev1___2022.07.27-114856/aslclassifier_qat_checkpoint.pth.tar
2022-07-27 11:55:13,517 - Training epoch: 70470 samples (64 per mini-batch)
2022-07-27 11:55:16,813 - Epoch: [5][   50/ 1102]    objective_loss 3.367266                                        LR 0.000750    
2022-07-27 11:55:19,738 - Epoch: [5][  100/ 1102]    objective_loss 3.367211                                        LR 0.000750    
2022-07-27 11:55:22,737 - Epoch: [5][  150/ 1102]    objective_loss 3.367246                                        LR 0.000750    
2022-07-27 11:55:25,722 - Epoch: [5][  200/ 1102]    objective_loss 3.367244                                        LR 0.000750    

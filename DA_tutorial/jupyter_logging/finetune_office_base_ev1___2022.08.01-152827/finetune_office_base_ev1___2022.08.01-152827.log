2022-08-01 15:28:27,716 - Log file for this run: /home/geffencooper/Model_Development/DA_ai8x-training/DA_tutorial/jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/finetune_office_base_ev1___2022.08.01-152827.log
2022-08-01 15:28:27,717 - Number of CPUs: 16
2022-08-01 15:28:27,717 - Number of GPUs: 1
2022-08-01 15:28:27,717 - CUDA version: 10.2
2022-08-01 15:28:27,717 - CUDNN version: 7605
2022-08-01 15:28:27,717 - Kernel: 5.4.0-113-generic
2022-08-01 15:28:27,717 - Python: 3.8.11 (default, Jun 14 2022, 10:01:20) 
[GCC 9.4.0]
2022-08-01 15:28:27,718 - pip freeze: {'absl-py': '1.2.0', 'appdirs': '1.4.4', 'argon2-cffi': '21.3.0', 'argon2-cffi-bindings': '21.2.0', 'asttokens': '2.0.5', 'atomicwrites': '1.4.1', 'attrs': '21.4.0', 'audioread': '2.1.9', 'backcall': '0.2.0', 'beautifulsoup4': '4.11.1', 'bleach': '5.0.1', 'bqplot': '0.11.5', 'cachetools': '5.2.0', 'certifi': '2022.6.15', 'cffi': '1.15.1', 'charset-normalizer': '2.1.0', 'cloudpickle': '2.1.0', 'cycler': '0.11.0', 'debugpy': '1.6.2', 'decorator': '5.1.1', 'defusedxml': '0.7.1', 'distiller': '0.4.0rc0', 'entrypoints': '0.4', 'executing': '0.9.1', 'fastjsonschema': '2.16.1', 'fonttools': '4.34.4', 'google-auth': '2.9.1', 'google-auth-oauthlib': '0.4.6', 'graphviz': '0.10.1', 'grpcio': '1.47.0', 'gym': '0.12.5', 'h5py': '3.7.0', 'idna': '3.3', 'importlib-metadata': '4.12.0', 'importlib-resources': '5.9.0', 'ipykernel': '6.15.1', 'ipython': '8.4.0', 'ipython-genutils': '0.2.0', 'ipywidgets': '7.4.2', 'jedi': '0.18.1', 'jinja2': '3.1.2', 'joblib': '1.1.0', 'jsonpatch': '1.32', 'jsonpointer': '2.3', 'jsonschema': '4.7.2', 'jupyter': '1.0.0', 'jupyter-client': '7.3.4', 'jupyter-console': '6.4.4', 'jupyter-core': '4.11.1', 'jupyterlab-pygments': '0.2.2', 'kiwisolver': '1.4.4', 'librosa': '0.9.2', 'llvmlite': '0.32.1', 'markdown': '3.4.1', 'markupsafe': '2.1.1', 'matplotlib': '3.5.2', 'matplotlib-inline': '0.1.3', 'mistune': '0.8.4', 'more-itertools': '8.13.0', 'munch': '2.5.0', 'nbclient': '0.6.6', 'nbconvert': '6.5.0', 'nbformat': '5.4.0', 'nest-asyncio': '1.5.5', 'notebook': '6.4.12', 'numba': '0.49.1', 'numpy': '1.22.4', 'oauthlib': '3.2.0', 'opencv-python': '4.6.0.66', 'packaging': '21.3', 'pandas': '1.4.3', 'pandocfilters': '1.5.0', 'parso': '0.8.3', 'pexpect': '4.8.0', 'pickleshare': '0.7.5', 'pillow': '9.2.0', 'pip': '22.2', 'pluggy': '0.13.1', 'pooch': '1.6.0', 'pretrainedmodels': '0.7.4', 'prometheus-client': '0.14.1', 'prompt-toolkit': '3.0.30', 'protobuf': '3.20.1', 'psutil': '5.9.1', 'ptyprocess': '0.7.0', 'pure-eval': '0.2.2', 'py': '1.11.0', 'pyasn1': '0.4.8', 'pyasn1-modules': '0.2.8', 'pycparser': '2.21', 'pydot': '1.4.1', 'pyglet': '1.5.26', 'pygments': '2.12.0', 'pyparsing': '3.0.9', 'pyrsistent': '0.18.1', 'pytest': '4.6.11', 'python-dateutil': '2.8.2', 'pytsmod': '0.3.5', 'pytz': '2022.1', 'pyyaml': '6.0', 'pyzmq': '23.2.0', 'qgrid': '1.1.1', 'qtconsole': '5.3.1', 'qtpy': '2.1.0', 'requests': '2.28.1', 'requests-oauthlib': '1.3.1', 'resampy': '0.3.1', 'rsa': '4.9', 'scikit-learn': '0.23.2', 'scipy': '1.8.1', 'send2trash': '1.8.0', 'setuptools': '63.2.0', 'shap': '0.41.0', 'six': '1.16.0', 'slicer': '0.0.7', 'soundfile': '0.10.3.post1', 'soupsieve': '2.3.2.post1', 'stack-data': '0.3.0', 'tabulate': '0.8.3', 'tensorboard': '2.9.0', 'tensorboard-data-server': '0.6.1', 'tensorboard-plugin-wit': '1.8.1', 'terminado': '0.15.0', 'threadpoolctl': '3.1.0', 'tinycss2': '1.1.1', 'tk': '0.1.0', 'torch': '1.8.1', 'torchfile': '0.1.0', 'torchnet': '0.0.4', 'torchvision': '0.9.1', 'tornado': '6.2', 'tqdm': '4.33.0', 'traitlets': '5.3.0', 'traittypes': '0.2.1', 'typing-extensions': '4.3.0', 'urllib3': '1.26.11', 'visdom': '0.1.8.9', 'wcwidth': '0.2.5', 'webencodings': '0.5.1', 'websocket-client': '1.3.3', 'werkzeug': '2.2.0', 'wheel': '0.37.1', 'widgetsnbextension': '3.4.2', 'xlsxwriter': '3.0.3', 'zipp': '3.8.1'}
2022-08-01 15:28:27,718 - Command line: /home/geffencooper/Model_Development/DA_ai8x-training/venv/lib/python3.8/site-packages/ipykernel_launcher.py --ip=127.0.0.1 --stdin=9003 --control=9001 --hb=9000 --Session.signature_scheme="hmac-sha256" --Session.key=b"25ad75a3-5bc0-44f0-a00c-d6d1e3e4825b" --shell=9002 --transport="tcp" --iopub=9004 --f=/home/geffencooper/.local/share/jupyter/runtime/kernel-v2-3810819vTR0vbU26xBD.json
2022-08-01 15:28:27,719 - dataset_name:office
dataset_fn=<function office_get_datasets at 0x7fb324a26c10>
num_classes=6
model_name=officeclassifier
dimensions=(3, 128, 128)
batch_size=32
validation_split=0.1
lr=0.001000
num_epochs=32
qat_policy={'start_epoch': 4, 'weight_bits': 8}
2022-08-01 15:28:30,034 - Dataset sizes:
	training=501
	validation=55
	test=64
2022-08-01 15:28:30,035 - Augmentations:Compose(
    Resize(size=(128, 128), interpolation=bilinear)
    ColorJitter(brightness=(0.85, 1.15), contrast=(0.75, 1.25), saturation=(0.75, 1.25), hue=(-0.4, 0.4))
    RandomGrayscale(p=0.15)
    RandomAffine(degrees=[-10.0, 10.0], translate=(0.27, 0.27))
    RandomHorizontalFlip(p=0.5)
    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 1.5))
    ToTensor()
    <ai8x.normalize object at 0x7fb314495e20>
)
Augmentation Seed:1316206506
2022-08-01 15:28:32,402 - => loading checkpoint jupyter_logging/SSL___2022.07.15-171757/classifierbackbonenet_qat_checkpoint.pth.tar
2022-08-01 15:28:32,408 - => Checkpoint contents:
+----------------------+-------------+---------------------------+
| Key                  | Type        | Value                     |
|----------------------+-------------+---------------------------|
| arch                 | str         | classifierbackbonenet_qat |
| compression_sched    | dict        |                           |
| epoch                | int         | 99                        |
| extras               | dict        |                           |
| optimizer_state_dict | dict        |                           |
| optimizer_type       | type        | Adam                      |
| state_dict           | OrderedDict |                           |
+----------------------+-------------+---------------------------+

2022-08-01 15:28:32,408 - => Checkpoint['extras'] contents:
+-------+--------+---------+
| Key   | Type   |   Value |
|-------+--------+---------|
| epoch | int    |      99 |
+-------+--------+---------+

2022-08-01 15:28:32,409 - Loaded compression schedule from checkpoint (epoch 99)
2022-08-01 15:28:32,414 - => loaded 'state_dict' from checkpoint 'jupyter_logging/SSL___2022.07.15-171757/classifierbackbonenet_qat_checkpoint.pth.tar'
2022-08-01 15:28:32,427 - model: OfficeClassifier(
  (feature_extractor): ClassifierBackbone(
    (conv1): FusedConv2dReLU(
      (activate): ReLU(inplace=True)
      (op): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv2): FusedConv2dReLU(
      (activate): ReLU(inplace=True)
      (op): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv3): FusedMaxPoolConv2dReLU(
      (activate): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (op): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv4): FusedConv2dBNReLU(
      (activate): ReLU(inplace=True)
      (op): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): None
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv5): FusedMaxPoolConv2dBNReLU(
      (activate): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (op): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): None
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv6): FusedConv2dBNReLU(
      (activate): ReLU(inplace=True)
      (op): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): None
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv7): FusedMaxPoolConv2dBNReLU(
      (activate): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (op): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): None
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv8): FusedConv2dBNReLU(
      (activate): ReLU(inplace=True)
      (op): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): None
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv9): FusedMaxPoolConv2dBNReLU(
      (activate): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (op): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): None
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv10): FusedMaxPoolConv2dBNReLU(
      (activate): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (op): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): None
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (fc1): FusedLinearReLU(
      (activate): ReLU(inplace=True)
      (op): Linear(in_features=1024, out_features=128, bias=True)
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (do1): Dropout(p=0.5, inplace=False)
    (fc2): FusedLinearReLU(
      (activate): ReLU(inplace=True)
      (op): Linear(in_features=128, out_features=64, bias=True)
      (calc_out_shift): OutputShiftSqueeze()
      (calc_weight_scale): One()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Empty()
      (quantize_bias): Empty()
      (clamp_weight): Empty()
      (clamp_bias): Empty()
      (quantize): Empty()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (fc3): Linear(
      (activate): Empty()
      (op): Linear(in_features=64, out_features=6, bias=True)
      (calc_out_shift): OutputShiftSqueeze()
      (calc_weight_scale): One()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Empty()
      (quantize_bias): Empty()
      (clamp_weight): Empty()
      (clamp_bias): Empty()
      (quantize): Empty()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
  )
  (do1): Dropout(p=0.25, inplace=False)
)
2022-08-01 15:28:32,471 - Number of Model Params: 287278
2022-08-01 15:28:33,301 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2022-08-01 15:28:33,305 - lr_schedule:base: [0.001] milestones: Counter({4: 1, 8: 1, 20: 1, 100: 1}) gamma: 0.75
2022-08-01 15:28:36,129 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:28:37,025 - Epoch: [0][   16/   16]    objective_loss 1.753904    Top1 35.849057    Top5 94.339623    LR 0.001000    
2022-08-01 15:28:37,062 - --- validate (epoch=0)-----------
2022-08-01 15:28:37,063 - 55 samples (32 per mini-batch)
2022-08-01 15:28:37,339 - Epoch: [0][    2/    2]    Loss 1.724728    Top1 27.272727    Top5 98.181818    
2022-08-01 15:28:37,374 - ==> Top1: 27.273    Top5: 98.182    Loss: 1.725

2022-08-01 15:28:37,375 - ==> Confusion:
[[ 0  5  0  0  0  0]
 [ 0 14  0  0  0  0]
 [ 0  5  0  0  0  0]
 [ 0 12  0  0  0  0]
 [ 0  8  2  0  1  0]
 [ 0  8  0  0  0  0]]

2022-08-01 15:28:37,469 - ==> Best [Top1: 27.273   Top5: 98.182  on epoch: 0]
2022-08-01 15:28:37,479 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_checkpoint.pth.tar
2022-08-01 15:28:37,496 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:28:38,449 - Epoch: [1][   16/   16]    objective_loss 1.511546    Top1 50.943396    Top5 92.452830    LR 0.001000    
2022-08-01 15:28:38,495 - --- validate (epoch=1)-----------
2022-08-01 15:28:38,497 - 55 samples (32 per mini-batch)
2022-08-01 15:28:38,749 - Epoch: [1][    2/    2]    Loss 1.225180    Top1 54.545455    Top5 100.000000    
2022-08-01 15:28:38,800 - ==> Top1: 54.545    Top5: 100.000    Loss: 1.225

2022-08-01 15:28:38,801 - ==> Confusion:
[[ 2  0  0  0  3  0]
 [ 0 13  1  0  0  0]
 [ 0  0  5  0  0  0]
 [ 1  4  4  0  3  0]
 [ 1  0  0  0 10  0]
 [ 1  0  3  0  4  0]]

2022-08-01 15:28:38,897 - ==> Best [Top1: 54.545   Top5: 100.000  on epoch: 1]
2022-08-01 15:28:38,907 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_checkpoint.pth.tar
2022-08-01 15:28:38,932 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:28:39,913 - Epoch: [2][   16/   16]    objective_loss 1.137931    Top1 64.150943    Top5 100.000000    LR 0.001000    
2022-08-01 15:28:39,949 - --- validate (epoch=2)-----------
2022-08-01 15:28:39,950 - 55 samples (32 per mini-batch)
2022-08-01 15:28:40,222 - Epoch: [2][    2/    2]    Loss 1.071070    Top1 56.363636    Top5 100.000000    
2022-08-01 15:28:40,258 - ==> Top1: 56.364    Top5: 100.000    Loss: 1.071

2022-08-01 15:28:40,259 - ==> Confusion:
[[ 1  0  0  0  3  1]
 [ 0 14  0  0  0  0]
 [ 0  0  4  1  0  0]
 [ 0  4  3  1  0  4]
 [ 0  0  0  0 10  1]
 [ 1  0  1  0  5  1]]

2022-08-01 15:28:40,354 - ==> Best [Top1: 56.364   Top5: 100.000  on epoch: 2]
2022-08-01 15:28:40,363 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_checkpoint.pth.tar
2022-08-01 15:28:40,389 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:28:41,307 - Epoch: [3][   16/   16]    objective_loss 1.020687    Top1 71.698113    Top5 100.000000    LR 0.001000    
2022-08-01 15:28:41,351 - --- validate (epoch=3)-----------
2022-08-01 15:28:41,352 - 55 samples (32 per mini-batch)
2022-08-01 15:28:41,601 - Epoch: [3][    2/    2]    Loss 0.933199    Top1 69.090909    Top5 98.181818    
2022-08-01 15:28:41,639 - ==> Top1: 69.091    Top5: 98.182    Loss: 0.933

2022-08-01 15:28:41,640 - ==> Confusion:
[[ 5  0  0  0  0  0]
 [ 0 14  0  0  0  0]
 [ 1  0  4  0  0  0]
 [ 1  4  2  3  0  2]
 [ 3  0  0  0  8  0]
 [ 0  1  0  2  1  4]]

2022-08-01 15:28:41,730 - ==> Best [Top1: 69.091   Top5: 98.182  on epoch: 3]
2022-08-01 15:28:41,740 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_checkpoint.pth.tar
2022-08-01 15:28:41,779 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:28:42,830 - Epoch: [4][   16/   16]    objective_loss 1.113358    Top1 73.584906    Top5 100.000000    LR 0.000750    
2022-08-01 15:28:42,867 - --- validate (epoch=4)-----------
2022-08-01 15:28:42,867 - 55 samples (32 per mini-batch)
2022-08-01 15:28:43,149 - Epoch: [4][    2/    2]    Loss 1.029072    Top1 76.363636    Top5 98.181818    
2022-08-01 15:28:43,184 - ==> Top1: 76.364    Top5: 98.182    Loss: 1.029

2022-08-01 15:28:43,185 - ==> Confusion:
[[ 5  0  0  0  0  0]
 [ 0 14  0  0  0  0]
 [ 1  0  4  0  0  0]
 [ 0  1  3  7  0  1]
 [ 3  0  0  0  8  0]
 [ 0  1  0  2  1  4]]

2022-08-01 15:28:43,282 - ==> Best [Top1: 76.364   Top5: 98.182  on epoch: 4]
2022-08-01 15:28:43,292 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:28:43,306 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:28:44,285 - Epoch: [5][   16/   16]    objective_loss 0.983792    Top1 77.358491    Top5 98.113208    LR 0.000750    
2022-08-01 15:28:44,323 - --- validate (epoch=5)-----------
2022-08-01 15:28:44,323 - 55 samples (32 per mini-batch)
2022-08-01 15:28:44,612 - Epoch: [5][    2/    2]    Loss 0.916911    Top1 80.000000    Top5 98.181818    
2022-08-01 15:28:44,650 - ==> Top1: 80.000    Top5: 98.182    Loss: 0.917

2022-08-01 15:28:44,651 - ==> Confusion:
[[ 4  0  0  0  0  1]
 [ 0 14  0  0  0  0]
 [ 0  0  4  1  0  0]
 [ 0  0  2 10  0  0]
 [ 3  0  0  0  8  0]
 [ 0  1  1  1  1  4]]

2022-08-01 15:28:44,748 - ==> Best [Top1: 80.000   Top5: 98.182  on epoch: 5]
2022-08-01 15:28:44,759 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:28:44,784 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:28:45,812 - Epoch: [6][   16/   16]    objective_loss 0.921911    Top1 81.132075    Top5 100.000000    LR 0.000750    
2022-08-01 15:28:45,850 - --- validate (epoch=6)-----------
2022-08-01 15:28:45,851 - 55 samples (32 per mini-batch)
2022-08-01 15:28:46,129 - Epoch: [6][    2/    2]    Loss 0.937253    Top1 74.545455    Top5 100.000000    
2022-08-01 15:28:46,163 - ==> Top1: 74.545    Top5: 100.000    Loss: 0.937

2022-08-01 15:28:46,164 - ==> Confusion:
[[ 3  0  0  1  0  1]
 [ 0 14  0  0  0  0]
 [ 0  0  4  1  0  0]
 [ 0  0  3  9  0  0]
 [ 3  0  0  0  8  0]
 [ 0  1  2  1  1  3]]

2022-08-01 15:28:46,259 - ==> Best [Top1: 80.000   Top5: 98.182  on epoch: 5]
2022-08-01 15:28:46,270 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:28:46,292 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:28:47,268 - Epoch: [7][   16/   16]    objective_loss 0.847648    Top1 81.132075    Top5 100.000000    LR 0.000750    
2022-08-01 15:28:47,305 - --- validate (epoch=7)-----------
2022-08-01 15:28:47,305 - 55 samples (32 per mini-batch)
2022-08-01 15:28:47,580 - Epoch: [7][    2/    2]    Loss 0.886478    Top1 78.181818    Top5 100.000000    
2022-08-01 15:28:47,616 - ==> Top1: 78.182    Top5: 100.000    Loss: 0.886

2022-08-01 15:28:47,616 - ==> Confusion:
[[ 4  0  0  1  0  0]
 [ 0 14  0  0  0  0]
 [ 0  0  4  1  0  0]
 [ 0  0  1 11  0  0]
 [ 3  0  0  0  7  1]
 [ 0  1  2  1  1  3]]

2022-08-01 15:28:47,696 - ==> Best [Top1: 80.000   Top5: 98.182  on epoch: 5]
2022-08-01 15:28:47,705 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:28:47,731 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:28:48,744 - Epoch: [8][   16/   16]    objective_loss 0.805350    Top1 67.924528    Top5 100.000000    LR 0.000563    
2022-08-01 15:28:48,784 - --- validate (epoch=8)-----------
2022-08-01 15:28:48,784 - 55 samples (32 per mini-batch)
2022-08-01 15:28:49,061 - Epoch: [8][    2/    2]    Loss 0.870972    Top1 78.181818    Top5 98.181818    
2022-08-01 15:28:49,114 - ==> Top1: 78.182    Top5: 98.182    Loss: 0.871

2022-08-01 15:28:49,116 - ==> Confusion:
[[ 4  0  0  0  0  1]
 [ 0 14  0  0  0  0]
 [ 0  0  4  1  0  0]
 [ 0  0  2 10  0  0]
 [ 3  0  0  0  8  0]
 [ 0  1  2  1  1  3]]

2022-08-01 15:28:49,191 - ==> Best [Top1: 80.000   Top5: 98.182  on epoch: 5]
2022-08-01 15:28:49,196 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:28:49,218 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:28:50,142 - Epoch: [9][   16/   16]    objective_loss 0.733276    Top1 79.245283    Top5 100.000000    LR 0.000563    
2022-08-01 15:28:50,178 - --- validate (epoch=9)-----------
2022-08-01 15:28:50,179 - 55 samples (32 per mini-batch)
2022-08-01 15:28:50,430 - Epoch: [9][    2/    2]    Loss 0.780793    Top1 78.181818    Top5 98.181818    
2022-08-01 15:28:50,465 - ==> Top1: 78.182    Top5: 98.182    Loss: 0.781

2022-08-01 15:28:50,466 - ==> Confusion:
[[ 4  0  0  0  0  1]
 [ 0 14  0  0  0  0]
 [ 1  0  3  0  0  1]
 [ 0  0  2  9  0  1]
 [ 3  0  0  0  8  0]
 [ 0  1  0  1  1  5]]

2022-08-01 15:28:50,560 - ==> Best [Top1: 80.000   Top5: 98.182  on epoch: 5]
2022-08-01 15:28:50,570 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:28:50,594 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:28:51,514 - Epoch: [10][   16/   16]    objective_loss 0.672606    Top1 79.245283    Top5 96.226415    LR 0.000563    
2022-08-01 15:28:51,558 - --- validate (epoch=10)-----------
2022-08-01 15:28:51,560 - 55 samples (32 per mini-batch)
2022-08-01 15:28:51,837 - Epoch: [10][    2/    2]    Loss 0.728868    Top1 78.181818    Top5 100.000000    
2022-08-01 15:28:51,872 - ==> Top1: 78.182    Top5: 100.000    Loss: 0.729

2022-08-01 15:28:51,873 - ==> Confusion:
[[ 4  0  0  0  0  1]
 [ 0 14  0  0  0  0]
 [ 0  0  3  1  0  1]
 [ 0  0  2 10  0  0]
 [ 1  0  0  0  8  2]
 [ 0  1  1  1  1  4]]

2022-08-01 15:28:52,100 - ==> Best [Top1: 80.000   Top5: 98.182  on epoch: 5]
2022-08-01 15:28:52,110 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:28:52,124 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:28:53,089 - Epoch: [11][   16/   16]    objective_loss 0.674404    Top1 77.358491    Top5 100.000000    LR 0.000563    
2022-08-01 15:28:53,127 - --- validate (epoch=11)-----------
2022-08-01 15:28:53,128 - 55 samples (32 per mini-batch)
2022-08-01 15:28:53,385 - Epoch: [11][    2/    2]    Loss 0.745600    Top1 81.818182    Top5 98.181818    
2022-08-01 15:28:53,433 - ==> Top1: 81.818    Top5: 98.182    Loss: 0.746

2022-08-01 15:28:53,434 - ==> Confusion:
[[ 4  0  0  0  0  1]
 [ 0 14  0  0  0  0]
 [ 1  0  3  0  0  1]
 [ 0  0  1 11  0  0]
 [ 3  0  0  0  8  0]
 [ 0  0  0  2  1  5]]

2022-08-01 15:28:53,532 - ==> Best [Top1: 81.818   Top5: 98.182  on epoch: 11]
2022-08-01 15:28:53,538 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:28:53,568 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:28:54,582 - Epoch: [12][   16/   16]    objective_loss 0.629028    Top1 88.679245    Top5 100.000000    LR 0.000563    
2022-08-01 15:28:54,620 - --- validate (epoch=12)-----------
2022-08-01 15:28:54,620 - 55 samples (32 per mini-batch)
2022-08-01 15:28:54,879 - Epoch: [12][    2/    2]    Loss 0.747460    Top1 78.181818    Top5 98.181818    
2022-08-01 15:28:54,915 - ==> Top1: 78.182    Top5: 98.182    Loss: 0.747

2022-08-01 15:28:54,916 - ==> Confusion:
[[ 3  0  0  0  0  2]
 [ 0 14  0  0  0  0]
 [ 0  0  3  0  0  2]
 [ 0  0  1 11  0  0]
 [ 1  0  0  0  7  3]
 [ 0  0  0  2  1  5]]

2022-08-01 15:28:54,990 - ==> Best [Top1: 81.818   Top5: 98.182  on epoch: 11]
2022-08-01 15:28:54,997 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:28:55,018 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:28:56,000 - Epoch: [13][   16/   16]    objective_loss 0.604515    Top1 88.679245    Top5 100.000000    LR 0.000563    
2022-08-01 15:28:56,039 - --- validate (epoch=13)-----------
2022-08-01 15:28:56,040 - 55 samples (32 per mini-batch)
2022-08-01 15:28:56,327 - Epoch: [13][    2/    2]    Loss 0.840690    Top1 70.909091    Top5 100.000000    
2022-08-01 15:28:56,362 - ==> Top1: 70.909    Top5: 100.000    Loss: 0.841

2022-08-01 15:28:56,363 - ==> Confusion:
[[ 2  0  0  1  0  2]
 [ 0 14  0  0  0  0]
 [ 0  0  5  0  0  0]
 [ 0  0  6  6  0  0]
 [ 0  0  0  0  8  3]
 [ 0  0  1  2  1  4]]

2022-08-01 15:28:56,458 - ==> Best [Top1: 81.818   Top5: 98.182  on epoch: 11]
2022-08-01 15:28:56,468 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:28:56,493 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:28:57,460 - Epoch: [14][   16/   16]    objective_loss 0.638062    Top1 83.018868    Top5 98.113208    LR 0.000563    
2022-08-01 15:28:57,497 - --- validate (epoch=14)-----------
2022-08-01 15:28:57,497 - 55 samples (32 per mini-batch)
2022-08-01 15:28:57,781 - Epoch: [14][    2/    2]    Loss 0.812263    Top1 74.545455    Top5 98.181818    
2022-08-01 15:28:57,818 - ==> Top1: 74.545    Top5: 98.182    Loss: 0.812

2022-08-01 15:28:57,819 - ==> Confusion:
[[ 4  0  0  0  0  1]
 [ 0 13  0  1  0  0]
 [ 0  0  3  0  0  2]
 [ 0  0  3  9  0  0]
 [ 1  0  0  0  7  3]
 [ 0  1  0  1  1  5]]

2022-08-01 15:28:57,913 - ==> Best [Top1: 81.818   Top5: 98.182  on epoch: 11]
2022-08-01 15:28:57,923 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:28:57,948 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:28:58,882 - Epoch: [15][   16/   16]    objective_loss 0.537687    Top1 84.905660    Top5 100.000000    LR 0.000563    
2022-08-01 15:28:58,919 - --- validate (epoch=15)-----------
2022-08-01 15:28:58,920 - 55 samples (32 per mini-batch)
2022-08-01 15:28:59,179 - Epoch: [15][    2/    2]    Loss 0.777499    Top1 81.818182    Top5 98.181818    
2022-08-01 15:28:59,225 - ==> Top1: 81.818    Top5: 98.182    Loss: 0.777

2022-08-01 15:28:59,227 - ==> Confusion:
[[ 4  0  0  0  0  1]
 [ 0 14  0  0  0  0]
 [ 1  0  3  0  0  1]
 [ 0  0  1 11  0  0]
 [ 3  0  0  0  7  1]
 [ 0  1  0  0  1  6]]

2022-08-01 15:28:59,303 - ==> Best [Top1: 81.818   Top5: 98.182  on epoch: 15]
2022-08-01 15:28:59,312 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:28:59,340 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:29:00,304 - Epoch: [16][   16/   16]    objective_loss 0.602725    Top1 90.566038    Top5 100.000000    LR 0.000563    
2022-08-01 15:29:00,341 - --- validate (epoch=16)-----------
2022-08-01 15:29:00,342 - 55 samples (32 per mini-batch)
2022-08-01 15:29:00,623 - Epoch: [16][    2/    2]    Loss 0.811326    Top1 74.545455    Top5 98.181818    
2022-08-01 15:29:00,659 - ==> Top1: 74.545    Top5: 98.182    Loss: 0.811

2022-08-01 15:29:00,660 - ==> Confusion:
[[ 4  0  0  0  0  1]
 [ 0 14  0  0  0  0]
 [ 1  0  3  0  0  1]
 [ 1  0  1  7  0  3]
 [ 3  0  0  0  8  0]
 [ 2  0  0  0  1  5]]

2022-08-01 15:29:00,757 - ==> Best [Top1: 81.818   Top5: 98.182  on epoch: 15]
2022-08-01 15:29:00,766 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:29:00,790 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:29:01,786 - Epoch: [17][   16/   16]    objective_loss 0.572078    Top1 84.905660    Top5 100.000000    LR 0.000563    
2022-08-01 15:29:01,823 - --- validate (epoch=17)-----------
2022-08-01 15:29:01,824 - 55 samples (32 per mini-batch)
2022-08-01 15:29:02,081 - Epoch: [17][    2/    2]    Loss 0.616756    Top1 81.818182    Top5 98.181818    
2022-08-01 15:29:02,117 - ==> Top1: 81.818    Top5: 98.182    Loss: 0.617

2022-08-01 15:29:02,117 - ==> Confusion:
[[ 4  0  0  0  0  1]
 [ 0 14  0  0  0  0]
 [ 0  0  3  0  0  2]
 [ 0  0  1 11  0  0]
 [ 1  0  0  0  7  3]
 [ 1  0  0  0  1  6]]

2022-08-01 15:29:02,194 - ==> Best [Top1: 81.818   Top5: 98.182  on epoch: 17]
2022-08-01 15:29:02,199 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:29:02,221 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:29:03,300 - Epoch: [18][   16/   16]    objective_loss 0.489176    Top1 90.566038    Top5 100.000000    LR 0.000563    
2022-08-01 15:29:03,337 - --- validate (epoch=18)-----------
2022-08-01 15:29:03,338 - 55 samples (32 per mini-batch)
2022-08-01 15:29:03,596 - Epoch: [18][    2/    2]    Loss 0.632905    Top1 80.000000    Top5 100.000000    
2022-08-01 15:29:03,632 - ==> Top1: 80.000    Top5: 100.000    Loss: 0.633

2022-08-01 15:29:03,632 - ==> Confusion:
[[ 4  0  0  1  0  0]
 [ 0 14  0  0  0  0]
 [ 0  0  3  1  0  1]
 [ 0  0  1 11  0  0]
 [ 1  0  0  0  7  3]
 [ 0  1  0  1  1  5]]

2022-08-01 15:29:03,710 - ==> Best [Top1: 81.818   Top5: 98.182  on epoch: 17]
2022-08-01 15:29:03,714 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:29:03,727 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:29:04,744 - Epoch: [19][   16/   16]    objective_loss 0.476990    Top1 92.452830    Top5 100.000000    LR 0.000563    
2022-08-01 15:29:04,783 - --- validate (epoch=19)-----------
2022-08-01 15:29:04,784 - 55 samples (32 per mini-batch)
2022-08-01 15:29:05,053 - Epoch: [19][    2/    2]    Loss 0.627086    Top1 80.000000    Top5 100.000000    
2022-08-01 15:29:05,089 - ==> Top1: 80.000    Top5: 100.000    Loss: 0.627

2022-08-01 15:29:05,089 - ==> Confusion:
[[ 4  0  0  1  0  0]
 [ 0 14  0  0  0  0]
 [ 0  0  3  1  0  1]
 [ 0  0  1 11  0  0]
 [ 1  0  0  0  7  3]
 [ 0  0  0  2  1  5]]

2022-08-01 15:29:05,189 - ==> Best [Top1: 81.818   Top5: 98.182  on epoch: 17]
2022-08-01 15:29:05,198 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:29:05,221 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:29:06,356 - Epoch: [20][   16/   16]    objective_loss 0.397705    Top1 90.566038    Top5 98.113208    LR 0.000422    
2022-08-01 15:29:06,394 - --- validate (epoch=20)-----------
2022-08-01 15:29:06,394 - 55 samples (32 per mini-batch)
2022-08-01 15:29:06,651 - Epoch: [20][    2/    2]    Loss 0.700900    Top1 80.000000    Top5 100.000000    
2022-08-01 15:29:06,697 - ==> Top1: 80.000    Top5: 100.000    Loss: 0.701

2022-08-01 15:29:06,698 - ==> Confusion:
[[ 4  0  0  1  0  0]
 [ 0 14  0  0  0  0]
 [ 0  1  3  0  0  1]
 [ 0  0  1 11  0  0]
 [ 1  0  0  0  7  3]
 [ 0  1  0  1  1  5]]

2022-08-01 15:29:06,790 - ==> Best [Top1: 81.818   Top5: 98.182  on epoch: 17]
2022-08-01 15:29:06,800 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:29:06,816 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:29:07,873 - Epoch: [21][   16/   16]    objective_loss 0.448539    Top1 88.679245    Top5 100.000000    LR 0.000422    
2022-08-01 15:29:07,910 - --- validate (epoch=21)-----------
2022-08-01 15:29:07,911 - 55 samples (32 per mini-batch)
2022-08-01 15:29:08,170 - Epoch: [21][    2/    2]    Loss 0.725898    Top1 81.818182    Top5 100.000000    
2022-08-01 15:29:08,204 - ==> Top1: 81.818    Top5: 100.000    Loss: 0.726

2022-08-01 15:29:08,205 - ==> Confusion:
[[ 4  0  0  1  0  0]
 [ 0 14  0  0  0  0]
 [ 0  0  3  1  0  1]
 [ 0  0  1 11  0  0]
 [ 3  0  0  0  8  0]
 [ 0  1  0  1  1  5]]

2022-08-01 15:29:08,283 - ==> Best [Top1: 81.818   Top5: 100.000  on epoch: 21]
2022-08-01 15:29:08,287 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:29:08,309 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:29:09,287 - Epoch: [22][   16/   16]    objective_loss 0.406072    Top1 90.566038    Top5 100.000000    LR 0.000422    
2022-08-01 15:29:09,326 - --- validate (epoch=22)-----------
2022-08-01 15:29:09,326 - 55 samples (32 per mini-batch)
2022-08-01 15:29:09,608 - Epoch: [22][    2/    2]    Loss 0.658730    Top1 78.181818    Top5 100.000000    
2022-08-01 15:29:09,643 - ==> Top1: 78.182    Top5: 100.000    Loss: 0.659

2022-08-01 15:29:09,644 - ==> Confusion:
[[ 3  0  0  0  0  2]
 [ 0 14  0  0  0  0]
 [ 0  0  3  1  0  1]
 [ 0  0  2 10  0  0]
 [ 0  0  0  0  7  4]
 [ 0  0  0  1  1  6]]

2022-08-01 15:29:09,721 - ==> Best [Top1: 81.818   Top5: 100.000  on epoch: 21]
2022-08-01 15:29:09,726 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:29:09,752 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:29:10,729 - Epoch: [23][   16/   16]    objective_loss 0.405870    Top1 81.132075    Top5 98.113208    LR 0.000422    
2022-08-01 15:29:10,766 - --- validate (epoch=23)-----------
2022-08-01 15:29:10,766 - 55 samples (32 per mini-batch)
2022-08-01 15:29:11,044 - Epoch: [23][    2/    2]    Loss 0.580708    Top1 83.636364    Top5 100.000000    
2022-08-01 15:29:11,079 - ==> Top1: 83.636    Top5: 100.000    Loss: 0.581

2022-08-01 15:29:11,080 - ==> Confusion:
[[ 4  0  0  1  0  0]
 [ 0 14  0  0  0  0]
 [ 0  0  3  1  0  1]
 [ 0  0  1 11  0  0]
 [ 0  0  0  0  9  2]
 [ 0  1  0  1  1  5]]

2022-08-01 15:29:11,174 - ==> Best [Top1: 83.636   Top5: 100.000  on epoch: 23]
2022-08-01 15:29:11,183 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:29:11,210 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:29:12,157 - Epoch: [24][   16/   16]    objective_loss 0.395261    Top1 94.339623    Top5 100.000000    LR 0.000422    
2022-08-01 15:29:12,194 - --- validate (epoch=24)-----------
2022-08-01 15:29:12,195 - 55 samples (32 per mini-batch)
2022-08-01 15:29:12,474 - Epoch: [24][    2/    2]    Loss 0.677323    Top1 78.181818    Top5 100.000000    
2022-08-01 15:29:12,511 - ==> Top1: 78.182    Top5: 100.000    Loss: 0.677

2022-08-01 15:29:12,512 - ==> Confusion:
[[ 4  0  0  1  0  0]
 [ 0 13  0  1  0  0]
 [ 0  0  3  1  0  1]
 [ 0  0  1 11  0  0]
 [ 3  0  0  0  7  1]
 [ 0  0  0  2  1  5]]

2022-08-01 15:29:12,609 - ==> Best [Top1: 83.636   Top5: 100.000  on epoch: 23]
2022-08-01 15:29:12,618 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:29:12,632 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:29:13,737 - Epoch: [25][   16/   16]    objective_loss 0.383276    Top1 94.339623    Top5 100.000000    LR 0.000422    
2022-08-01 15:29:13,774 - --- validate (epoch=25)-----------
2022-08-01 15:29:13,775 - 55 samples (32 per mini-batch)
2022-08-01 15:29:14,037 - Epoch: [25][    2/    2]    Loss 0.592941    Top1 81.818182    Top5 100.000000    
2022-08-01 15:29:14,072 - ==> Top1: 81.818    Top5: 100.000    Loss: 0.593

2022-08-01 15:29:14,073 - ==> Confusion:
[[ 4  0  0  1  0  0]
 [ 0 14  0  0  0  0]
 [ 0  0  3  1  0  1]
 [ 0  0  1 11  0  0]
 [ 2  0  0  0  8  1]
 [ 0  0  0  2  1  5]]

2022-08-01 15:29:14,169 - ==> Best [Top1: 83.636   Top5: 100.000  on epoch: 23]
2022-08-01 15:29:14,178 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:29:14,203 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:29:15,270 - Epoch: [26][   16/   16]    objective_loss 0.323885    Top1 94.339623    Top5 100.000000    LR 0.000422    
2022-08-01 15:29:15,309 - --- validate (epoch=26)-----------
2022-08-01 15:29:15,311 - 55 samples (32 per mini-batch)
2022-08-01 15:29:15,570 - Epoch: [26][    2/    2]    Loss 0.561689    Top1 83.636364    Top5 100.000000    
2022-08-01 15:29:15,605 - ==> Top1: 83.636    Top5: 100.000    Loss: 0.562

2022-08-01 15:29:15,605 - ==> Confusion:
[[ 4  0  0  0  0  1]
 [ 0 14  0  0  0  0]
 [ 0  0  3  0  0  2]
 [ 0  0  1 11  0  0]
 [ 1  0  0  0  8  2]
 [ 0  0  0  1  1  6]]

2022-08-01 15:29:15,686 - ==> Best [Top1: 83.636   Top5: 100.000  on epoch: 26]
2022-08-01 15:29:15,695 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:29:15,723 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:29:16,797 - Epoch: [27][   16/   16]    objective_loss 0.348084    Top1 90.566038    Top5 98.113208    LR 0.000422    
2022-08-01 15:29:16,836 - --- validate (epoch=27)-----------
2022-08-01 15:29:16,837 - 55 samples (32 per mini-batch)
2022-08-01 15:29:17,134 - Epoch: [27][    2/    2]    Loss 0.486281    Top1 85.454545    Top5 100.000000    
2022-08-01 15:29:17,173 - ==> Top1: 85.455    Top5: 100.000    Loss: 0.486

2022-08-01 15:29:17,180 - ==> Confusion:
[[ 3  0  0  0  1  1]
 [ 0 14  0  0  0  0]
 [ 0  0  3  0  0  2]
 [ 0  0  1 11  0  0]
 [ 1  0  0  0  9  1]
 [ 0  0  0  0  1  7]]

2022-08-01 15:29:17,279 - ==> Best [Top1: 85.455   Top5: 100.000  on epoch: 27]
2022-08-01 15:29:17,288 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:29:17,307 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:29:18,261 - Epoch: [28][   16/   16]    objective_loss 0.338521    Top1 86.792453    Top5 100.000000    LR 0.000422    
2022-08-01 15:29:18,302 - --- validate (epoch=28)-----------
2022-08-01 15:29:18,303 - 55 samples (32 per mini-batch)
2022-08-01 15:29:18,586 - Epoch: [28][    2/    2]    Loss 0.577991    Top1 83.636364    Top5 100.000000    
2022-08-01 15:29:18,623 - ==> Top1: 83.636    Top5: 100.000    Loss: 0.578

2022-08-01 15:29:18,624 - ==> Confusion:
[[ 4  0  0  0  0  1]
 [ 0 14  0  0  0  0]
 [ 0  0  3  0  0  2]
 [ 0  0  1 11  0  0]
 [ 3  0  0  0  8  0]
 [ 0  0  0  1  1  6]]

2022-08-01 15:29:18,717 - ==> Best [Top1: 85.455   Top5: 100.000  on epoch: 27]
2022-08-01 15:29:18,726 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:29:18,751 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:29:19,728 - Epoch: [29][   16/   16]    objective_loss 0.318954    Top1 96.226415    Top5 100.000000    LR 0.000422    
2022-08-01 15:29:19,765 - --- validate (epoch=29)-----------
2022-08-01 15:29:19,766 - 55 samples (32 per mini-batch)
2022-08-01 15:29:20,044 - Epoch: [29][    2/    2]    Loss 0.543655    Top1 83.636364    Top5 98.181818    
2022-08-01 15:29:20,081 - ==> Top1: 83.636    Top5: 98.182    Loss: 0.544

2022-08-01 15:29:20,082 - ==> Confusion:
[[ 4  0  0  1  0  0]
 [ 0 14  0  0  0  0]
 [ 0  0  3  0  0  2]
 [ 0  0  1 11  0  0]
 [ 2  0  0  0  8  1]
 [ 0  1  0  0  1  6]]

2022-08-01 15:29:20,159 - ==> Best [Top1: 85.455   Top5: 100.000  on epoch: 27]
2022-08-01 15:29:20,164 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:29:20,186 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:29:21,158 - Epoch: [30][   16/   16]    objective_loss 0.318770    Top1 94.339623    Top5 100.000000    LR 0.000422    
2022-08-01 15:29:21,196 - --- validate (epoch=30)-----------
2022-08-01 15:29:21,197 - 55 samples (32 per mini-batch)
2022-08-01 15:29:21,478 - Epoch: [30][    2/    2]    Loss 0.659880    Top1 80.000000    Top5 100.000000    
2022-08-01 15:29:21,514 - ==> Top1: 80.000    Top5: 100.000    Loss: 0.660

2022-08-01 15:29:21,515 - ==> Confusion:
[[ 4  0  0  1  0  0]
 [ 0 14  0  0  0  0]
 [ 0  0  3  1  0  1]
 [ 0  0  1 11  0  0]
 [ 1  0  0  0  7  3]
 [ 0  0  0  2  1  5]]

2022-08-01 15:29:21,612 - ==> Best [Top1: 85.455   Top5: 100.000  on epoch: 27]
2022-08-01 15:29:21,621 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:29:21,644 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:29:22,584 - Epoch: [31][   16/   16]    objective_loss 0.326706    Top1 96.226415    Top5 100.000000    LR 0.000422    
2022-08-01 15:29:22,621 - --- validate (epoch=31)-----------
2022-08-01 15:29:22,622 - 55 samples (32 per mini-batch)
2022-08-01 15:29:22,874 - Epoch: [31][    2/    2]    Loss 0.662732    Top1 81.818182    Top5 100.000000    
2022-08-01 15:29:22,909 - ==> Top1: 81.818    Top5: 100.000    Loss: 0.663

2022-08-01 15:29:22,910 - ==> Confusion:
[[ 4  0  0  1  0  0]
 [ 0 14  0  0  0  0]
 [ 0  0  3  0  0  2]
 [ 0  0  1 11  0  0]
 [ 3  0  0  0  7  1]
 [ 0  0  0  1  1  6]]

2022-08-01 15:29:23,006 - ==> Best [Top1: 85.455   Top5: 100.000  on epoch: 27]
2022-08-01 15:29:23,016 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:29:23,029 - Training time: 0:00:46.899880
2022-08-01 15:29:47,965 - Dataset sizes:
	training=501
	validation=55
	test=64
2022-08-01 15:29:47,966 - Augmentations:Compose(
    Resize(size=(128, 128), interpolation=bilinear)
    ColorJitter(brightness=(0.85, 1.15), contrast=(0.75, 1.25), saturation=(0.75, 1.25), hue=(-0.4, 0.4))
    RandomGrayscale(p=0.15)
    RandomAffine(degrees=[-10.0, 10.0], translate=(0.27, 0.27))
    RandomHorizontalFlip(p=0.5)
    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 1.5))
    ToTensor()
    <ai8x.normalize object at 0x7fb3d20ceb80>
)
Augmentation Seed:2050055943
2022-08-01 15:29:51,085 - => loading checkpoint jupyter_logging/SSL___2022.07.15-171757/classifierbackbonenet_qat_checkpoint.pth.tar
2022-08-01 15:29:51,090 - => Checkpoint contents:
+----------------------+-------------+---------------------------+
| Key                  | Type        | Value                     |
|----------------------+-------------+---------------------------|
| arch                 | str         | classifierbackbonenet_qat |
| compression_sched    | dict        |                           |
| epoch                | int         | 99                        |
| extras               | dict        |                           |
| optimizer_state_dict | dict        |                           |
| optimizer_type       | type        | Adam                      |
| state_dict           | OrderedDict |                           |
+----------------------+-------------+---------------------------+

2022-08-01 15:29:51,091 - => Checkpoint['extras'] contents:
+-------+--------+---------+
| Key   | Type   |   Value |
|-------+--------+---------|
| epoch | int    |      99 |
+-------+--------+---------+

2022-08-01 15:29:51,091 - Loaded compression schedule from checkpoint (epoch 99)
2022-08-01 15:29:51,095 - => loaded 'state_dict' from checkpoint 'jupyter_logging/SSL___2022.07.15-171757/classifierbackbonenet_qat_checkpoint.pth.tar'
2022-08-01 15:29:51,106 - model: OfficeClassifier(
  (feature_extractor): ClassifierBackbone(
    (conv1): FusedConv2dReLU(
      (activate): ReLU(inplace=True)
      (op): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv2): FusedConv2dReLU(
      (activate): ReLU(inplace=True)
      (op): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv3): FusedMaxPoolConv2dReLU(
      (activate): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (op): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv4): FusedConv2dBNReLU(
      (activate): ReLU(inplace=True)
      (op): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): None
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv5): FusedMaxPoolConv2dBNReLU(
      (activate): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (op): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): None
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv6): FusedConv2dBNReLU(
      (activate): ReLU(inplace=True)
      (op): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): None
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv7): FusedMaxPoolConv2dBNReLU(
      (activate): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (op): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): None
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv8): FusedConv2dBNReLU(
      (activate): ReLU(inplace=True)
      (op): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): None
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv9): FusedMaxPoolConv2dBNReLU(
      (activate): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (op): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): None
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv10): FusedMaxPoolConv2dBNReLU(
      (activate): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (op): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): None
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (fc1): FusedLinearReLU(
      (activate): ReLU(inplace=True)
      (op): Linear(in_features=1024, out_features=128, bias=True)
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (do1): Dropout(p=0.5, inplace=False)
    (fc2): FusedLinearReLU(
      (activate): ReLU(inplace=True)
      (op): Linear(in_features=128, out_features=64, bias=True)
      (calc_out_shift): OutputShiftSqueeze()
      (calc_weight_scale): One()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Empty()
      (quantize_bias): Empty()
      (clamp_weight): Empty()
      (clamp_bias): Empty()
      (quantize): Empty()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (fc3): Linear(
      (activate): Empty()
      (op): Linear(in_features=64, out_features=6, bias=True)
      (calc_out_shift): OutputShiftSqueeze()
      (calc_weight_scale): One()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Empty()
      (quantize_bias): Empty()
      (clamp_weight): Empty()
      (clamp_bias): Empty()
      (quantize): Empty()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
  )
  (do1): Dropout(p=0.25, inplace=False)
)
2022-08-01 15:29:51,150 - Number of Model Params: 287278
2022-08-01 15:29:52,094 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2022-08-01 15:29:52,097 - lr_schedule:base: [0.001] milestones: Counter({4: 1, 8: 1, 20: 1, 100: 1}) gamma: 0.75
2022-08-01 15:29:52,868 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:29:53,824 - Epoch: [0][   16/   16]    objective_loss 1.686109    Top1 49.056604    Top5 90.566038    LR 0.001000    
2022-08-01 15:29:53,864 - --- validate (epoch=0)-----------
2022-08-01 15:29:53,864 - 55 samples (32 per mini-batch)
2022-08-01 15:29:54,055 - Epoch: [0][    2/    2]    Loss 1.300715    Top1 38.181818    Top5 100.000000    
2022-08-01 15:29:54,090 - ==> Top1: 38.182    Top5: 100.000    Loss: 1.301

2022-08-01 15:29:54,091 - ==> Confusion:
[[ 0  3 10  0  0  0]
 [ 0 16  0  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  1  5  1  0  0]
 [ 0  0 10  0  0  0]
 [ 0  0  5  0  0  0]]

2022-08-01 15:29:54,312 - ==> Best [Top1: 38.182   Top5: 100.000  on epoch: 0]
2022-08-01 15:29:54,322 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:29:54,337 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:29:55,362 - Epoch: [1][   16/   16]    objective_loss 1.364421    Top1 54.716981    Top5 96.226415    LR 0.001000    
2022-08-01 15:29:55,400 - --- validate (epoch=1)-----------
2022-08-01 15:29:55,401 - 55 samples (32 per mini-batch)
2022-08-01 15:29:55,626 - Epoch: [1][    2/    2]    Loss 1.158361    Top1 56.363636    Top5 100.000000    
2022-08-01 15:29:55,664 - ==> Top1: 56.364    Top5: 100.000    Loss: 1.158

2022-08-01 15:29:55,664 - ==> Confusion:
[[ 3  0  7  0  1  2]
 [ 0 16  0  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  0  6  0  0  1]
 [ 0  0  1  0  7  2]
 [ 0  0  3  0  1  1]]

2022-08-01 15:29:55,758 - ==> Best [Top1: 56.364   Top5: 100.000  on epoch: 1]
2022-08-01 15:29:55,768 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:29:55,785 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:29:56,777 - Epoch: [2][   16/   16]    objective_loss 1.191256    Top1 69.811321    Top5 100.000000    LR 0.001000    
2022-08-01 15:29:56,816 - --- validate (epoch=2)-----------
2022-08-01 15:29:56,817 - 55 samples (32 per mini-batch)
2022-08-01 15:29:57,014 - Epoch: [2][    2/    2]    Loss 0.944226    Top1 60.000000    Top5 100.000000    
2022-08-01 15:29:57,049 - ==> Top1: 60.000    Top5: 100.000    Loss: 0.944

2022-08-01 15:29:57,050 - ==> Confusion:
[[ 1  0  0  2  9  1]
 [ 0 16  0  0  0  0]
 [ 0  0  3  0  1  0]
 [ 0  0  0  3  3  1]
 [ 0  0  0  0 10  0]
 [ 0  0  1  0  4  0]]

2022-08-01 15:29:57,146 - ==> Best [Top1: 60.000   Top5: 100.000  on epoch: 2]
2022-08-01 15:29:57,154 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:29:57,172 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:29:58,211 - Epoch: [3][   16/   16]    objective_loss 1.112486    Top1 56.603774    Top5 100.000000    LR 0.001000    
2022-08-01 15:29:58,248 - --- validate (epoch=3)-----------
2022-08-01 15:29:58,249 - 55 samples (32 per mini-batch)
2022-08-01 15:29:58,472 - Epoch: [3][    2/    2]    Loss 0.758425    Top1 80.000000    Top5 100.000000    
2022-08-01 15:29:58,508 - ==> Top1: 80.000    Top5: 100.000    Loss: 0.758

2022-08-01 15:29:58,509 - ==> Confusion:
[[ 8  0  0  3  1  1]
 [ 0 16  0  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  0  0  7  0  0]
 [ 0  0  0  0  8  2]
 [ 0  0  2  0  2  1]]

2022-08-01 15:29:58,601 - ==> Best [Top1: 80.000   Top5: 100.000  on epoch: 3]
2022-08-01 15:29:58,610 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_checkpoint.pth.tar
2022-08-01 15:29:58,648 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:29:59,686 - Epoch: [4][   16/   16]    objective_loss 0.915575    Top1 75.471698    Top5 98.113208    LR 0.000750    
2022-08-01 15:29:59,725 - --- validate (epoch=4)-----------
2022-08-01 15:29:59,725 - 55 samples (32 per mini-batch)
2022-08-01 15:29:59,949 - Epoch: [4][    2/    2]    Loss 0.617744    Top1 81.818182    Top5 100.000000    
2022-08-01 15:29:59,984 - ==> Top1: 81.818    Top5: 100.000    Loss: 0.618

2022-08-01 15:29:59,985 - ==> Confusion:
[[ 8  0  0  3  2  0]
 [ 0 16  0  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  0  0  6  0  1]
 [ 0  0  0  0 10  0]
 [ 0  0  0  1  3  1]]

2022-08-01 15:30:00,068 - ==> Best [Top1: 81.818   Top5: 100.000  on epoch: 4]
2022-08-01 15:30:00,077 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_qat_checkpoint.pth.tar
2022-08-01 15:30:00,095 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:30:01,113 - Epoch: [5][   16/   16]    objective_loss 0.854458    Top1 71.698113    Top5 100.000000    LR 0.000750    
2022-08-01 15:30:01,151 - --- validate (epoch=5)-----------
2022-08-01 15:30:01,151 - 55 samples (32 per mini-batch)
2022-08-01 15:30:01,384 - Epoch: [5][    2/    2]    Loss 0.615735    Top1 78.181818    Top5 100.000000    
2022-08-01 15:30:01,419 - ==> Top1: 78.182    Top5: 100.000    Loss: 0.616

2022-08-01 15:30:01,420 - ==> Confusion:
[[ 8  0  0  2  1  2]
 [ 0 15  1  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  0  0  6  0  1]
 [ 0  0  0  0 10  0]
 [ 0  0  1  1  3  0]]

2022-08-01 15:30:01,514 - ==> Best [Top1: 81.818   Top5: 100.000  on epoch: 4]
2022-08-01 15:30:01,524 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_qat_checkpoint.pth.tar
2022-08-01 15:30:01,541 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:30:02,501 - Epoch: [6][   16/   16]    objective_loss 0.787411    Top1 71.698113    Top5 98.113208    LR 0.000750    
2022-08-01 15:30:02,540 - --- validate (epoch=6)-----------
2022-08-01 15:30:02,541 - 55 samples (32 per mini-batch)
2022-08-01 15:30:02,733 - Epoch: [6][    2/    2]    Loss 0.529795    Top1 81.818182    Top5 100.000000    
2022-08-01 15:30:02,768 - ==> Top1: 81.818    Top5: 100.000    Loss: 0.530

2022-08-01 15:30:02,769 - ==> Confusion:
[[ 7  0  0  3  3  0]
 [ 0 16  0  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  0  0  7  0  0]
 [ 0  0  0  0 10  0]
 [ 0  0  0  2  2  1]]

2022-08-01 15:30:02,864 - ==> Best [Top1: 81.818   Top5: 100.000  on epoch: 6]
2022-08-01 15:30:02,873 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_qat_checkpoint.pth.tar
2022-08-01 15:30:02,901 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:30:03,907 - Epoch: [7][   16/   16]    objective_loss 0.710520    Top1 69.811321    Top5 100.000000    LR 0.000750    
2022-08-01 15:30:03,944 - --- validate (epoch=7)-----------
2022-08-01 15:30:03,945 - 55 samples (32 per mini-batch)
2022-08-01 15:30:04,161 - Epoch: [7][    2/    2]    Loss 0.533675    Top1 85.454545    Top5 100.000000    
2022-08-01 15:30:04,196 - ==> Top1: 85.455    Top5: 100.000    Loss: 0.534

2022-08-01 15:30:04,197 - ==> Confusion:
[[ 8  0  0  4  1  0]
 [ 0 16  0  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  0  0  7  0  0]
 [ 0  0  0  0 10  0]
 [ 0  0  1  1  1  2]]

2022-08-01 15:30:04,274 - ==> Best [Top1: 85.455   Top5: 100.000  on epoch: 7]
2022-08-01 15:30:04,278 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_qat_checkpoint.pth.tar
2022-08-01 15:30:04,307 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:30:05,283 - Epoch: [8][   16/   16]    objective_loss 0.602415    Top1 86.792453    Top5 98.113208    LR 0.000563    
2022-08-01 15:30:05,321 - --- validate (epoch=8)-----------
2022-08-01 15:30:05,321 - 55 samples (32 per mini-batch)
2022-08-01 15:30:05,518 - Epoch: [8][    2/    2]    Loss 0.378702    Top1 89.090909    Top5 100.000000    
2022-08-01 15:30:05,554 - ==> Top1: 89.091    Top5: 100.000    Loss: 0.379

2022-08-01 15:30:05,554 - ==> Confusion:
[[11  0  0  2  0  0]
 [ 0 16  0  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  0  0  7  0  0]
 [ 1  0  0  0  9  0]
 [ 0  0  0  2  1  2]]

2022-08-01 15:30:05,649 - ==> Best [Top1: 89.091   Top5: 100.000  on epoch: 8]
2022-08-01 15:30:05,659 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_qat_checkpoint.pth.tar
2022-08-01 15:30:05,686 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:30:06,810 - Epoch: [9][   16/   16]    objective_loss 0.613406    Top1 86.792453    Top5 100.000000    LR 0.000563    
2022-08-01 15:30:06,848 - --- validate (epoch=9)-----------
2022-08-01 15:30:06,849 - 55 samples (32 per mini-batch)
2022-08-01 15:30:07,048 - Epoch: [9][    2/    2]    Loss 0.437918    Top1 87.272727    Top5 100.000000    
2022-08-01 15:30:07,083 - ==> Top1: 87.273    Top5: 100.000    Loss: 0.438

2022-08-01 15:30:07,084 - ==> Confusion:
[[ 8  0  0  2  1  2]
 [ 0 16  0  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  0  0  7  0  0]
 [ 0  0  0  0 10  0]
 [ 0  0  0  2  0  3]]

2022-08-01 15:30:07,179 - ==> Best [Top1: 89.091   Top5: 100.000  on epoch: 8]
2022-08-01 15:30:07,188 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_qat_checkpoint.pth.tar
2022-08-01 15:30:07,214 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:30:08,260 - Epoch: [10][   16/   16]    objective_loss 0.533885    Top1 83.018868    Top5 100.000000    LR 0.000563    
2022-08-01 15:30:08,321 - --- validate (epoch=10)-----------
2022-08-01 15:30:08,322 - 55 samples (32 per mini-batch)
2022-08-01 15:30:08,544 - Epoch: [10][    2/    2]    Loss 0.505999    Top1 83.636364    Top5 100.000000    
2022-08-01 15:30:08,582 - ==> Top1: 83.636    Top5: 100.000    Loss: 0.506

2022-08-01 15:30:08,583 - ==> Confusion:
[[ 7  0  0  3  2  1]
 [ 0 16  0  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  0  0  7  0  0]
 [ 0  0  0  0 10  0]
 [ 0  0  0  2  1  2]]

2022-08-01 15:30:08,684 - ==> Best [Top1: 89.091   Top5: 100.000  on epoch: 8]
2022-08-01 15:30:08,690 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_qat_checkpoint.pth.tar
2022-08-01 15:30:08,715 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:30:09,791 - Epoch: [11][   16/   16]    objective_loss 0.554910    Top1 83.018868    Top5 98.113208    LR 0.000563    
2022-08-01 15:30:09,828 - --- validate (epoch=11)-----------
2022-08-01 15:30:09,829 - 55 samples (32 per mini-batch)
2022-08-01 15:30:10,048 - Epoch: [11][    2/    2]    Loss 0.417248    Top1 87.272727    Top5 100.000000    
2022-08-01 15:30:10,084 - ==> Top1: 87.273    Top5: 100.000    Loss: 0.417

2022-08-01 15:30:10,085 - ==> Confusion:
[[ 8  0  0  2  1  2]
 [ 0 16  0  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  0  1  5  0  1]
 [ 0  0  0  0 10  0]
 [ 0  0  0  0  0  5]]

2022-08-01 15:30:10,166 - ==> Best [Top1: 89.091   Top5: 100.000  on epoch: 8]
2022-08-01 15:30:10,175 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_qat_checkpoint.pth.tar
2022-08-01 15:30:10,201 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:30:11,123 - Epoch: [12][   16/   16]    objective_loss 0.457514    Top1 81.132075    Top5 98.113208    LR 0.000563    
2022-08-01 15:30:11,161 - --- validate (epoch=12)-----------
2022-08-01 15:30:11,161 - 55 samples (32 per mini-batch)
2022-08-01 15:30:11,355 - Epoch: [12][    2/    2]    Loss 0.329213    Top1 90.909091    Top5 100.000000    
2022-08-01 15:30:11,391 - ==> Top1: 90.909    Top5: 100.000    Loss: 0.329

2022-08-01 15:30:11,392 - ==> Confusion:
[[ 9  0  0  2  1  1]
 [ 0 16  0  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  0  0  7  0  0]
 [ 1  0  0  0  9  0]
 [ 0  0  0  0  0  5]]

2022-08-01 15:30:11,489 - ==> Best [Top1: 90.909   Top5: 100.000  on epoch: 12]
2022-08-01 15:30:11,498 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_qat_checkpoint.pth.tar
2022-08-01 15:30:11,523 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:30:12,628 - Epoch: [13][   16/   16]    objective_loss 0.446969    Top1 86.792453    Top5 100.000000    LR 0.000563    
2022-08-01 15:30:12,666 - --- validate (epoch=13)-----------
2022-08-01 15:30:12,667 - 55 samples (32 per mini-batch)
2022-08-01 15:30:12,886 - Epoch: [13][    2/    2]    Loss 0.421180    Top1 87.272727    Top5 100.000000    
2022-08-01 15:30:12,921 - ==> Top1: 87.273    Top5: 100.000    Loss: 0.421

2022-08-01 15:30:12,922 - ==> Confusion:
[[ 9  0  0  3  0  1]
 [ 0 16  0  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  0  1  6  0  0]
 [ 1  0  0  0  9  0]
 [ 0  0  0  1  0  4]]

2022-08-01 15:30:13,000 - ==> Best [Top1: 90.909   Top5: 100.000  on epoch: 12]
2022-08-01 15:30:13,004 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_qat_checkpoint.pth.tar
2022-08-01 15:30:13,027 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:30:14,208 - Epoch: [14][   16/   16]    objective_loss 0.475721    Top1 90.566038    Top5 100.000000    LR 0.000563    
2022-08-01 15:30:14,248 - --- validate (epoch=14)-----------
2022-08-01 15:30:14,248 - 55 samples (32 per mini-batch)
2022-08-01 15:30:14,471 - Epoch: [14][    2/    2]    Loss 0.953112    Top1 69.090909    Top5 98.181818    
2022-08-01 15:30:14,508 - ==> Top1: 69.091    Top5: 98.182    Loss: 0.953

2022-08-01 15:30:14,509 - ==> Confusion:
[[ 7  0  1  0  4  1]
 [ 0 14  2  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  0  5  2  0  0]
 [ 0  0  0  0 10  0]
 [ 0  0  2  0  2  1]]

2022-08-01 15:30:14,605 - ==> Best [Top1: 90.909   Top5: 100.000  on epoch: 12]
2022-08-01 15:30:14,615 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_qat_checkpoint.pth.tar
2022-08-01 15:30:14,638 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:30:15,702 - Epoch: [15][   16/   16]    objective_loss 0.689842    Top1 88.679245    Top5 100.000000    LR 0.000563    
2022-08-01 15:30:15,740 - --- validate (epoch=15)-----------
2022-08-01 15:30:15,741 - 55 samples (32 per mini-batch)
2022-08-01 15:30:15,931 - Epoch: [15][    2/    2]    Loss 0.711614    Top1 85.454545    Top5 100.000000    
2022-08-01 15:30:15,967 - ==> Top1: 85.455    Top5: 100.000    Loss: 0.712

2022-08-01 15:30:15,968 - ==> Confusion:
[[ 5  0  0  3  5  0]
 [ 0 16  0  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  0  0  7  0  0]
 [ 0  0  0  0 10  0]
 [ 0  0  0  0  0  5]]

2022-08-01 15:30:16,061 - ==> Best [Top1: 90.909   Top5: 100.000  on epoch: 12]
2022-08-01 15:30:16,071 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_qat_checkpoint.pth.tar
2022-08-01 15:30:16,095 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:30:17,105 - Epoch: [16][   16/   16]    objective_loss 0.619306    Top1 94.339623    Top5 100.000000    LR 0.000563    
2022-08-01 15:30:17,143 - --- validate (epoch=16)-----------
2022-08-01 15:30:17,144 - 55 samples (32 per mini-batch)
2022-08-01 15:30:17,336 - Epoch: [16][    2/    2]    Loss 0.600147    Top1 87.272727    Top5 100.000000    
2022-08-01 15:30:17,371 - ==> Top1: 87.273    Top5: 100.000    Loss: 0.600

2022-08-01 15:30:17,372 - ==> Confusion:
[[ 7  0  0  0  5  1]
 [ 0 16  0  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  0  0  6  0  1]
 [ 0  0  0  0 10  0]
 [ 0  0  0  0  0  5]]

2022-08-01 15:30:17,470 - ==> Best [Top1: 90.909   Top5: 100.000  on epoch: 12]
2022-08-01 15:30:17,479 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_qat_checkpoint.pth.tar
2022-08-01 15:30:17,493 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:30:18,448 - Epoch: [17][   16/   16]    objective_loss 0.601746    Top1 88.679245    Top5 100.000000    LR 0.000563    
2022-08-01 15:30:18,486 - --- validate (epoch=17)-----------
2022-08-01 15:30:18,487 - 55 samples (32 per mini-batch)
2022-08-01 15:30:18,682 - Epoch: [17][    2/    2]    Loss 0.438832    Top1 96.363636    Top5 100.000000    
2022-08-01 15:30:18,719 - ==> Top1: 96.364    Top5: 100.000    Loss: 0.439

2022-08-01 15:30:18,720 - ==> Confusion:
[[12  0  0  1  0  0]
 [ 0 16  0  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  0  0  7  0  0]
 [ 1  0  0  0  9  0]
 [ 0  0  0  0  0  5]]

2022-08-01 15:30:18,814 - ==> Best [Top1: 96.364   Top5: 100.000  on epoch: 17]
2022-08-01 15:30:18,823 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_qat_checkpoint.pth.tar
2022-08-01 15:30:18,849 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:30:19,877 - Epoch: [18][   16/   16]    objective_loss 0.563741    Top1 88.679245    Top5 100.000000    LR 0.000563    
2022-08-01 15:30:19,915 - --- validate (epoch=18)-----------
2022-08-01 15:30:19,915 - 55 samples (32 per mini-batch)
2022-08-01 15:30:20,114 - Epoch: [18][    2/    2]    Loss 0.466110    Top1 90.909091    Top5 100.000000    
2022-08-01 15:30:20,150 - ==> Top1: 90.909    Top5: 100.000    Loss: 0.466

2022-08-01 15:30:20,150 - ==> Confusion:
[[ 9  0  0  2  1  1]
 [ 0 16  0  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  0  0  7  0  0]
 [ 1  0  0  0  9  0]
 [ 0  0  0  0  0  5]]

2022-08-01 15:30:20,243 - ==> Best [Top1: 96.364   Top5: 100.000  on epoch: 17]
2022-08-01 15:30:20,252 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_qat_checkpoint.pth.tar
2022-08-01 15:30:20,278 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:30:21,323 - Epoch: [19][   16/   16]    objective_loss 0.516771    Top1 86.792453    Top5 100.000000    LR 0.000563    
2022-08-01 15:30:21,362 - --- validate (epoch=19)-----------
2022-08-01 15:30:21,363 - 55 samples (32 per mini-batch)
2022-08-01 15:30:21,557 - Epoch: [19][    2/    2]    Loss 0.402692    Top1 92.727273    Top5 100.000000    
2022-08-01 15:30:21,594 - ==> Top1: 92.727    Top5: 100.000    Loss: 0.403

2022-08-01 15:30:21,601 - ==> Confusion:
[[11  0  0  1  1  0]
 [ 0 16  0  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  0  1  6  0  0]
 [ 1  0  0  0  9  0]
 [ 0  0  0  0  0  5]]

2022-08-01 15:30:21,698 - ==> Best [Top1: 96.364   Top5: 100.000  on epoch: 17]
2022-08-01 15:30:21,707 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_qat_checkpoint.pth.tar
2022-08-01 15:30:21,722 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:30:22,780 - Epoch: [20][   16/   16]    objective_loss 0.501100    Top1 88.679245    Top5 100.000000    LR 0.000422    
2022-08-01 15:30:22,818 - --- validate (epoch=20)-----------
2022-08-01 15:30:22,818 - 55 samples (32 per mini-batch)
2022-08-01 15:30:23,035 - Epoch: [20][    2/    2]    Loss 0.397849    Top1 92.727273    Top5 100.000000    
2022-08-01 15:30:23,071 - ==> Top1: 92.727    Top5: 100.000    Loss: 0.398

2022-08-01 15:30:23,072 - ==> Confusion:
[[10  0  0  1  2  0]
 [ 0 16  0  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  0  1  6  0  0]
 [ 1  0  0  0  9  0]
 [ 0  0  0  0  0  5]]

2022-08-01 15:30:23,169 - ==> Best [Top1: 96.364   Top5: 100.000  on epoch: 17]
2022-08-01 15:30:23,178 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_qat_checkpoint.pth.tar
2022-08-01 15:30:23,194 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:30:24,184 - Epoch: [21][   16/   16]    objective_loss 0.457150    Top1 90.566038    Top5 100.000000    LR 0.000422    
2022-08-01 15:30:24,221 - --- validate (epoch=21)-----------
2022-08-01 15:30:24,221 - 55 samples (32 per mini-batch)
2022-08-01 15:30:24,433 - Epoch: [21][    2/    2]    Loss 0.425915    Top1 90.909091    Top5 100.000000    
2022-08-01 15:30:24,468 - ==> Top1: 90.909    Top5: 100.000    Loss: 0.426

2022-08-01 15:30:24,469 - ==> Confusion:
[[10  0  0  0  1  2]
 [ 0 16  0  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  0  1  6  0  0]
 [ 1  0  0  0  9  0]
 [ 0  0  0  0  0  5]]

2022-08-01 15:30:24,568 - ==> Best [Top1: 96.364   Top5: 100.000  on epoch: 17]
2022-08-01 15:30:24,577 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_qat_checkpoint.pth.tar
2022-08-01 15:30:24,600 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:30:25,623 - Epoch: [22][   16/   16]    objective_loss 0.449123    Top1 90.566038    Top5 98.113208    LR 0.000422    
2022-08-01 15:30:25,660 - --- validate (epoch=22)-----------
2022-08-01 15:30:25,661 - 55 samples (32 per mini-batch)
2022-08-01 15:30:25,853 - Epoch: [22][    2/    2]    Loss 0.385028    Top1 92.727273    Top5 100.000000    
2022-08-01 15:30:25,888 - ==> Top1: 92.727    Top5: 100.000    Loss: 0.385

2022-08-01 15:30:25,889 - ==> Confusion:
[[12  0  0  1  0  0]
 [ 0 16  0  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  0  1  6  0  0]
 [ 1  0  0  0  9  0]
 [ 0  0  1  0  0  4]]

2022-08-01 15:30:25,983 - ==> Best [Top1: 96.364   Top5: 100.000  on epoch: 17]
2022-08-01 15:30:25,992 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_qat_checkpoint.pth.tar
2022-08-01 15:30:26,015 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:30:27,054 - Epoch: [23][   16/   16]    objective_loss 0.423836    Top1 94.339623    Top5 100.000000    LR 0.000422    
2022-08-01 15:30:27,092 - --- validate (epoch=23)-----------
2022-08-01 15:30:27,093 - 55 samples (32 per mini-batch)
2022-08-01 15:30:27,317 - Epoch: [23][    2/    2]    Loss 0.462486    Top1 89.090909    Top5 100.000000    
2022-08-01 15:30:27,352 - ==> Top1: 89.091    Top5: 100.000    Loss: 0.462

2022-08-01 15:30:27,353 - ==> Confusion:
[[ 8  0  0  1  4  0]
 [ 0 16  0  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  0  0  7  0  0]
 [ 0  0  0  0 10  0]
 [ 0  0  1  0  0  4]]

2022-08-01 15:30:27,445 - ==> Best [Top1: 96.364   Top5: 100.000  on epoch: 17]
2022-08-01 15:30:27,455 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_qat_checkpoint.pth.tar
2022-08-01 15:30:27,471 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:30:28,462 - Epoch: [24][   16/   16]    objective_loss 0.482720    Top1 94.339623    Top5 100.000000    LR 0.000422    
2022-08-01 15:30:28,499 - --- validate (epoch=24)-----------
2022-08-01 15:30:28,500 - 55 samples (32 per mini-batch)
2022-08-01 15:30:28,712 - Epoch: [24][    2/    2]    Loss 0.465440    Top1 85.454545    Top5 100.000000    
2022-08-01 15:30:28,748 - ==> Top1: 85.455    Top5: 100.000    Loss: 0.465

2022-08-01 15:30:28,749 - ==> Confusion:
[[10  0  0  1  2  0]
 [ 0 16  0  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  0  1  6  0  0]
 [ 1  0  0  0  9  0]
 [ 0  0  2  0  1  2]]

2022-08-01 15:30:28,842 - ==> Best [Top1: 96.364   Top5: 100.000  on epoch: 17]
2022-08-01 15:30:28,852 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_qat_checkpoint.pth.tar
2022-08-01 15:30:28,868 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:30:29,804 - Epoch: [25][   16/   16]    objective_loss 0.424386    Top1 96.226415    Top5 100.000000    LR 0.000422    
2022-08-01 15:30:29,841 - --- validate (epoch=25)-----------
2022-08-01 15:30:29,842 - 55 samples (32 per mini-batch)
2022-08-01 15:30:30,059 - Epoch: [25][    2/    2]    Loss 0.345408    Top1 92.727273    Top5 100.000000    
2022-08-01 15:30:30,094 - ==> Top1: 92.727    Top5: 100.000    Loss: 0.345

2022-08-01 15:30:30,095 - ==> Confusion:
[[11  0  0  1  1  0]
 [ 0 16  0  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  0  0  7  0  0]
 [ 1  0  0  0  9  0]
 [ 0  0  1  0  0  4]]

2022-08-01 15:30:30,191 - ==> Best [Top1: 96.364   Top5: 100.000  on epoch: 17]
2022-08-01 15:30:30,200 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_qat_checkpoint.pth.tar
2022-08-01 15:30:30,223 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:30:31,255 - Epoch: [26][   16/   16]    objective_loss 0.407406    Top1 90.566038    Top5 100.000000    LR 0.000422    
2022-08-01 15:30:31,293 - --- validate (epoch=26)-----------
2022-08-01 15:30:31,294 - 55 samples (32 per mini-batch)
2022-08-01 15:30:31,508 - Epoch: [26][    2/    2]    Loss 0.297157    Top1 96.363636    Top5 100.000000    
2022-08-01 15:30:31,543 - ==> Top1: 96.364    Top5: 100.000    Loss: 0.297

2022-08-01 15:30:31,544 - ==> Confusion:
[[12  0  0  1  0  0]
 [ 0 16  0  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  0  0  7  0  0]
 [ 1  0  0  0  9  0]
 [ 0  0  0  0  0  5]]

2022-08-01 15:30:31,640 - ==> Best [Top1: 96.364   Top5: 100.000  on epoch: 26]
2022-08-01 15:30:31,648 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_qat_checkpoint.pth.tar
2022-08-01 15:30:31,674 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:30:32,650 - Epoch: [27][   16/   16]    objective_loss 0.399545    Top1 96.226415    Top5 98.113208    LR 0.000422    
2022-08-01 15:30:32,688 - --- validate (epoch=27)-----------
2022-08-01 15:30:32,688 - 55 samples (32 per mini-batch)
2022-08-01 15:30:32,906 - Epoch: [27][    2/    2]    Loss 0.592211    Top1 83.636364    Top5 100.000000    
2022-08-01 15:30:32,942 - ==> Top1: 83.636    Top5: 100.000    Loss: 0.592

2022-08-01 15:30:32,942 - ==> Confusion:
[[12  0  0  1  0  0]
 [ 0 16  0  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  0  1  6  0  0]
 [ 1  0  3  0  6  0]
 [ 0  0  3  0  0  2]]

2022-08-01 15:30:33,038 - ==> Best [Top1: 96.364   Top5: 100.000  on epoch: 26]
2022-08-01 15:30:33,048 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_qat_checkpoint.pth.tar
2022-08-01 15:30:33,062 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:30:33,983 - Epoch: [28][   16/   16]    objective_loss 0.346146    Top1 92.452830    Top5 100.000000    LR 0.000422    
2022-08-01 15:30:34,020 - --- validate (epoch=28)-----------
2022-08-01 15:30:34,021 - 55 samples (32 per mini-batch)
2022-08-01 15:30:34,232 - Epoch: [28][    2/    2]    Loss 0.464392    Top1 87.272727    Top5 100.000000    
2022-08-01 15:30:34,268 - ==> Top1: 87.273    Top5: 100.000    Loss: 0.464

2022-08-01 15:30:34,268 - ==> Confusion:
[[10  0  1  1  1  0]
 [ 0 16  0  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  0  1  6  0  0]
 [ 1  0  0  0  9  0]
 [ 0  0  2  0  0  3]]

2022-08-01 15:30:34,361 - ==> Best [Top1: 96.364   Top5: 100.000  on epoch: 26]
2022-08-01 15:30:34,371 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_qat_checkpoint.pth.tar
2022-08-01 15:30:34,386 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:30:35,368 - Epoch: [29][   16/   16]    objective_loss 0.347834    Top1 92.452830    Top5 100.000000    LR 0.000422    
2022-08-01 15:30:35,405 - --- validate (epoch=29)-----------
2022-08-01 15:30:35,405 - 55 samples (32 per mini-batch)
2022-08-01 15:30:35,616 - Epoch: [29][    2/    2]    Loss 0.301024    Top1 94.545455    Top5 100.000000    
2022-08-01 15:30:35,652 - ==> Top1: 94.545    Top5: 100.000    Loss: 0.301

2022-08-01 15:30:35,652 - ==> Confusion:
[[11  0  0  2  0  0]
 [ 0 16  0  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  0  0  7  0  0]
 [ 1  0  0  0  9  0]
 [ 0  0  0  0  0  5]]

2022-08-01 15:30:35,747 - ==> Best [Top1: 96.364   Top5: 100.000  on epoch: 26]
2022-08-01 15:30:35,757 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_qat_checkpoint.pth.tar
2022-08-01 15:30:35,770 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:30:36,796 - Epoch: [30][   16/   16]    objective_loss 0.326777    Top1 94.339623    Top5 100.000000    LR 0.000422    
2022-08-01 15:30:36,834 - --- validate (epoch=30)-----------
2022-08-01 15:30:36,834 - 55 samples (32 per mini-batch)
2022-08-01 15:30:37,032 - Epoch: [30][    2/    2]    Loss 0.341631    Top1 92.727273    Top5 100.000000    
2022-08-01 15:30:37,067 - ==> Top1: 92.727    Top5: 100.000    Loss: 0.342

2022-08-01 15:30:37,068 - ==> Confusion:
[[11  0  0  1  1  0]
 [ 0 16  0  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  0  0  7  0  0]
 [ 0  0  0  0 10  0]
 [ 0  0  2  0  0  3]]

2022-08-01 15:30:37,145 - ==> Best [Top1: 96.364   Top5: 100.000  on epoch: 26]
2022-08-01 15:30:37,150 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_qat_checkpoint.pth.tar
2022-08-01 15:30:37,171 - Training epoch: 501 samples (32 per mini-batch)
2022-08-01 15:30:38,207 - Epoch: [31][   16/   16]    objective_loss 0.318897    Top1 96.226415    Top5 100.000000    LR 0.000422    
2022-08-01 15:30:38,244 - --- validate (epoch=31)-----------
2022-08-01 15:30:38,245 - 55 samples (32 per mini-batch)
2022-08-01 15:30:38,465 - Epoch: [31][    2/    2]    Loss 0.655565    Top1 81.818182    Top5 100.000000    
2022-08-01 15:30:38,500 - ==> Top1: 81.818    Top5: 100.000    Loss: 0.656

2022-08-01 15:30:38,501 - ==> Confusion:
[[ 7  0  2  1  3  0]
 [ 0 16  0  0  0  0]
 [ 0  0  4  0  0  0]
 [ 0  0  1  6  0  0]
 [ 0  0  0  0 10  0]
 [ 0  0  2  0  1  2]]

2022-08-01 15:30:38,595 - ==> Best [Top1: 96.364   Top5: 100.000  on epoch: 26]
2022-08-01 15:30:38,604 - Saving checkpoint to: jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_qat_checkpoint.pth.tar
2022-08-01 15:30:38,739 - Training time: 0:00:45.871617
2022-08-01 15:30:58,824 - => loading checkpoint jupyter_logging/SSL___2022.07.15-171757/classifierbackbonenet_qat_checkpoint.pth.tar
2022-08-01 15:30:58,830 - => Checkpoint contents:
+----------------------+-------------+---------------------------+
| Key                  | Type        | Value                     |
|----------------------+-------------+---------------------------|
| arch                 | str         | classifierbackbonenet_qat |
| compression_sched    | dict        |                           |
| epoch                | int         | 99                        |
| extras               | dict        |                           |
| optimizer_state_dict | dict        |                           |
| optimizer_type       | type        | Adam                      |
| state_dict           | OrderedDict |                           |
+----------------------+-------------+---------------------------+

2022-08-01 15:30:58,831 - => Checkpoint['extras'] contents:
+-------+--------+---------+
| Key   | Type   |   Value |
|-------+--------+---------|
| epoch | int    |      99 |
+-------+--------+---------+

2022-08-01 15:30:58,832 - Loaded compression schedule from checkpoint (epoch 99)
2022-08-01 15:30:58,837 - => loaded 'state_dict' from checkpoint 'jupyter_logging/SSL___2022.07.15-171757/classifierbackbonenet_qat_checkpoint.pth.tar'
2022-08-01 15:30:58,860 - => loading checkpoint jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_qat_best.pth.tar
2022-08-01 15:30:58,865 - => Checkpoint contents:
+----------------------+-------------+--------------------------+
| Key                  | Type        | Value                    |
|----------------------+-------------+--------------------------|
| arch                 | str         | officeclassifier_qat_qat |
| compression_sched    | dict        |                          |
| epoch                | int         | 26                       |
| extras               | dict        |                          |
| optimizer_state_dict | dict        |                          |
| optimizer_type       | type        | Adam                     |
| state_dict           | OrderedDict |                          |
+----------------------+-------------+--------------------------+

2022-08-01 15:30:58,865 - => Checkpoint['extras'] contents:
+--------------+--------+---------+
| Key          | Type   |   Value |
|--------------+--------+---------|
| best_epoch   | int    | 26      |
| best_top1    | float  | 96.3636 |
| current_top1 | float  | 96.3636 |
+--------------+--------+---------+

2022-08-01 15:30:58,866 - Loaded compression schedule from checkpoint (epoch 26)
2022-08-01 15:30:58,903 - => loaded 'state_dict' from checkpoint 'jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_qat_best.pth.tar'
2022-08-01 15:30:58,914 - 64 samples (32 per mini-batch)
2022-08-01 15:30:59,231 - Test: [    2/    2]    Loss 0.387881    Top1 90.625000    Top5 100.000000    
2022-08-01 15:30:59,268 - ==> Top1: 90.625    Top5: 100.000    Loss: 0.388

2022-08-01 15:30:59,268 - ==> Confusion:
[[ 9  0  0  0  0  0]
 [ 0 14  0  0  0  0]
 [ 1  0  9  0  1  0]
 [ 0  0  0 10  0  0]
 [ 0  0  0  0 10  0]
 [ 3  0  0  1  0  6]]

2022-08-01 15:30:59,269 - ==> Test Set [Top1: 90.625   Top5: 100.000  on test set]
2022-08-01 15:32:15,638 - => loading checkpoint jupyter_logging/SSL___2022.07.15-171757/classifierbackbonenet_qat_checkpoint.pth.tar
2022-08-01 15:32:15,646 - => Checkpoint contents:
+----------------------+-------------+---------------------------+
| Key                  | Type        | Value                     |
|----------------------+-------------+---------------------------|
| arch                 | str         | classifierbackbonenet_qat |
| compression_sched    | dict        |                           |
| epoch                | int         | 99                        |
| extras               | dict        |                           |
| optimizer_state_dict | dict        |                           |
| optimizer_type       | type        | Adam                      |
| state_dict           | OrderedDict |                           |
+----------------------+-------------+---------------------------+

2022-08-01 15:32:15,646 - => Checkpoint['extras'] contents:
+-------+--------+---------+
| Key   | Type   |   Value |
|-------+--------+---------|
| epoch | int    |      99 |
+-------+--------+---------+

2022-08-01 15:32:15,647 - Loaded compression schedule from checkpoint (epoch 99)
2022-08-01 15:32:15,652 - => loaded 'state_dict' from checkpoint 'jupyter_logging/SSL___2022.07.15-171757/classifierbackbonenet_qat_checkpoint.pth.tar'
2022-08-01 15:32:15,676 - => loading checkpoint jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_best.pth.tar
2022-08-01 15:32:15,681 - => Checkpoint contents:
+----------------------+-------------+--------------------------+
| Key                  | Type        | Value                    |
|----------------------+-------------+--------------------------|
| arch                 | str         | officeclassifier_qat_qat |
| compression_sched    | dict        |                          |
| epoch                | int         | 26                       |
| extras               | dict        |                          |
| optimizer_state_dict | dict        |                          |
| optimizer_type       | type        | Adam                     |
| state_dict           | OrderedDict |                          |
+----------------------+-------------+--------------------------+

2022-08-01 15:32:15,681 - => Checkpoint['extras'] contents:
+--------------+--------+---------+
| Key          | Type   |   Value |
|--------------+--------+---------|
| best_epoch   | int    | 26      |
| best_top1    | float  | 96.3636 |
| current_top1 | float  | 96.3636 |
+--------------+--------+---------+

2022-08-01 15:32:15,682 - Loaded compression schedule from checkpoint (epoch 26)
2022-08-01 15:32:15,730 - => loaded 'state_dict' from checkpoint 'jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_best.pth.tar'
2022-08-01 15:32:15,741 - 78 samples (32 per mini-batch)
2022-08-01 15:32:15,905 - Test: [    3/    3]    Loss 1.837532    Top1 39.743590    Top5 93.589744    
2022-08-01 15:32:15,941 - ==> Top1: 39.744    Top5: 93.590    Loss: 1.838

2022-08-01 15:32:15,943 - ==> Confusion:
[[ 1  9  0  1  0  0]
 [ 0  7  3  0  0  0]
 [ 0  2  3  3  0  0]
 [ 0  4  2 10  0  0]
 [ 0  2  3  2  9  3]
 [ 0  8  0  5  0  1]]

2022-08-01 15:32:15,943 - ==> Test Set [Top1: 39.744   Top5: 93.590  on test set]
2022-08-01 15:32:42,723 - => loading checkpoint jupyter_logging/SSL___2022.07.15-171757/classifierbackbonenet_qat_checkpoint.pth.tar
2022-08-01 15:32:42,730 - => Checkpoint contents:
+----------------------+-------------+---------------------------+
| Key                  | Type        | Value                     |
|----------------------+-------------+---------------------------|
| arch                 | str         | classifierbackbonenet_qat |
| compression_sched    | dict        |                           |
| epoch                | int         | 99                        |
| extras               | dict        |                           |
| optimizer_state_dict | dict        |                           |
| optimizer_type       | type        | Adam                      |
| state_dict           | OrderedDict |                           |
+----------------------+-------------+---------------------------+

2022-08-01 15:32:42,730 - => Checkpoint['extras'] contents:
+-------+--------+---------+
| Key   | Type   |   Value |
|-------+--------+---------|
| epoch | int    |      99 |
+-------+--------+---------+

2022-08-01 15:32:42,731 - Loaded compression schedule from checkpoint (epoch 99)
2022-08-01 15:32:42,735 - => loaded 'state_dict' from checkpoint 'jupyter_logging/SSL___2022.07.15-171757/classifierbackbonenet_qat_checkpoint.pth.tar'
2022-08-01 15:32:42,758 - => loading checkpoint jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_best.pth.tar
2022-08-01 15:32:42,763 - => Checkpoint contents:
+----------------------+-------------+--------------------------+
| Key                  | Type        | Value                    |
|----------------------+-------------+--------------------------|
| arch                 | str         | officeclassifier_qat_qat |
| compression_sched    | dict        |                          |
| epoch                | int         | 26                       |
| extras               | dict        |                          |
| optimizer_state_dict | dict        |                          |
| optimizer_type       | type        | Adam                     |
| state_dict           | OrderedDict |                          |
+----------------------+-------------+--------------------------+

2022-08-01 15:32:42,763 - => Checkpoint['extras'] contents:
+--------------+--------+---------+
| Key          | Type   |   Value |
|--------------+--------+---------|
| best_epoch   | int    | 26      |
| best_top1    | float  | 96.3636 |
| current_top1 | float  | 96.3636 |
+--------------+--------+---------+

2022-08-01 15:32:42,764 - Loaded compression schedule from checkpoint (epoch 26)
2022-08-01 15:32:42,801 - => loaded 'state_dict' from checkpoint 'jupyter_logging/finetune_office_base_ev1___2022.08.01-152827/officeclassifier_qat_best.pth.tar'

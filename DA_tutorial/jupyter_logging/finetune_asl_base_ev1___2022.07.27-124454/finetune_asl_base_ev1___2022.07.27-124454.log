2022-07-27 12:44:54,017 - Log file for this run: /home/geffencooper/Model_Development/DA_ai8x-training/DA_tutorial/jupyter_logging/finetune_asl_base_ev1___2022.07.27-124454/finetune_asl_base_ev1___2022.07.27-124454.log
2022-07-27 12:44:54,018 - Number of CPUs: 16
2022-07-27 12:44:54,018 - Number of GPUs: 1
2022-07-27 12:44:54,018 - CUDA version: 10.2
2022-07-27 12:44:54,018 - CUDNN version: 7605
2022-07-27 12:44:54,018 - Kernel: 5.4.0-113-generic
2022-07-27 12:44:54,018 - Python: 3.8.11 (default, Jun 14 2022, 10:01:20) 
[GCC 9.4.0]
2022-07-27 12:44:54,018 - pip freeze: {'absl-py': '1.2.0', 'appdirs': '1.4.4', 'argon2-cffi': '21.3.0', 'argon2-cffi-bindings': '21.2.0', 'asttokens': '2.0.5', 'atomicwrites': '1.4.1', 'attrs': '21.4.0', 'audioread': '2.1.9', 'backcall': '0.2.0', 'beautifulsoup4': '4.11.1', 'bleach': '5.0.1', 'bqplot': '0.11.5', 'cachetools': '5.2.0', 'certifi': '2022.6.15', 'cffi': '1.15.1', 'charset-normalizer': '2.1.0', 'cloudpickle': '2.1.0', 'cycler': '0.11.0', 'debugpy': '1.6.2', 'decorator': '5.1.1', 'defusedxml': '0.7.1', 'distiller': '0.4.0rc0', 'entrypoints': '0.4', 'executing': '0.9.1', 'fastjsonschema': '2.16.1', 'fonttools': '4.34.4', 'google-auth': '2.9.1', 'google-auth-oauthlib': '0.4.6', 'graphviz': '0.10.1', 'grpcio': '1.47.0', 'gym': '0.12.5', 'h5py': '3.7.0', 'idna': '3.3', 'importlib-metadata': '4.12.0', 'importlib-resources': '5.9.0', 'ipykernel': '6.15.1', 'ipython': '8.4.0', 'ipython-genutils': '0.2.0', 'ipywidgets': '7.4.2', 'jedi': '0.18.1', 'jinja2': '3.1.2', 'joblib': '1.1.0', 'jsonpatch': '1.32', 'jsonpointer': '2.3', 'jsonschema': '4.7.2', 'jupyter': '1.0.0', 'jupyter-client': '7.3.4', 'jupyter-console': '6.4.4', 'jupyter-core': '4.11.1', 'jupyterlab-pygments': '0.2.2', 'kiwisolver': '1.4.4', 'librosa': '0.9.2', 'llvmlite': '0.32.1', 'markdown': '3.4.1', 'markupsafe': '2.1.1', 'matplotlib': '3.5.2', 'matplotlib-inline': '0.1.3', 'mistune': '0.8.4', 'more-itertools': '8.13.0', 'munch': '2.5.0', 'nbclient': '0.6.6', 'nbconvert': '6.5.0', 'nbformat': '5.4.0', 'nest-asyncio': '1.5.5', 'notebook': '6.4.12', 'numba': '0.49.1', 'numpy': '1.22.4', 'oauthlib': '3.2.0', 'opencv-python': '4.6.0.66', 'packaging': '21.3', 'pandas': '1.4.3', 'pandocfilters': '1.5.0', 'parso': '0.8.3', 'pexpect': '4.8.0', 'pickleshare': '0.7.5', 'pillow': '9.2.0', 'pip': '22.2', 'pluggy': '0.13.1', 'pooch': '1.6.0', 'pretrainedmodels': '0.7.4', 'prometheus-client': '0.14.1', 'prompt-toolkit': '3.0.30', 'protobuf': '3.20.1', 'psutil': '5.9.1', 'ptyprocess': '0.7.0', 'pure-eval': '0.2.2', 'py': '1.11.0', 'pyasn1': '0.4.8', 'pyasn1-modules': '0.2.8', 'pycparser': '2.21', 'pydot': '1.4.1', 'pyglet': '1.5.26', 'pygments': '2.12.0', 'pyparsing': '3.0.9', 'pyrsistent': '0.18.1', 'pytest': '4.6.11', 'python-dateutil': '2.8.2', 'pytsmod': '0.3.5', 'pytz': '2022.1', 'pyyaml': '6.0', 'pyzmq': '23.2.0', 'qgrid': '1.1.1', 'qtconsole': '5.3.1', 'qtpy': '2.1.0', 'requests': '2.28.1', 'requests-oauthlib': '1.3.1', 'resampy': '0.3.1', 'rsa': '4.9', 'scikit-learn': '0.23.2', 'scipy': '1.8.1', 'send2trash': '1.8.0', 'setuptools': '63.2.0', 'shap': '0.41.0', 'six': '1.16.0', 'slicer': '0.0.7', 'soundfile': '0.10.3.post1', 'soupsieve': '2.3.2.post1', 'stack-data': '0.3.0', 'tabulate': '0.8.3', 'tensorboard': '2.9.0', 'tensorboard-data-server': '0.6.1', 'tensorboard-plugin-wit': '1.8.1', 'terminado': '0.15.0', 'threadpoolctl': '3.1.0', 'tinycss2': '1.1.1', 'tk': '0.1.0', 'torch': '1.8.1', 'torchfile': '0.1.0', 'torchnet': '0.0.4', 'torchvision': '0.9.1', 'tornado': '6.2', 'tqdm': '4.33.0', 'traitlets': '5.3.0', 'traittypes': '0.2.1', 'typing-extensions': '4.3.0', 'urllib3': '1.26.11', 'visdom': '0.1.8.9', 'wcwidth': '0.2.5', 'webencodings': '0.5.1', 'websocket-client': '1.3.3', 'werkzeug': '2.2.0', 'wheel': '0.37.1', 'widgetsnbextension': '3.4.2', 'xlsxwriter': '3.0.3', 'zipp': '3.8.1'}
2022-07-27 12:44:54,018 - Command line: /home/geffencooper/Model_Development/DA_ai8x-training/venv/lib/python3.8/site-packages/ipykernel_launcher.py --ip=127.0.0.1 --stdin=9008 --control=9006 --hb=9005 --Session.signature_scheme="hmac-sha256" --Session.key=b"45c013f0-79b8-466b-950a-b2b587bb6266" --shell=9007 --transport="tcp" --iopub=9009 --f=/home/geffencooper/.local/share/jupyter/runtime/kernel-v2-3782366aY0tBYt3GEba.json
2022-07-27 12:44:54,019 - dataset_name:asl
dataset_fn=<function asl_get_datasets at 0x7fe8dc6008b0>
num_classes=29
model_name=aslclassifier
dimensions=(3, 128, 128)
batch_size=64
validation_split=0.1
lr=0.001000
num_epochs=12
qat_policy={'start_epoch': 4, 'weight_bits': 8}
2022-07-27 12:45:00,970 - Dataset sizes:
	training=70470
	validation=7830
	test=8700
2022-07-27 12:45:00,971 - Augmentations:Compose(
    Resize(size=(128, 128), interpolation=bilinear)
    ColorJitter(brightness=(0.85, 1.15), contrast=(0.75, 1.25), saturation=(0.75, 1.25), hue=(-0.4, 0.4))
    RandomGrayscale(p=0.15)
    RandomAffine(degrees=[-5.0, 5.0], translate=(0.1, 0.1))
    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 1.5))
    ToTensor()
    <ai8x.normalize object at 0x7fe97982a5b0>
)
Augmentation Seed:2030916208
2022-07-27 12:46:42,188 - => loading checkpoint jupyter_logging/SSL___2022.07.15-171757/classifierbackbonenet_qat_checkpoint.pth.tar
2022-07-27 12:46:42,193 - => Checkpoint contents:
+----------------------+-------------+---------------------------+
| Key                  | Type        | Value                     |
|----------------------+-------------+---------------------------|
| arch                 | str         | classifierbackbonenet_qat |
| compression_sched    | dict        |                           |
| epoch                | int         | 99                        |
| extras               | dict        |                           |
| optimizer_state_dict | dict        |                           |
| optimizer_type       | type        | Adam                      |
| state_dict           | OrderedDict |                           |
+----------------------+-------------+---------------------------+

2022-07-27 12:46:42,194 - => Checkpoint['extras'] contents:
+-------+--------+---------+
| Key   | Type   |   Value |
|-------+--------+---------|
| epoch | int    |      99 |
+-------+--------+---------+

2022-07-27 12:46:42,195 - Loaded compression schedule from checkpoint (epoch 99)
2022-07-27 12:46:42,199 - => loaded 'state_dict' from checkpoint 'jupyter_logging/SSL___2022.07.15-171757/classifierbackbonenet_qat_checkpoint.pth.tar'
2022-07-27 12:46:42,210 - model: ASLClassifier(
  (feature_extractor): ClassifierBackbone(
    (conv1): FusedConv2dReLU(
      (activate): ReLU(inplace=True)
      (op): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv2): FusedConv2dReLU(
      (activate): ReLU(inplace=True)
      (op): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv3): FusedMaxPoolConv2dReLU(
      (activate): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (op): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv4): FusedConv2dBNReLU(
      (activate): ReLU(inplace=True)
      (op): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): None
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv5): FusedMaxPoolConv2dBNReLU(
      (activate): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (op): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): None
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv6): FusedConv2dBNReLU(
      (activate): ReLU(inplace=True)
      (op): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): None
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv7): FusedMaxPoolConv2dBNReLU(
      (activate): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (op): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): None
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv8): FusedConv2dBNReLU(
      (activate): ReLU(inplace=True)
      (op): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): None
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv9): FusedMaxPoolConv2dBNReLU(
      (activate): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (op): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): None
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (conv10): FusedMaxPoolConv2dBNReLU(
      (activate): ReLU(inplace=True)
      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (op): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn): None
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (fc1): FusedLinearReLU(
      (activate): ReLU(inplace=True)
      (op): Linear(in_features=1024, out_features=128, bias=True)
      (calc_out_shift): OutputShift()
      (calc_weight_scale): WeightScale()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Quantize()
      (quantize_bias): Quantize()
      (clamp_weight): Clamp()
      (clamp_bias): Clamp()
      (quantize): Quantize()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (do1): Dropout(p=0.5, inplace=False)
    (fc2): FusedLinearReLU(
      (activate): ReLU(inplace=True)
      (op): Linear(in_features=128, out_features=64, bias=True)
      (calc_out_shift): OutputShiftSqueeze()
      (calc_weight_scale): One()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Empty()
      (quantize_bias): Empty()
      (clamp_weight): Empty()
      (clamp_bias): Empty()
      (quantize): Empty()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
    (fc3): Linear(
      (activate): Empty()
      (op): Linear(in_features=64, out_features=29, bias=True)
      (calc_out_shift): OutputShiftSqueeze()
      (calc_weight_scale): One()
      (scale): Scaler()
      (calc_out_scale): OutputScale()
      (quantize_weight): Empty()
      (quantize_bias): Empty()
      (clamp_weight): Empty()
      (clamp_bias): Empty()
      (quantize): Empty()
      (clamp): Clamp()
      (quantize_pool): Empty()
      (clamp_pool): Empty()
    )
  )
  (do1): Dropout(p=0.5, inplace=False)
)
2022-07-27 12:46:42,248 - Number of Model Params: 288773
2022-07-27 12:46:45,600 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2022-07-27 12:46:45,600 - lr_schedule:base: [0.001] milestones: Counter({4: 1, 8: 1, 20: 1, 100: 1}) gamma: 0.75
2022-07-27 12:46:49,975 - Training epoch: 70470 samples (64 per mini-batch)
2022-07-27 12:46:53,184 - Epoch: [0][   50/ 1102]    objective_loss 3.373435                                        LR 0.001000    
2022-07-27 12:46:56,011 - Epoch: [0][  100/ 1102]    objective_loss 3.360751                                        LR 0.001000    
2022-07-27 12:46:58,987 - Epoch: [0][  150/ 1102]    objective_loss 3.332549                                        LR 0.001000    
2022-07-27 12:47:01,882 - Epoch: [0][  200/ 1102]    objective_loss 3.313645                                        LR 0.001000    
2022-07-27 12:47:04,822 - Epoch: [0][  250/ 1102]    objective_loss 3.282994                                        LR 0.001000    
2022-07-27 12:47:07,774 - Epoch: [0][  300/ 1102]    objective_loss 3.239389                                        LR 0.001000    
2022-07-27 12:47:10,654 - Epoch: [0][  350/ 1102]    objective_loss 3.184820                                        LR 0.001000    
2022-07-27 12:47:13,610 - Epoch: [0][  400/ 1102]    objective_loss 3.124408                                        LR 0.001000    
2022-07-27 12:47:16,451 - Epoch: [0][  450/ 1102]    objective_loss 3.061597                                        LR 0.001000    
2022-07-27 12:47:19,417 - Epoch: [0][  500/ 1102]    objective_loss 2.997676                                        LR 0.001000    
2022-07-27 12:47:22,275 - Epoch: [0][  550/ 1102]    objective_loss 2.934374                                        LR 0.001000    
2022-07-27 12:47:25,247 - Epoch: [0][  600/ 1102]    objective_loss 2.871041                                        LR 0.001000    
2022-07-27 12:47:28,113 - Epoch: [0][  650/ 1102]    objective_loss 2.809372                                        LR 0.001000    
2022-07-27 12:47:31,039 - Epoch: [0][  700/ 1102]    objective_loss 2.747339                                        LR 0.001000    
2022-07-27 12:47:33,998 - Epoch: [0][  750/ 1102]    objective_loss 2.683794                                        LR 0.001000    
2022-07-27 12:47:36,850 - Epoch: [0][  800/ 1102]    objective_loss 2.619001                                        LR 0.001000    
2022-07-27 12:47:39,802 - Epoch: [0][  850/ 1102]    objective_loss 2.558792                                        LR 0.001000    
2022-07-27 12:47:42,637 - Epoch: [0][  900/ 1102]    objective_loss 2.499201                                        LR 0.001000    
2022-07-27 12:47:45,540 - Epoch: [0][  950/ 1102]    objective_loss 2.442367                                        LR 0.001000    
2022-07-27 12:47:48,434 - Epoch: [0][ 1000/ 1102]    objective_loss 2.389240                                        LR 0.001000    
2022-07-27 12:47:51,293 - Epoch: [0][ 1050/ 1102]    objective_loss 2.336858                                        LR 0.001000    
2022-07-27 12:47:54,214 - Epoch: [0][ 1100/ 1102]    objective_loss 2.287135                                        LR 0.001000    
2022-07-27 12:47:54,266 - Epoch: [0][ 1102/ 1102]    objective_loss 2.285982    Top1 38.571429    Top5 95.714286    LR 0.001000    
2022-07-27 12:47:54,316 - --- validate (epoch=0)-----------
2022-07-27 12:47:54,317 - 7830 samples (64 per mini-batch)
2022-07-27 12:47:55,450 - Epoch: [0][   50/  123]    Loss 0.842535    Top1 72.625000    Top5 99.062500    
2022-07-27 12:47:56,380 - Epoch: [0][  100/  123]    Loss 0.843855    Top1 72.375000    Top5 99.062500    
2022-07-27 12:47:56,782 - Epoch: [0][  123/  123]    Loss 0.844707    Top1 72.145594    Top5 99.093231    
2022-07-27 12:47:56,829 - ==> Top1: 72.146    Top5: 99.093    Loss: 0.845

2022-07-27 12:47:56,831 - ==> Confusion:
[[258   2   0   0  10   0   0   0   0   0   0   0   0   0   0   0   0   0
    1   3   0   0   0   5   0   4   0   0   0]
 [  0 261   0   0   6   0   0   0   0   0   8   0   1   0   0   0   0   0
    0   0   0   0   3   0   0   0   0   0   0]
 [  0   0 256   0   0   0   0   0   0   0   0   3   0   0   0   0   2   0
    0   0   0   0   0   0   0   1   0   0   0]
 [  0   0   0 256   0   0   0   0   1   0   1   0   0   0   0   0   0   5
    0   0   0   0   0   0   0   0   0   0   0]
 [ 13   9   0   0 229   0   0   0   4   0   0   0   1   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  1   0   0   0   6 250   0   0   0   0   0   0   0   9   4   0   0   0
    0   2   0   0   0   0   1   0   0   0   0]
 [  0   1   0   0   0   0 156  85   1   0   0   0   0   0   0   0   0   1
    0   0   0   0   0   0   0   0   0   0   1]
 [  0   0   0   0   0   0  17 259   0   0   0   0   0   0   0  14   0   1
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   7   0   0   0   3   0   0 260   0   0   0   0   2   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   9   7   0   8 221   0   0   0   2   0   0   0   0
    0   0   0   0   0   0   0   0   0   0  17]
 [  0  27   0   0   0   0   0   0   0   0 113   0   0   0   0   0   0 114
    0   0   0  20   1   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 268   0   0   0   0   0   0
    0   0   0   0   0   0   0   1   0   0   0]
 [ 18   5   0   0 259   0   0   0   0   0   0   0   0   6   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  1   1   0   1 162   1   0   0   5   0   0   0   4  97   2   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  4   1   2   2   3   6   0   0   0   0   0   0   0   3 202   0   0   0
    7  11   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 213  12   0
    0   2   0   0   0   0   3   0  16   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   4 294   0
    0   0   0   0   0   0   0   0   2   0   0]
 [  0   0   0   0   0   0   0   0   0   0   4   0   0   0   0   0   0 259
    0   0   0   9   0   0   0   0   0   0   0]
 [179   0   0   0  13   0   0   0   0   0   0   0   0   0   0   0   0   0
   36  13   0   0   2   7   0   0   0   0   0]
 [  5   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    2 153   0   0   0   0   0 107   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 255
    0   0   0   6   0   1   0   0   0   0   0]
 [  0   5   0   0   0   0   0   0   0   0  26   0   0   0   0   0   0 176
    0   0   0  49   1   0   0   0   0   0   0]
 [  0 195   0   0  42   0   0   0   0   0   6   0   2   0   0   0   0   0
    0   0   0   0  25   0   0   0   0   0   0]
 [ 23   0   0   0   1   0   0   0   0   0   0   3   0   0   0   0   0   2
    0   0   0   3   1 252   0   1   0   0   0]
 [  0   0   0   0   0   8   0   0   0   0   0   0   0   0   4   0   0   0
    3  37   0   0   0   0 248   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2   0   0   0   0   0   0
    0   5   0   0   0   2   0 247   0   0   0]
 [  0   0   0   0   0   1   0   0   0   0   0   0   0   3   0   5  10   0
    0   4   0   0   0   0   1   0 258   0   0]
 [  0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0 273   0]
 [  0   0   0   0   0   1   0   0   0   2   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   2   0  12   0 256]]

2022-07-27 12:47:57,717 - ==> Best [Top1: 72.146   Top5: 99.093  on epoch: 0]
2022-07-27 12:47:57,731 - Saving checkpoint to: jupyter_logging/finetune_asl_base_ev1___2022.07.27-124454/aslclassifier_checkpoint.pth.tar
2022-07-27 12:47:57,745 - Training epoch: 70470 samples (64 per mini-batch)
2022-07-27 12:48:01,057 - Epoch: [1][   50/ 1102]    objective_loss 1.172348                                        LR 0.001000    
2022-07-27 12:48:03,981 - Epoch: [1][  100/ 1102]    objective_loss 1.140309                                        LR 0.001000    
2022-07-27 12:48:07,028 - Epoch: [1][  150/ 1102]    objective_loss 1.129833                                        LR 0.001000    
2022-07-27 12:48:09,958 - Epoch: [1][  200/ 1102]    objective_loss 1.112169                                        LR 0.001000    
2022-07-27 12:48:12,980 - Epoch: [1][  250/ 1102]    objective_loss 1.087892                                        LR 0.001000    
2022-07-27 12:48:15,954 - Epoch: [1][  300/ 1102]    objective_loss 1.070136                                        LR 0.001000    
2022-07-27 12:48:18,955 - Epoch: [1][  350/ 1102]    objective_loss 1.052188                                        LR 0.001000    
2022-07-27 12:48:22,028 - Epoch: [1][  400/ 1102]    objective_loss 1.029636                                        LR 0.001000    
2022-07-27 12:48:24,945 - Epoch: [1][  450/ 1102]    objective_loss 1.012157                                        LR 0.001000    
2022-07-27 12:48:27,972 - Epoch: [1][  500/ 1102]    objective_loss 0.992142                                        LR 0.001000    
2022-07-27 12:48:30,899 - Epoch: [1][  550/ 1102]    objective_loss 0.974512                                        LR 0.001000    
2022-07-27 12:48:33,921 - Epoch: [1][  600/ 1102]    objective_loss 0.958323                                        LR 0.001000    
2022-07-27 12:48:36,853 - Epoch: [1][  650/ 1102]    objective_loss 0.942272                                        LR 0.001000    
2022-07-27 12:48:39,883 - Epoch: [1][  700/ 1102]    objective_loss 0.928453                                        LR 0.001000    
2022-07-27 12:48:42,798 - Epoch: [1][  750/ 1102]    objective_loss 0.912536                                        LR 0.001000    
2022-07-27 12:48:45,827 - Epoch: [1][  800/ 1102]    objective_loss 0.897185                                        LR 0.001000    
2022-07-27 12:48:48,721 - Epoch: [1][  850/ 1102]    objective_loss 0.883820                                        LR 0.001000    
2022-07-27 12:48:51,739 - Epoch: [1][  900/ 1102]    objective_loss 0.870500                                        LR 0.001000    
2022-07-27 12:48:54,677 - Epoch: [1][  950/ 1102]    objective_loss 0.859306                                        LR 0.001000    
2022-07-27 12:48:57,711 - Epoch: [1][ 1000/ 1102]    objective_loss 0.848813                                        LR 0.001000    
2022-07-27 12:49:00,616 - Epoch: [1][ 1050/ 1102]    objective_loss 0.837940                                        LR 0.001000    
2022-07-27 12:49:03,619 - Epoch: [1][ 1100/ 1102]    objective_loss 0.826405                                        LR 0.001000    
2022-07-27 12:49:03,670 - Epoch: [1][ 1102/ 1102]    objective_loss 0.825575    Top1 85.714286    Top5 98.571429    LR 0.001000    
2022-07-27 12:49:03,719 - --- validate (epoch=1)-----------
2022-07-27 12:49:03,720 - 7830 samples (64 per mini-batch)
2022-07-27 12:49:04,919 - Epoch: [1][   50/  123]    Loss 0.314223    Top1 90.312500    Top5 99.750000    
2022-07-27 12:49:05,930 - Epoch: [1][  100/  123]    Loss 0.312220    Top1 90.656250    Top5 99.796875    
2022-07-27 12:49:06,360 - Epoch: [1][  123/  123]    Loss 0.314154    Top1 90.510856    Top5 99.782886    
2022-07-27 12:49:06,411 - ==> Top1: 90.511    Top5: 99.783    Loss: 0.314

2022-07-27 12:49:06,412 - ==> Confusion:
[[263   0   0   0   7   0   0   0   0   0   0   0   0   0   0   0   0   0
   13   0   0   0   0   0   0   0   0   0   0]
 [  0 279   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 256   0   0   0   0   0   0   0   0   2   0   0   3   0   0   0
    0   0   0   0   0   0   0   1   0   0   0]
 [  0   0   0 262   0   0   0   0   1   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  1  14   0   0 220   0   0   0   2   0   0   0  12   6   0   0   0   0
    0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0   0   0 270   0   0   0   0   0   0   0   2   0   0   0   0
    0   0   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0   0   0 234  11   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   8 283   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 269   2   0   0   0   1   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   1 262   0   0   0   1   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0   0   0   0   0 247   0   0   0   0   0   0   2
    0   0   5  19   1   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 269   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0  18   0   0   0   0   0   0   0 116 154   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   3   0   0   2   0   0   0   3 266   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   1   0   7   0   0   0   0   0   0   1   2 229   0   0   0
    1   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   2   6   0   0   0   0   0   0   0 238   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   4 293   0
    0   0   0   0   0   0   0   0   3   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 139
    0   0 133   0   0   0   0   0   0   0   0]
 [ 58   0   0   0   2   0   0   0   0   0   0   0   4   1   0   0   0   0
  179   2   0   0   0   4   0   0   0   0   0]
 [  4   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0
   14 243   0   0   0   0   4   1   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  34
    0   0 228   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0  75   0   0   0   0   0   0   1
    0   0  29 152   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0   0   0   0   0   4   0   0   0   0   0   0   0
    0   0   0   0 265   0   0   0   0   0   0]
 [  5   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   9   0   0 271   0   0   0   0   0]
 [  0   0   0   0   0   2   0   0   0   2   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 296   0   0   0   0]
 [  3   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0
    5  13   0   0   0   3   0 231   0   0   0]
 [  0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0 280   0   1]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0 274   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0 273]]

2022-07-27 12:49:07,150 - ==> Best [Top1: 90.511   Top5: 99.783  on epoch: 1]
2022-07-27 12:49:07,163 - Saving checkpoint to: jupyter_logging/finetune_asl_base_ev1___2022.07.27-124454/aslclassifier_checkpoint.pth.tar
2022-07-27 12:49:07,187 - Training epoch: 70470 samples (64 per mini-batch)
2022-07-27 12:49:10,477 - Epoch: [2][   50/ 1102]    objective_loss 0.555263                                        LR 0.001000    
2022-07-27 12:49:13,416 - Epoch: [2][  100/ 1102]    objective_loss 0.568327                                        LR 0.001000    
2022-07-27 12:49:16,430 - Epoch: [2][  150/ 1102]    objective_loss 0.558438                                        LR 0.001000    
2022-07-27 12:49:19,345 - Epoch: [2][  200/ 1102]    objective_loss 0.556740                                        LR 0.001000    
2022-07-27 12:49:22,367 - Epoch: [2][  250/ 1102]    objective_loss 0.545522                                        LR 0.001000    
2022-07-27 12:49:25,297 - Epoch: [2][  300/ 1102]    objective_loss 0.544711                                        LR 0.001000    
2022-07-27 12:49:28,266 - Epoch: [2][  350/ 1102]    objective_loss 0.537994                                        LR 0.001000    
2022-07-27 12:49:31,268 - Epoch: [2][  400/ 1102]    objective_loss 0.533781                                        LR 0.001000    
2022-07-27 12:49:34,225 - Epoch: [2][  450/ 1102]    objective_loss 0.525109                                        LR 0.001000    
2022-07-27 12:49:37,234 - Epoch: [2][  500/ 1102]    objective_loss 0.520345                                        LR 0.001000    
2022-07-27 12:49:40,179 - Epoch: [2][  550/ 1102]    objective_loss 0.514499                                        LR 0.001000    
2022-07-27 12:49:43,231 - Epoch: [2][  600/ 1102]    objective_loss 0.508559                                        LR 0.001000    
2022-07-27 12:49:46,202 - Epoch: [2][  650/ 1102]    objective_loss 0.504390                                        LR 0.001000    
2022-07-27 12:49:49,249 - Epoch: [2][  700/ 1102]    objective_loss 0.499721                                        LR 0.001000    
2022-07-27 12:49:52,172 - Epoch: [2][  750/ 1102]    objective_loss 0.497552                                        LR 0.001000    
2022-07-27 12:49:55,193 - Epoch: [2][  800/ 1102]    objective_loss 0.494783                                        LR 0.001000    
2022-07-27 12:49:58,112 - Epoch: [2][  850/ 1102]    objective_loss 0.489459                                        LR 0.001000    
2022-07-27 12:50:01,170 - Epoch: [2][  900/ 1102]    objective_loss 0.483949                                        LR 0.001000    
2022-07-27 12:50:04,113 - Epoch: [2][  950/ 1102]    objective_loss 0.479904                                        LR 0.001000    
2022-07-27 12:50:07,183 - Epoch: [2][ 1000/ 1102]    objective_loss 0.475789                                        LR 0.001000    
2022-07-27 12:50:10,130 - Epoch: [2][ 1050/ 1102]    objective_loss 0.471838                                        LR 0.001000    
2022-07-27 12:50:13,111 - Epoch: [2][ 1100/ 1102]    objective_loss 0.468506                                        LR 0.001000    
2022-07-27 12:50:13,163 - Epoch: [2][ 1102/ 1102]    objective_loss 0.468253    Top1 85.714286    Top5 100.000000    LR 0.001000    
2022-07-27 12:50:13,212 - --- validate (epoch=2)-----------
2022-07-27 12:50:13,213 - 7830 samples (64 per mini-batch)
2022-07-27 12:50:14,380 - Epoch: [2][   50/  123]    Loss 0.145250    Top1 95.937500    Top5 99.968750    
2022-07-27 12:50:15,335 - Epoch: [2][  100/  123]    Loss 0.147900    Top1 95.921875    Top5 99.953125    
2022-07-27 12:50:15,762 - Epoch: [2][  123/  123]    Loss 0.145304    Top1 96.015326    Top5 99.961686    
2022-07-27 12:50:15,810 - ==> Top1: 96.015    Top5: 99.962    Loss: 0.145

2022-07-27 12:50:15,812 - ==> Confusion:
[[270   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0
   10   1   0   0   0   1   0   0   0   0   0]
 [  0 278   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 262   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 263   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   2   0   0 250   0   0   0   0   0   0   0   3   0   0   0   0   0
    1   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 270   0   0   0   0   0   0   0   1   0   0   0   0
    0   0   0   0   0   0   2   0   0   0   0]
 [  0   0   0   0   0   0 245   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   5 286   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 268   4   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   1 263   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 265   0   0   0   0   0   0   0
    0   0   0  10   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 268   0   0   0   0   0   0
    0   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6   0   0   0   0   0   0   0 234  48   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   2   0   0   0  11 261   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   1   0   0   0   0   0   0   0   0 240   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 246   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1 299   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 246
    0   0  26   0   0   0   0   0   0   0   0]
 [  1   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0
  243   0   0   0   0   4   0   0   1   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    1 265   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  92
    0   0 169   0   0   1   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0  41   0   0   0   0   0   0   2
    0   0   7 207   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   4   0   0   0   0   0   0   0
    0   0   0   1 265   0   0   0   0   0   0]
 [  1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    3   0   3   0   0 279   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 300   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0
    0   3   1   0   0   0   0 251   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0 282   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0
    0   0   0   0   0   0   0   0   2 271   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0
    0   0   0   0   0   0   0   0   0   0 272]]

2022-07-27 12:50:16,694 - ==> Best [Top1: 96.015   Top5: 99.962  on epoch: 2]
2022-07-27 12:50:16,705 - Saving checkpoint to: jupyter_logging/finetune_asl_base_ev1___2022.07.27-124454/aslclassifier_checkpoint.pth.tar
2022-07-27 12:50:16,730 - Training epoch: 70470 samples (64 per mini-batch)
2022-07-27 12:50:20,001 - Epoch: [3][   50/ 1102]    objective_loss 0.369165                                        LR 0.001000    
2022-07-27 12:50:22,904 - Epoch: [3][  100/ 1102]    objective_loss 0.365706                                        LR 0.001000    
2022-07-27 12:50:25,955 - Epoch: [3][  150/ 1102]    objective_loss 0.346766                                        LR 0.001000    
2022-07-27 12:50:28,892 - Epoch: [3][  200/ 1102]    objective_loss 0.347655                                        LR 0.001000    
2022-07-27 12:50:31,959 - Epoch: [3][  250/ 1102]    objective_loss 0.343560                                        LR 0.001000    
2022-07-27 12:50:34,852 - Epoch: [3][  300/ 1102]    objective_loss 0.346279                                        LR 0.001000    
2022-07-27 12:50:37,932 - Epoch: [3][  350/ 1102]    objective_loss 0.344403                                        LR 0.001000    
2022-07-27 12:50:40,823 - Epoch: [3][  400/ 1102]    objective_loss 0.345650                                        LR 0.001000    
2022-07-27 12:50:43,847 - Epoch: [3][  450/ 1102]    objective_loss 0.345868                                        LR 0.001000    
2022-07-27 12:50:46,759 - Epoch: [3][  500/ 1102]    objective_loss 0.348874                                        LR 0.001000    
2022-07-27 12:50:49,807 - Epoch: [3][  550/ 1102]    objective_loss 0.352020                                        LR 0.001000    
2022-07-27 12:50:52,723 - Epoch: [3][  600/ 1102]    objective_loss 0.347837                                        LR 0.001000    
2022-07-27 12:50:55,768 - Epoch: [3][  650/ 1102]    objective_loss 0.344729                                        LR 0.001000    
2022-07-27 12:50:58,673 - Epoch: [3][  700/ 1102]    objective_loss 0.342830                                        LR 0.001000    
2022-07-27 12:51:01,713 - Epoch: [3][  750/ 1102]    objective_loss 0.341896                                        LR 0.001000    
2022-07-27 12:51:04,637 - Epoch: [3][  800/ 1102]    objective_loss 0.340210                                        LR 0.001000    
2022-07-27 12:51:07,695 - Epoch: [3][  850/ 1102]    objective_loss 0.339366                                        LR 0.001000    
2022-07-27 12:51:10,637 - Epoch: [3][  900/ 1102]    objective_loss 0.336466                                        LR 0.001000    
2022-07-27 12:51:13,656 - Epoch: [3][  950/ 1102]    objective_loss 0.334644                                        LR 0.001000    
2022-07-27 12:51:16,560 - Epoch: [3][ 1000/ 1102]    objective_loss 0.332803                                        LR 0.001000    
2022-07-27 12:51:19,584 - Epoch: [3][ 1050/ 1102]    objective_loss 0.330687                                        LR 0.001000    
2022-07-27 12:51:22,480 - Epoch: [3][ 1100/ 1102]    objective_loss 0.328265                                        LR 0.001000    
2022-07-27 12:51:22,630 - Epoch: [3][ 1102/ 1102]    objective_loss 0.328076    Top1 97.142857    Top5 100.000000    LR 0.001000    
2022-07-27 12:51:22,680 - --- validate (epoch=3)-----------
2022-07-27 12:51:22,681 - 7830 samples (64 per mini-batch)
2022-07-27 12:51:23,837 - Epoch: [3][   50/  123]    Loss 0.086557    Top1 97.625000    Top5 100.000000    
2022-07-27 12:51:24,815 - Epoch: [3][  100/  123]    Loss 0.087179    Top1 97.703125    Top5 100.000000    
2022-07-27 12:51:25,243 - Epoch: [3][  123/  123]    Loss 0.087696    Top1 97.637292    Top5 100.000000    
2022-07-27 12:51:25,292 - ==> Top1: 97.637    Top5: 100.000    Loss: 0.088

2022-07-27 12:51:25,294 - ==> Confusion:
[[274   0   0   0   2   0   0   0   0   0   0   0   1   0   0   0   0   0
    4   0   0   0   0   1   0   1   0   0   0]
 [  0 279   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 262   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 262   0   0   0   0   0   0   0   0   0   0   0   0   0   1
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   4   0   0 252   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 271   0   0   0   0   0   0   0   1   1   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 245   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   5 286   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0   0   0 271   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   1 263   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   2   0   0   0   0   0   0   0   0 267   0   0   0   0   0   0   0
    0   0   0   6   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 268   0   0   0   0   0   0
    0   1   0   0   0   0   0   0   0   0   0]
 [  2   0   0   0   2   0   0   0   0   0   0   0 242  42   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   1   0   0   0   9 264   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 239   0   0   0
    1   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 244   0   0
    0   0   0   0   0   0   0   0   2   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 300   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249
    0   0  23   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
  249   0   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    2 265   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  17
    0   0 244   1   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0  21   0   0   0   0   0   0   0
    0   0  17 219   0   0   0   0   0   0   0]
 [  0   2   0   0   0   0   0   0   0   0   4   0   0   0   0   0   0   0
    0   0   0   0 264   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    2   0   0   0   0 284   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 300   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   3   0   0   0   0   0 253   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0 282   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0 274   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0 273]]

2022-07-27 12:51:26,151 - ==> Best [Top1: 97.637   Top5: 100.000  on epoch: 3]
2022-07-27 12:51:26,162 - Saving checkpoint to: jupyter_logging/finetune_asl_base_ev1___2022.07.27-124454/aslclassifier_checkpoint.pth.tar
2022-07-27 12:51:26,205 - Training epoch: 70470 samples (64 per mini-batch)
2022-07-27 12:51:29,506 - Epoch: [4][   50/ 1102]    objective_loss 1.138500                                        LR 0.000750    
2022-07-27 12:51:32,422 - Epoch: [4][  100/ 1102]    objective_loss 1.071571                                        LR 0.000750    
2022-07-27 12:51:35,455 - Epoch: [4][  150/ 1102]    objective_loss 1.033919                                        LR 0.000750    
2022-07-27 12:51:38,417 - Epoch: [4][  200/ 1102]    objective_loss 0.997987                                        LR 0.000750    
2022-07-27 12:51:41,365 - Epoch: [4][  250/ 1102]    objective_loss 0.964665                                        LR 0.000750    
2022-07-27 12:51:44,418 - Epoch: [4][  300/ 1102]    objective_loss 0.936653                                        LR 0.000750    
2022-07-27 12:51:47,415 - Epoch: [4][  350/ 1102]    objective_loss 0.909049                                        LR 0.000750    
2022-07-27 12:51:50,456 - Epoch: [4][  400/ 1102]    objective_loss 0.884015                                        LR 0.000750    
2022-07-27 12:51:53,546 - Epoch: [4][  450/ 1102]    objective_loss 0.861557                                        LR 0.000750    
2022-07-27 12:51:56,539 - Epoch: [4][  500/ 1102]    objective_loss 0.843091                                        LR 0.000750    
2022-07-27 12:51:59,650 - Epoch: [4][  550/ 1102]    objective_loss 0.825447                                        LR 0.000750    
2022-07-27 12:52:02,569 - Epoch: [4][  600/ 1102]    objective_loss 0.807275                                        LR 0.000750    
2022-07-27 12:52:05,595 - Epoch: [4][  650/ 1102]    objective_loss 0.791452                                        LR 0.000750    
2022-07-27 12:52:08,652 - Epoch: [4][  700/ 1102]    objective_loss 0.776630                                        LR 0.000750    
2022-07-27 12:52:11,612 - Epoch: [4][  750/ 1102]    objective_loss 0.762275                                        LR 0.000750    
2022-07-27 12:52:14,707 - Epoch: [4][  800/ 1102]    objective_loss 0.772554                                        LR 0.000750    
2022-07-27 12:52:17,683 - Epoch: [4][  850/ 1102]    objective_loss 0.802752                                        LR 0.000750    
2022-07-27 12:52:20,747 - Epoch: [4][  900/ 1102]    objective_loss 0.827087                                        LR 0.000750    
2022-07-27 12:52:23,697 - Epoch: [4][  950/ 1102]    objective_loss 0.846875                                        LR 0.000750    
2022-07-27 12:52:26,757 - Epoch: [4][ 1000/ 1102]    objective_loss 0.863500                                        LR 0.000750    
2022-07-27 12:52:29,716 - Epoch: [4][ 1050/ 1102]    objective_loss 0.877383                                        LR 0.000750    
2022-07-27 12:52:32,750 - Epoch: [4][ 1100/ 1102]    objective_loss 0.888145                                        LR 0.000750    
2022-07-27 12:52:32,803 - Epoch: [4][ 1102/ 1102]    objective_loss 0.888309    Top1 94.285714    Top5 100.000000    LR 0.000750    
2022-07-27 12:52:32,852 - --- validate (epoch=4)-----------
2022-07-27 12:52:32,853 - 7830 samples (64 per mini-batch)
2022-07-27 12:52:34,040 - Epoch: [4][   50/  123]    Loss 0.912665    Top1 99.031250    Top5 99.968750    
2022-07-27 12:52:35,056 - Epoch: [4][  100/  123]    Loss 0.915680    Top1 98.968750    Top5 99.968750    
2022-07-27 12:52:35,508 - Epoch: [4][  123/  123]    Loss 0.915674    Top1 99.003831    Top5 99.974457    
2022-07-27 12:52:35,559 - ==> Top1: 99.004    Top5: 99.974    Loss: 0.916

2022-07-27 12:52:35,561 - ==> Confusion:
[[283   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0 278   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 262   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 262   0   0   0   0   1   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 256   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 273   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 245   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   2 289   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 272   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 263   0   0   0   1   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 261   0   0   0   0   0   0   0
    0   0   0  11   3   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 269   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 284   4   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0  21 253   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 240   0   0   0
    0   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 246   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2 298   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 251
    0   0  21   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
  250   0   0   0   0   0   0   0   0   0   0]
 [  2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0 265   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1
    0   0 259   2   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0
    0   0   1 255   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 270   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    2   0   0   0   0 284   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 300   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0 256   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0 282   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0 274   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0 273]]

2022-07-27 12:52:36,340 - ==> Best [Top1: 99.004   Top5: 99.974  on epoch: 4]
2022-07-27 12:52:36,346 - Saving checkpoint to: jupyter_logging/finetune_asl_base_ev1___2022.07.27-124454/aslclassifier_qat_checkpoint.pth.tar
2022-07-27 12:52:36,364 - Training epoch: 70470 samples (64 per mini-batch)
2022-07-27 12:52:39,654 - Epoch: [5][   50/ 1102]    objective_loss 1.080022                                        LR 0.000750    
2022-07-27 12:52:42,661 - Epoch: [5][  100/ 1102]    objective_loss 1.067974                                        LR 0.000750    
2022-07-27 12:52:45,740 - Epoch: [5][  150/ 1102]    objective_loss 1.061079                                        LR 0.000750    
2022-07-27 12:52:48,689 - Epoch: [5][  200/ 1102]    objective_loss 1.047137                                        LR 0.000750    
2022-07-27 12:52:51,745 - Epoch: [5][  250/ 1102]    objective_loss 1.035311                                        LR 0.000750    
2022-07-27 12:52:54,696 - Epoch: [5][  300/ 1102]    objective_loss 1.023220                                        LR 0.000750    
2022-07-27 12:52:57,752 - Epoch: [5][  350/ 1102]    objective_loss 1.010938                                        LR 0.000750    
2022-07-27 12:53:00,697 - Epoch: [5][  400/ 1102]    objective_loss 0.999480                                        LR 0.000750    
2022-07-27 12:53:03,764 - Epoch: [5][  450/ 1102]    objective_loss 0.989014                                        LR 0.000750    
2022-07-27 12:53:06,717 - Epoch: [5][  500/ 1102]    objective_loss 0.976751                                        LR 0.000750    
2022-07-27 12:53:09,746 - Epoch: [5][  550/ 1102]    objective_loss 0.966058                                        LR 0.000750    
2022-07-27 12:53:12,682 - Epoch: [5][  600/ 1102]    objective_loss 0.956453                                        LR 0.000750    
2022-07-27 12:53:15,731 - Epoch: [5][  650/ 1102]    objective_loss 0.946712                                        LR 0.000750    
2022-07-27 12:53:18,682 - Epoch: [5][  700/ 1102]    objective_loss 0.937667                                        LR 0.000750    
2022-07-27 12:53:21,714 - Epoch: [5][  750/ 1102]    objective_loss 0.928907                                        LR 0.000750    
2022-07-27 12:53:24,633 - Epoch: [5][  800/ 1102]    objective_loss 0.918991                                        LR 0.000750    
2022-07-27 12:53:27,658 - Epoch: [5][  850/ 1102]    objective_loss 0.909623                                        LR 0.000750    
2022-07-27 12:53:30,606 - Epoch: [5][  900/ 1102]    objective_loss 0.900917                                        LR 0.000750    
2022-07-27 12:53:33,637 - Epoch: [5][  950/ 1102]    objective_loss 0.892680                                        LR 0.000750    
2022-07-27 12:53:36,538 - Epoch: [5][ 1000/ 1102]    objective_loss 0.883858                                        LR 0.000750    
2022-07-27 12:53:39,536 - Epoch: [5][ 1050/ 1102]    objective_loss 0.875250                                        LR 0.000750    
2022-07-27 12:53:42,444 - Epoch: [5][ 1100/ 1102]    objective_loss 0.867433                                        LR 0.000750    
2022-07-27 12:53:42,498 - Epoch: [5][ 1102/ 1102]    objective_loss 0.867231    Top1 94.285714    Top5 100.000000    LR 0.000750    
2022-07-27 12:53:42,546 - --- validate (epoch=5)-----------
2022-07-27 12:53:42,546 - 7830 samples (64 per mini-batch)
2022-07-27 12:53:43,721 - Epoch: [5][   50/  123]    Loss 0.472580    Top1 99.062500    Top5 100.000000    
2022-07-27 12:53:44,692 - Epoch: [5][  100/  123]    Loss 0.471903    Top1 99.078125    Top5 100.000000    
2022-07-27 12:53:45,124 - Epoch: [5][  123/  123]    Loss 0.472116    Top1 99.042146    Top5 100.000000    
2022-07-27 12:53:45,177 - ==> Top1: 99.042    Top5: 100.000    Loss: 0.472

2022-07-27 12:53:45,180 - ==> Confusion:
[[282   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0 278   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 262   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 263   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 255   0   0   0   0   0   0   0   0   0   0   0   0   0
    1   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 273   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 244   0   0   1   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0 291   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 272   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 264   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0   0   0   0   0 274   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 269   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2   0   0   0   0   0   0   0 285   1   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   1   0   0   0   0   0   0  17 256   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 241   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 246   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 300   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 272
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
  250   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    1 266   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  31
    0   0 231   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0  11   0   0   0   0   0   0   0
    0   0   2 243   1   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 270   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    1   0   0   0   0 285   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 300   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   1   0   0   0   0   0 255   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0
    0   0   0   0   0   0   0   0 280   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0 274   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0 273]]

2022-07-27 12:53:46,079 - ==> Best [Top1: 99.042   Top5: 100.000  on epoch: 5]
2022-07-27 12:53:46,089 - Saving checkpoint to: jupyter_logging/finetune_asl_base_ev1___2022.07.27-124454/aslclassifier_qat_checkpoint.pth.tar
2022-07-27 12:53:46,113 - Training epoch: 70470 samples (64 per mini-batch)
2022-07-27 12:53:49,474 - Epoch: [6][   50/ 1102]    objective_loss 0.725279                                        LR 0.000750    
2022-07-27 12:53:52,423 - Epoch: [6][  100/ 1102]    objective_loss 0.703207                                        LR 0.000750    
2022-07-27 12:53:55,510 - Epoch: [6][  150/ 1102]    objective_loss 0.689682                                        LR 0.000750    
2022-07-27 12:53:58,467 - Epoch: [6][  200/ 1102]    objective_loss 0.679946                                        LR 0.000750    
2022-07-27 12:54:01,535 - Epoch: [6][  250/ 1102]    objective_loss 0.668871                                        LR 0.000750    
2022-07-27 12:54:04,463 - Epoch: [6][  300/ 1102]    objective_loss 0.660149                                        LR 0.000750    
2022-07-27 12:54:07,530 - Epoch: [6][  350/ 1102]    objective_loss 0.655466                                        LR 0.000750    
2022-07-27 12:54:10,485 - Epoch: [6][  400/ 1102]    objective_loss 0.648933                                        LR 0.000750    
2022-07-27 12:54:13,532 - Epoch: [6][  450/ 1102]    objective_loss 0.643329                                        LR 0.000750    
2022-07-27 12:54:16,483 - Epoch: [6][  500/ 1102]    objective_loss 0.636627                                        LR 0.000750    
2022-07-27 12:54:19,545 - Epoch: [6][  550/ 1102]    objective_loss 0.630259                                        LR 0.000750    
2022-07-27 12:54:22,484 - Epoch: [6][  600/ 1102]    objective_loss 0.625185                                        LR 0.000750    
2022-07-27 12:54:25,525 - Epoch: [6][  650/ 1102]    objective_loss 0.619222                                        LR 0.000750    
2022-07-27 12:54:28,465 - Epoch: [6][  700/ 1102]    objective_loss 0.614251                                        LR 0.000750    
2022-07-27 12:54:31,511 - Epoch: [6][  750/ 1102]    objective_loss 0.609357                                        LR 0.000750    
2022-07-27 12:54:34,436 - Epoch: [6][  800/ 1102]    objective_loss 0.604034                                        LR 0.000750    
2022-07-27 12:54:37,467 - Epoch: [6][  850/ 1102]    objective_loss 0.599236                                        LR 0.000750    
2022-07-27 12:54:40,434 - Epoch: [6][  900/ 1102]    objective_loss 0.593905                                        LR 0.000750    
2022-07-27 12:54:43,479 - Epoch: [6][  950/ 1102]    objective_loss 0.590232                                        LR 0.000750    
2022-07-27 12:54:46,387 - Epoch: [6][ 1000/ 1102]    objective_loss 0.586264                                        LR 0.000750    
2022-07-27 12:54:49,444 - Epoch: [6][ 1050/ 1102]    objective_loss 0.581634                                        LR 0.000750    
2022-07-27 12:54:52,378 - Epoch: [6][ 1100/ 1102]    objective_loss 0.576561                                        LR 0.000750    
2022-07-27 12:54:52,520 - Epoch: [6][ 1102/ 1102]    objective_loss 0.576453    Top1 95.714286    Top5 100.000000    LR 0.000750    
2022-07-27 12:54:52,587 - --- validate (epoch=6)-----------
2022-07-27 12:54:52,588 - 7830 samples (64 per mini-batch)
2022-07-27 12:54:53,789 - Epoch: [6][   50/  123]    Loss 0.268466    Top1 99.281250    Top5 99.968750    
2022-07-27 12:54:54,866 - Epoch: [6][  100/  123]    Loss 0.268562    Top1 99.265625    Top5 99.984375    
2022-07-27 12:54:55,311 - Epoch: [6][  123/  123]    Loss 0.269302    Top1 99.233716    Top5 99.987229    
2022-07-27 12:54:55,370 - ==> Top1: 99.234    Top5: 99.987    Loss: 0.269

2022-07-27 12:54:55,372 - ==> Confusion:
[[282   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    1   0   0   0   0   0   0   0   0   0   0]
 [  0 277   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   1
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 262   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 263   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 256   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 273   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 244   0   0   1   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   1 290   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 271   1   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   2   0   0 262   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 273   0   0   0   0   0   0   0
    0   0   0   2   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 268   0   0   0   0   0   0
    0   1   0   0   0   0   0   0   0   0   0]
 [  1   0   0   0   0   0   0   0   0   0   0   0 268  19   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   4 269   0   0   0   0
    0   0   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0   0   1   0   0   0   0   0   0   0   0 240   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   4   0   0   0   0   0   0   0 242   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 300   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 268
    0   0   4   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
  250   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0 267   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2
    0   0 260   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   8   0   0   0   0   0   0   0
    0   0   3 246   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 270   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0 286   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 300   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   1   0   0   1   0 254   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0 282   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0 274   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0 273]]

2022-07-27 12:54:56,128 - ==> Best [Top1: 99.234   Top5: 99.987  on epoch: 6]
2022-07-27 12:54:56,138 - Saving checkpoint to: jupyter_logging/finetune_asl_base_ev1___2022.07.27-124454/aslclassifier_qat_checkpoint.pth.tar
2022-07-27 12:54:56,155 - Training epoch: 70470 samples (64 per mini-batch)
2022-07-27 12:54:59,545 - Epoch: [7][   50/ 1102]    objective_loss 0.483655                                        LR 0.000750    
2022-07-27 12:55:02,517 - Epoch: [7][  100/ 1102]    objective_loss 0.481063                                        LR 0.000750    
2022-07-27 12:55:05,576 - Epoch: [7][  150/ 1102]    objective_loss 0.476215                                        LR 0.000750    
2022-07-27 12:55:08,504 - Epoch: [7][  200/ 1102]    objective_loss 0.471647                                        LR 0.000750    
2022-07-27 12:55:11,538 - Epoch: [7][  250/ 1102]    objective_loss 0.467350                                        LR 0.000750    
2022-07-27 12:55:14,569 - Epoch: [7][  300/ 1102]    objective_loss 0.464983                                        LR 0.000750    
2022-07-27 12:55:17,523 - Epoch: [7][  350/ 1102]    objective_loss 0.460720                                        LR 0.000750    
2022-07-27 12:55:20,592 - Epoch: [7][  400/ 1102]    objective_loss 0.458346                                        LR 0.000750    
2022-07-27 12:55:23,532 - Epoch: [7][  450/ 1102]    objective_loss 0.454532                                        LR 0.000750    
2022-07-27 12:55:26,581 - Epoch: [7][  500/ 1102]    objective_loss 0.453387                                        LR 0.000750    
2022-07-27 12:55:29,542 - Epoch: [7][  550/ 1102]    objective_loss 0.450607                                        LR 0.000750    
2022-07-27 12:55:32,599 - Epoch: [7][  600/ 1102]    objective_loss 0.447141                                        LR 0.000750    
2022-07-27 12:55:35,534 - Epoch: [7][  650/ 1102]    objective_loss 0.444743                                        LR 0.000750    
2022-07-27 12:55:38,579 - Epoch: [7][  700/ 1102]    objective_loss 0.441518                                        LR 0.000750    
2022-07-27 12:55:41,513 - Epoch: [7][  750/ 1102]    objective_loss 0.438587                                        LR 0.000750    
2022-07-27 12:55:44,522 - Epoch: [7][  800/ 1102]    objective_loss 0.435116                                        LR 0.000750    
2022-07-27 12:55:47,422 - Epoch: [7][  850/ 1102]    objective_loss 0.431543                                        LR 0.000750    
2022-07-27 12:55:50,465 - Epoch: [7][  900/ 1102]    objective_loss 0.428290                                        LR 0.000750    
2022-07-27 12:55:53,467 - Epoch: [7][  950/ 1102]    objective_loss 0.425335                                        LR 0.000750    
2022-07-27 12:55:56,536 - Epoch: [7][ 1000/ 1102]    objective_loss 0.421926                                        LR 0.000750    
2022-07-27 12:55:59,471 - Epoch: [7][ 1050/ 1102]    objective_loss 0.419452                                        LR 0.000750    
2022-07-27 12:56:02,491 - Epoch: [7][ 1100/ 1102]    objective_loss 0.417668                                        LR 0.000750    
2022-07-27 12:56:02,544 - Epoch: [7][ 1102/ 1102]    objective_loss 0.417655    Top1 94.285714    Top5 98.571429    LR 0.000750    
2022-07-27 12:56:02,594 - --- validate (epoch=7)-----------
2022-07-27 12:56:02,594 - 7830 samples (64 per mini-batch)
2022-07-27 12:56:03,804 - Epoch: [7][   50/  123]    Loss 0.174864    Top1 99.031250    Top5 99.968750    
2022-07-27 12:56:04,772 - Epoch: [7][  100/  123]    Loss 0.181625    Top1 98.718750    Top5 99.953125    
2022-07-27 12:56:05,204 - Epoch: [7][  123/  123]    Loss 0.180941    Top1 98.722861    Top5 99.961686    
2022-07-27 12:56:05,252 - ==> Top1: 98.723    Top5: 99.962    Loss: 0.181

2022-07-27 12:56:05,254 - ==> Confusion:
[[283   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0 278   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 259   0   0   0   0   0   0   0   0   1   0   0   2   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 263   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 256   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 272   0   0   0   0   0   0   0   0   0   0   0   0
    0   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 245   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0 291   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 272   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   1 263   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0   0   0   0   0 261   0   0   0   0   0   0   0
    0   0   0   5   8   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 268   0   0   0   0   0   0
    0   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 288   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0  35 239   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 240   0   0   0
    0   0   0   0   0   0   0   1   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 246   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  12 285   0
    0   0   0   0   0   0   0   0   3   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 272
    0   0   0   0   0   0   0   0   0   0   0]
 [  6   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0
  243   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0 266   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1
    0   0 255   3   1   2   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0
    0   0   0 256   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 270   0   0   0   0   0   0]
 [  2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    3   0   0   0   0 281   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   1   0   0   0   0 299   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   2   0   0   0   1   0 253   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0 282   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0 274   0]
 [  0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0 272]]

2022-07-27 12:56:05,995 - ==> Best [Top1: 99.234   Top5: 99.987  on epoch: 6]
2022-07-27 12:56:06,006 - Saving checkpoint to: jupyter_logging/finetune_asl_base_ev1___2022.07.27-124454/aslclassifier_qat_checkpoint.pth.tar
2022-07-27 12:56:06,027 - Training epoch: 70470 samples (64 per mini-batch)
2022-07-27 12:56:09,340 - Epoch: [8][   50/ 1102]    objective_loss 0.344198                                        LR 0.000563    
2022-07-27 12:56:12,276 - Epoch: [8][  100/ 1102]    objective_loss 0.343635                                        LR 0.000563    
2022-07-27 12:56:15,302 - Epoch: [8][  150/ 1102]    objective_loss 0.345706                                        LR 0.000563    
2022-07-27 12:56:18,293 - Epoch: [8][  200/ 1102]    objective_loss 0.341290                                        LR 0.000563    
2022-07-27 12:56:21,346 - Epoch: [8][  250/ 1102]    objective_loss 0.339509                                        LR 0.000563    
2022-07-27 12:56:24,294 - Epoch: [8][  300/ 1102]    objective_loss 0.337433                                        LR 0.000563    
2022-07-27 12:56:27,340 - Epoch: [8][  350/ 1102]    objective_loss 0.334802                                        LR 0.000563    
2022-07-27 12:56:30,247 - Epoch: [8][  400/ 1102]    objective_loss 0.334513                                        LR 0.000563    
2022-07-27 12:56:33,303 - Epoch: [8][  450/ 1102]    objective_loss 0.338176                                        LR 0.000563    
2022-07-27 12:56:36,190 - Epoch: [8][  500/ 1102]    objective_loss 0.404482                                        LR 0.000563    
2022-07-27 12:56:39,224 - Epoch: [8][  550/ 1102]    objective_loss 0.457395                                        LR 0.000563    
2022-07-27 12:56:42,160 - Epoch: [8][  600/ 1102]    objective_loss 0.499679                                        LR 0.000563    
2022-07-27 12:56:45,187 - Epoch: [8][  650/ 1102]    objective_loss 0.534408                                        LR 0.000563    
2022-07-27 12:56:48,128 - Epoch: [8][  700/ 1102]    objective_loss 0.563094                                        LR 0.000563    
2022-07-27 12:56:51,153 - Epoch: [8][  750/ 1102]    objective_loss 0.587370                                        LR 0.000563    
2022-07-27 12:56:54,099 - Epoch: [8][  800/ 1102]    objective_loss 0.608271                                        LR 0.000563    
2022-07-27 12:56:57,139 - Epoch: [8][  850/ 1102]    objective_loss 0.625496                                        LR 0.000563    
2022-07-27 12:57:00,069 - Epoch: [8][  900/ 1102]    objective_loss 0.640106                                        LR 0.000563    
2022-07-27 12:57:03,110 - Epoch: [8][  950/ 1102]    objective_loss 0.652522                                        LR 0.000563    
2022-07-27 12:57:06,034 - Epoch: [8][ 1000/ 1102]    objective_loss 0.663791                                        LR 0.000563    
2022-07-27 12:57:09,057 - Epoch: [8][ 1050/ 1102]    objective_loss 0.673213                                        LR 0.000563    
2022-07-27 12:57:11,954 - Epoch: [8][ 1100/ 1102]    objective_loss 0.681650                                        LR 0.000563    
2022-07-27 12:57:12,096 - Epoch: [8][ 1102/ 1102]    objective_loss 0.681914    Top1 100.000000    Top5 100.000000    LR 0.000563    
2022-07-27 12:57:12,146 - --- validate (epoch=8)-----------
2022-07-27 12:57:12,146 - 7830 samples (64 per mini-batch)
2022-07-27 12:57:13,346 - Epoch: [8][   50/  123]    Loss 0.672711    Top1 99.687500    Top5 100.000000    
2022-07-27 12:57:14,371 - Epoch: [8][  100/  123]    Loss 0.670007    Top1 99.765625    Top5 100.000000    
2022-07-27 12:57:14,791 - Epoch: [8][  123/  123]    Loss 0.670854    Top1 99.757344    Top5 100.000000    
2022-07-27 12:57:14,843 - ==> Top1: 99.757    Top5: 100.000    Loss: 0.671

2022-07-27 12:57:14,845 - ==> Confusion:
[[283   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0 279   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 262   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 263   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 256   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 273   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 245   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0 291   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 272   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 264   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 272   0   0   0   0   0   0   0
    0   0   0   0   3   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 268   0   0   0   0   0   0
    0   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 287   1   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   8 265   0   0   0   0
    0   0   0   0   0   0   0   0   1   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 241   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 246   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2 298   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 271
    0   0   1   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
  250   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0 267   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0 261   0   0   1   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0
    0   0   0 256   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 270   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0 286   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 300   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0 256   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0 282   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0 274   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0 273]]

2022-07-27 12:57:15,763 - ==> Best [Top1: 99.757   Top5: 100.000  on epoch: 8]
2022-07-27 12:57:15,774 - Saving checkpoint to: jupyter_logging/finetune_asl_base_ev1___2022.07.27-124454/aslclassifier_qat_checkpoint.pth.tar
2022-07-27 12:57:15,798 - Training epoch: 70470 samples (64 per mini-batch)
2022-07-27 12:57:19,105 - Epoch: [9][   50/ 1102]    objective_loss 0.845657                                        LR 0.000563    
2022-07-27 12:57:22,037 - Epoch: [9][  100/ 1102]    objective_loss 0.835704                                        LR 0.000563    
2022-07-27 12:57:25,094 - Epoch: [9][  150/ 1102]    objective_loss 0.828575                                        LR 0.000563    
2022-07-27 12:57:28,143 - Epoch: [9][  200/ 1102]    objective_loss 0.825734                                        LR 0.000563    
2022-07-27 12:57:31,134 - Epoch: [9][  250/ 1102]    objective_loss 0.821211                                        LR 0.000563    
2022-07-27 12:57:34,230 - Epoch: [9][  300/ 1102]    objective_loss 0.814177                                        LR 0.000563    
2022-07-27 12:57:37,195 - Epoch: [9][  350/ 1102]    objective_loss 0.810404                                        LR 0.000563    
2022-07-27 12:57:40,286 - Epoch: [9][  400/ 1102]    objective_loss 0.808108                                        LR 0.000563    
2022-07-27 12:57:43,221 - Epoch: [9][  450/ 1102]    objective_loss 0.805320                                        LR 0.000563    
2022-07-27 12:57:46,306 - Epoch: [9][  500/ 1102]    objective_loss 0.802199                                        LR 0.000563    
2022-07-27 12:57:49,232 - Epoch: [9][  550/ 1102]    objective_loss 0.798655                                        LR 0.000563    
2022-07-27 12:57:52,213 - Epoch: [9][  600/ 1102]    objective_loss 0.795365                                        LR 0.000563    
2022-07-27 12:57:55,143 - Epoch: [9][  650/ 1102]    objective_loss 0.791830                                        LR 0.000563    
2022-07-27 12:57:58,174 - Epoch: [9][  700/ 1102]    objective_loss 0.787615                                        LR 0.000563    
2022-07-27 12:58:01,054 - Epoch: [9][  750/ 1102]    objective_loss 0.783593                                        LR 0.000563    
2022-07-27 12:58:04,118 - Epoch: [9][  800/ 1102]    objective_loss 0.779889                                        LR 0.000563    
2022-07-27 12:58:07,091 - Epoch: [9][  850/ 1102]    objective_loss 0.775783                                        LR 0.000563    
2022-07-27 12:58:10,155 - Epoch: [9][  900/ 1102]    objective_loss 0.772288                                        LR 0.000563    
2022-07-27 12:58:13,121 - Epoch: [9][  950/ 1102]    objective_loss 0.767976                                        LR 0.000563    
2022-07-27 12:58:16,114 - Epoch: [9][ 1000/ 1102]    objective_loss 0.764482                                        LR 0.000563    
2022-07-27 12:58:19,039 - Epoch: [9][ 1050/ 1102]    objective_loss 0.760245                                        LR 0.000563    
2022-07-27 12:58:22,019 - Epoch: [9][ 1100/ 1102]    objective_loss 0.756518                                        LR 0.000563    
2022-07-27 12:58:22,128 - Epoch: [9][ 1102/ 1102]    objective_loss 0.756389    Top1 98.571429    Top5 100.000000    LR 0.000563    
2022-07-27 12:58:22,177 - --- validate (epoch=9)-----------
2022-07-27 12:58:22,177 - 7830 samples (64 per mini-batch)
2022-07-27 12:58:23,370 - Epoch: [9][   50/  123]    Loss 0.482668    Top1 99.656250    Top5 100.000000    
2022-07-27 12:58:24,323 - Epoch: [9][  100/  123]    Loss 0.485165    Top1 99.625000    Top5 100.000000    
2022-07-27 12:58:24,749 - Epoch: [9][  123/  123]    Loss 0.485624    Top1 99.629630    Top5 100.000000    
2022-07-27 12:58:24,797 - ==> Top1: 99.630    Top5: 100.000    Loss: 0.486

2022-07-27 12:58:24,799 - ==> Confusion:
[[282   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0 279   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 262   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 263   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 256   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 273   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 243   2   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0 291   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 272   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 264   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 275   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 268   0   0   0   0   0   0
    0   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 282   6   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   2 272   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 241   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 246   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1 299   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 272
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
  250   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0 267   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   5
    0   0 257   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   7   0   0   0   0   0   0   0
    0   0   2 248   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0
    0   0   0   0 269   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0 286   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 300   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   1   0 255   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0 282   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0 274   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0 273]]

2022-07-27 12:58:25,659 - ==> Best [Top1: 99.757   Top5: 100.000  on epoch: 8]
2022-07-27 12:58:25,670 - Saving checkpoint to: jupyter_logging/finetune_asl_base_ev1___2022.07.27-124454/aslclassifier_qat_checkpoint.pth.tar
2022-07-27 12:58:25,692 - Training epoch: 70470 samples (64 per mini-batch)
2022-07-27 12:58:29,009 - Epoch: [10][   50/ 1102]    objective_loss 0.662468                                        LR 0.000563    
2022-07-27 12:58:31,951 - Epoch: [10][  100/ 1102]    objective_loss 0.664899                                        LR 0.000563    
2022-07-27 12:58:34,990 - Epoch: [10][  150/ 1102]    objective_loss 0.663163                                        LR 0.000563    
2022-07-27 12:58:37,956 - Epoch: [10][  200/ 1102]    objective_loss 0.661098                                        LR 0.000563    
2022-07-27 12:58:41,006 - Epoch: [10][  250/ 1102]    objective_loss 0.658528                                        LR 0.000563    
2022-07-27 12:58:43,925 - Epoch: [10][  300/ 1102]    objective_loss 0.654872                                        LR 0.000563    
2022-07-27 12:58:46,991 - Epoch: [10][  350/ 1102]    objective_loss 0.651570                                        LR 0.000563    
2022-07-27 12:58:49,961 - Epoch: [10][  400/ 1102]    objective_loss 0.647979                                        LR 0.000563    
2022-07-27 12:58:53,065 - Epoch: [10][  450/ 1102]    objective_loss 0.644291                                        LR 0.000563    
2022-07-27 12:58:56,076 - Epoch: [10][  500/ 1102]    objective_loss 0.641094                                        LR 0.000563    
2022-07-27 12:58:59,100 - Epoch: [10][  550/ 1102]    objective_loss 0.638698                                        LR 0.000563    
2022-07-27 12:59:01,971 - Epoch: [10][  600/ 1102]    objective_loss 0.636098                                        LR 0.000563    
2022-07-27 12:59:04,961 - Epoch: [10][  650/ 1102]    objective_loss 0.633348                                        LR 0.000563    
2022-07-27 12:59:07,838 - Epoch: [10][  700/ 1102]    objective_loss 0.630736                                        LR 0.000563    
2022-07-27 12:59:10,863 - Epoch: [10][  750/ 1102]    objective_loss 0.627730                                        LR 0.000563    
2022-07-27 12:59:13,741 - Epoch: [10][  800/ 1102]    objective_loss 0.625021                                        LR 0.000563    
2022-07-27 12:59:16,768 - Epoch: [10][  850/ 1102]    objective_loss 0.622383                                        LR 0.000563    
2022-07-27 12:59:19,720 - Epoch: [10][  900/ 1102]    objective_loss 0.619754                                        LR 0.000563    
2022-07-27 12:59:22,777 - Epoch: [10][  950/ 1102]    objective_loss 0.617579                                        LR 0.000563    
2022-07-27 12:59:25,736 - Epoch: [10][ 1000/ 1102]    objective_loss 0.614985                                        LR 0.000563    
2022-07-27 12:59:28,787 - Epoch: [10][ 1050/ 1102]    objective_loss 0.612394                                        LR 0.000563    
2022-07-27 12:59:31,697 - Epoch: [10][ 1100/ 1102]    objective_loss 0.609916                                        LR 0.000563    
2022-07-27 12:59:31,773 - Epoch: [10][ 1102/ 1102]    objective_loss 0.609937    Top1 98.571429    Top5 100.000000    LR 0.000563    
2022-07-27 12:59:31,822 - --- validate (epoch=10)-----------
2022-07-27 12:59:31,822 - 7830 samples (64 per mini-batch)
2022-07-27 12:59:33,006 - Epoch: [10][   50/  123]    Loss 0.361479    Top1 99.562500    Top5 100.000000    
2022-07-27 12:59:34,003 - Epoch: [10][  100/  123]    Loss 0.358481    Top1 99.640625    Top5 99.984375    
2022-07-27 12:59:34,429 - Epoch: [10][  123/  123]    Loss 0.357492    Top1 99.693487    Top5 99.987229    
2022-07-27 12:59:34,477 - ==> Top1: 99.693    Top5: 99.987    Loss: 0.357

2022-07-27 12:59:34,479 - ==> Confusion:
[[282   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0 279   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 262   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 263   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   1   0   0 254   0   0   0   0   0   0   0   0   0   0   0   0   0
    1   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 273   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 241   4   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0 291   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 272   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 264   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 275   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 268   0   0   0   0   0   0
    0   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 285   3   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   4 270   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 241   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 246   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 300   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 270
    0   0   2   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0
  249   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0
    0 266   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0 262   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   4   0   0   0   0   0   0   0
    0   0   1 252   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 270   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0 286   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 300   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0 256   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0 282   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0 274   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0 273]]

2022-07-27 12:59:35,216 - ==> Best [Top1: 99.757   Top5: 100.000  on epoch: 8]
2022-07-27 12:59:35,226 - Saving checkpoint to: jupyter_logging/finetune_asl_base_ev1___2022.07.27-124454/aslclassifier_qat_checkpoint.pth.tar
2022-07-27 12:59:35,248 - Training epoch: 70470 samples (64 per mini-batch)
2022-07-27 12:59:38,591 - Epoch: [11][   50/ 1102]    objective_loss 0.567290                                        LR 0.000563    
2022-07-27 12:59:41,521 - Epoch: [11][  100/ 1102]    objective_loss 0.554716                                        LR 0.000563    
2022-07-27 12:59:44,596 - Epoch: [11][  150/ 1102]    objective_loss 0.547825                                        LR 0.000563    
2022-07-27 12:59:47,545 - Epoch: [11][  200/ 1102]    objective_loss 0.545625                                        LR 0.000563    
2022-07-27 12:59:50,619 - Epoch: [11][  250/ 1102]    objective_loss 0.546162                                        LR 0.000563    
2022-07-27 12:59:53,613 - Epoch: [11][  300/ 1102]    objective_loss 0.546842                                        LR 0.000563    
2022-07-27 12:59:56,666 - Epoch: [11][  350/ 1102]    objective_loss 0.544074                                        LR 0.000563    
2022-07-27 12:59:59,605 - Epoch: [11][  400/ 1102]    objective_loss 0.540559                                        LR 0.000563    
2022-07-27 13:00:02,647 - Epoch: [11][  450/ 1102]    objective_loss 0.538863                                        LR 0.000563    
2022-07-27 13:00:05,537 - Epoch: [11][  500/ 1102]    objective_loss 0.537529                                        LR 0.000563    
2022-07-27 13:00:08,616 - Epoch: [11][  550/ 1102]    objective_loss 0.535463                                        LR 0.000563    
2022-07-27 13:00:11,528 - Epoch: [11][  600/ 1102]    objective_loss 0.533483                                        LR 0.000563    
2022-07-27 13:00:14,528 - Epoch: [11][  650/ 1102]    objective_loss 0.531479                                        LR 0.000563    
2022-07-27 13:00:17,443 - Epoch: [11][  700/ 1102]    objective_loss 0.528428                                        LR 0.000563    
2022-07-27 13:00:20,446 - Epoch: [11][  750/ 1102]    objective_loss 0.524809                                        LR 0.000563    
2022-07-27 13:00:23,378 - Epoch: [11][  800/ 1102]    objective_loss 0.522657                                        LR 0.000563    
2022-07-27 13:00:26,405 - Epoch: [11][  850/ 1102]    objective_loss 0.520134                                        LR 0.000563    
2022-07-27 13:00:29,298 - Epoch: [11][  900/ 1102]    objective_loss 0.518272                                        LR 0.000563    
2022-07-27 13:00:32,317 - Epoch: [11][  950/ 1102]    objective_loss 0.516407                                        LR 0.000563    
2022-07-27 13:00:35,211 - Epoch: [11][ 1000/ 1102]    objective_loss 0.514615                                        LR 0.000563    
2022-07-27 13:00:38,239 - Epoch: [11][ 1050/ 1102]    objective_loss 0.511994                                        LR 0.000563    
2022-07-27 13:00:41,131 - Epoch: [11][ 1100/ 1102]    objective_loss 0.509420                                        LR 0.000563    
2022-07-27 13:00:41,272 - Epoch: [11][ 1102/ 1102]    objective_loss 0.509183    Top1 98.571429    Top5 100.000000    LR 0.000563    
2022-07-27 13:00:41,320 - --- validate (epoch=11)-----------
2022-07-27 13:00:41,321 - 7830 samples (64 per mini-batch)
2022-07-27 13:00:42,458 - Epoch: [11][   50/  123]    Loss 0.268186    Top1 99.687500    Top5 99.968750    
2022-07-27 13:00:43,459 - Epoch: [11][  100/  123]    Loss 0.268402    Top1 99.734375    Top5 99.968750    
2022-07-27 13:00:43,879 - Epoch: [11][  123/  123]    Loss 0.268725    Top1 99.719029    Top5 99.974457    
2022-07-27 13:00:43,934 - ==> Top1: 99.719    Top5: 99.974    Loss: 0.269

2022-07-27 13:00:43,935 - ==> Confusion:
[[283   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0 279   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 262   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 263   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 255   0   0   0   0   0   0   0   0   0   0   0   0   0
    1   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 273   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 245   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0 291   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 272   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 264   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 274   0   0   0   0   0   0   0
    0   0   0   1   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 268   0   0   0   0   0   0
    0   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 281   7   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   4 270   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 241   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 246   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   3 297   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 271
    0   0   1   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
  250   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0 267   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0 262   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0
    0   0   0 256   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0 270   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    2   0   0   0   0 284   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 300   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   1   0 255   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0 282   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0 274   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0 273]]

2022-07-27 13:00:44,671 - ==> Best [Top1: 99.757   Top5: 100.000  on epoch: 8]
2022-07-27 13:00:44,681 - Saving checkpoint to: jupyter_logging/finetune_asl_base_ev1___2022.07.27-124454/aslclassifier_qat_checkpoint.pth.tar
2022-07-27 13:00:44,703 - Training time: 0:13:54.727832
2022-07-27 13:03:32,305 - => loading checkpoint jupyter_logging/SSL___2022.07.15-171757/classifierbackbonenet_qat_checkpoint.pth.tar
2022-07-27 13:03:32,311 - => Checkpoint contents:
+----------------------+-------------+---------------------------+
| Key                  | Type        | Value                     |
|----------------------+-------------+---------------------------|
| arch                 | str         | classifierbackbonenet_qat |
| compression_sched    | dict        |                           |
| epoch                | int         | 99                        |
| extras               | dict        |                           |
| optimizer_state_dict | dict        |                           |
| optimizer_type       | type        | Adam                      |
| state_dict           | OrderedDict |                           |
+----------------------+-------------+---------------------------+

2022-07-27 13:03:32,311 - => Checkpoint['extras'] contents:
+-------+--------+---------+
| Key   | Type   |   Value |
|-------+--------+---------|
| epoch | int    |      99 |
+-------+--------+---------+

2022-07-27 13:03:32,312 - Loaded compression schedule from checkpoint (epoch 99)
2022-07-27 13:03:32,317 - => loaded 'state_dict' from checkpoint 'jupyter_logging/SSL___2022.07.15-171757/classifierbackbonenet_qat_checkpoint.pth.tar'
2022-07-27 13:03:32,343 - => loading checkpoint jupyter_logging/finetune_asl_base_ev1___2022.07.27-124454/aslclassifier_checkpoint.pth.tar
2022-07-27 13:03:32,348 - => Checkpoint contents:
+----------------------+-------------+---------------+
| Key                  | Type        | Value         |
|----------------------+-------------+---------------|
| arch                 | str         | aslclassifier |
| compression_sched    | dict        |               |
| epoch                | int         | 3             |
| extras               | dict        |               |
| optimizer_state_dict | dict        |               |
| optimizer_type       | type        | Adam          |
| state_dict           | OrderedDict |               |
+----------------------+-------------+---------------+

2022-07-27 13:03:32,349 - => Checkpoint['extras'] contents:
+--------------+--------+---------+
| Key          | Type   |   Value |
|--------------+--------+---------|
| best_epoch   | int    |  3      |
| best_top1    | float  | 97.6373 |
| current_top1 | float  | 97.6373 |
+--------------+--------+---------+

2022-07-27 13:03:32,349 - Loaded compression schedule from checkpoint (epoch 3)
2022-07-27 13:03:32,395 - => loaded 'state_dict' from checkpoint 'jupyter_logging/finetune_asl_base_ev1___2022.07.27-124454/aslclassifier_checkpoint.pth.tar'
2022-07-27 13:03:32,407 - 8700 samples (64 per mini-batch)
2022-07-27 13:03:33,664 - Test: [   50/  136]    Loss 0.082979    Top1 97.750000    Top5 99.968750    
2022-07-27 13:03:34,658 - Test: [  100/  136]    Loss 0.084225    Top1 97.687500    Top5 99.984375    
2022-07-27 13:03:35,369 - Test: [  136/  136]    Loss 0.081686    Top1 97.839080    Top5 99.988506    
2022-07-27 13:03:35,416 - ==> Top1: 97.839    Top5: 99.989    Loss: 0.082

2022-07-27 13:03:35,418 - ==> Confusion:
[[293   0   0   0   1   0   0   0   0   0   0   0   2   0   0   0   0   0
    3   0   0   0   0   1   0   0   0   0   0]
 [  0 300   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 300   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 299   0   0   0   0   0   0   0   0   0   0   0   0   0   1
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   1   0   0 299   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 294   0   0   0   0   0   0   0   5   0   0   0   0
    0   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 299   0   0   0   0   0   0   0   0   0   0   1
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   6 294   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 300   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 300   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   2   0   0   0   0   0   0   0   0 290   0   0   0   0   0   0   1
    0   0   0   7   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 299   0   0   0   0   0   0
    0   1   0   0   0   0   0   0   0   0   0]
 [  1   0   0   0   2   0   0   0   0   0   0   0 259  37   0   0   0   0
    0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   2   0   0   0  11 287   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 299   0   0   0
    1   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 294   0   0
    0   0   0   0   0   0   0   0   6   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 300   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 281
    0   0  19   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
  298   0   0   0   0   2   0   0   0   0   0]
 [  2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0 296   0   0   0   0   0   2   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16
    0   0 284   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0  23   0   0   0   0   0   0   0
    0   0  19 258   0   0   0   0   0   0   0]
 [  0   2   0   0   0   0   0   0   0   0   3   0   0   0   0   0   0   0
    0   0   0   0 295   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    4   0   0   0   0 296   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 300   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    1   1   0   0   0   0   0 298   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0 300   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0 300   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0 300]]

2022-07-27 13:03:35,418 - ==> Test Set [Top1: 97.839   Top5: 99.989  on test set]
2022-07-27 13:05:12,618 - => loading checkpoint jupyter_logging/SSL___2022.07.15-171757/classifierbackbonenet_qat_checkpoint.pth.tar
2022-07-27 13:05:12,625 - => Checkpoint contents:
+----------------------+-------------+---------------------------+
| Key                  | Type        | Value                     |
|----------------------+-------------+---------------------------|
| arch                 | str         | classifierbackbonenet_qat |
| compression_sched    | dict        |                           |
| epoch                | int         | 99                        |
| extras               | dict        |                           |
| optimizer_state_dict | dict        |                           |
| optimizer_type       | type        | Adam                      |
| state_dict           | OrderedDict |                           |
+----------------------+-------------+---------------------------+

2022-07-27 13:05:12,625 - => Checkpoint['extras'] contents:
+-------+--------+---------+
| Key   | Type   |   Value |
|-------+--------+---------|
| epoch | int    |      99 |
+-------+--------+---------+

2022-07-27 13:05:12,626 - Loaded compression schedule from checkpoint (epoch 99)
2022-07-27 13:05:12,631 - => loaded 'state_dict' from checkpoint 'jupyter_logging/SSL___2022.07.15-171757/classifierbackbonenet_qat_checkpoint.pth.tar'
2022-07-27 13:05:12,654 - => loading checkpoint jupyter_logging/finetune_asl_base_ev1___2022.07.27-124454/aslclassifier_best.pth.tar
2022-07-27 13:05:12,659 - => Checkpoint contents:
+----------------------+-------------+---------------+
| Key                  | Type        | Value         |
|----------------------+-------------+---------------|
| arch                 | str         | aslclassifier |
| compression_sched    | dict        |               |
| epoch                | int         | 3             |
| extras               | dict        |               |
| optimizer_state_dict | dict        |               |
| optimizer_type       | type        | Adam          |
| state_dict           | OrderedDict |               |
+----------------------+-------------+---------------+

2022-07-27 13:05:12,660 - => Checkpoint['extras'] contents:
+--------------+--------+---------+
| Key          | Type   |   Value |
|--------------+--------+---------|
| best_epoch   | int    |  3      |
| best_top1    | float  | 97.6373 |
| current_top1 | float  | 97.6373 |
+--------------+--------+---------+

2022-07-27 13:05:12,660 - Loaded compression schedule from checkpoint (epoch 3)
2022-07-27 13:05:12,706 - => loaded 'state_dict' from checkpoint 'jupyter_logging/finetune_asl_base_ev1___2022.07.27-124454/aslclassifier_best.pth.tar'
2022-07-27 13:05:12,717 - 8700 samples (64 per mini-batch)
2022-07-27 13:05:13,871 - Test: [   50/  136]    Loss 0.084992    Top1 97.593750    Top5 99.968750    
2022-07-27 13:05:14,857 - Test: [  100/  136]    Loss 0.083930    Top1 97.734375    Top5 99.984375    
2022-07-27 13:05:15,548 - Test: [  136/  136]    Loss 0.081705    Top1 97.839080    Top5 99.988506    
2022-07-27 13:05:15,596 - ==> Top1: 97.839    Top5: 99.989    Loss: 0.082

2022-07-27 13:05:15,598 - ==> Confusion:
[[293   0   0   0   1   0   0   0   0   0   0   0   2   0   0   0   0   0
    3   0   0   0   0   1   0   0   0   0   0]
 [  0 300   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 300   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 299   0   0   0   0   0   0   0   0   0   0   0   0   0   1
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   1   0   0 299   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 294   0   0   0   0   0   0   0   5   0   0   0   0
    0   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 299   0   0   0   0   0   0   0   0   0   0   1
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   6 294   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 300   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 300   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   2   0   0   0   0   0   0   0   0 290   0   0   0   0   0   0   1
    0   0   0   7   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 299   0   0   0   0   0   0
    0   1   0   0   0   0   0   0   0   0   0]
 [  1   0   0   0   2   0   0   0   0   0   0   0 259  37   0   0   0   0
    0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   2   0   0   0  11 287   0   0   0   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 299   0   0   0
    1   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 294   0   0
    0   0   0   0   0   0   0   0   6   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 300   0
    0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 281
    0   0  19   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
  298   0   0   0   0   2   0   0   0   0   0]
 [  2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0 296   0   0   0   0   0   2   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16
    0   0 284   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0  23   0   0   0   0   0   0   0
    0   0  19 258   0   0   0   0   0   0   0]
 [  0   2   0   0   0   0   0   0   0   0   3   0   0   0   0   0   0   0
    0   0   0   0 295   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    4   0   0   0   0 296   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0 300   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    1   1   0   0   0   0   0 298   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0 300   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0 300   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0   0   0   0 300]]

2022-07-27 13:05:15,599 - ==> Test Set [Top1: 97.839   Top5: 99.989  on test set]
2022-07-27 13:06:44,160 - => loading checkpoint jupyter_logging/SSL___2022.07.15-171757/classifierbackbonenet_qat_checkpoint.pth.tar
2022-07-27 13:06:44,167 - => Checkpoint contents:
+----------------------+-------------+---------------------------+
| Key                  | Type        | Value                     |
|----------------------+-------------+---------------------------|
| arch                 | str         | classifierbackbonenet_qat |
| compression_sched    | dict        |                           |
| epoch                | int         | 99                        |
| extras               | dict        |                           |
| optimizer_state_dict | dict        |                           |
| optimizer_type       | type        | Adam                      |
| state_dict           | OrderedDict |                           |
+----------------------+-------------+---------------------------+

2022-07-27 13:06:44,167 - => Checkpoint['extras'] contents:
+-------+--------+---------+
| Key   | Type   |   Value |
|-------+--------+---------|
| epoch | int    |      99 |
+-------+--------+---------+

2022-07-27 13:06:44,168 - Loaded compression schedule from checkpoint (epoch 99)
2022-07-27 13:06:44,172 - => loaded 'state_dict' from checkpoint 'jupyter_logging/SSL___2022.07.15-171757/classifierbackbonenet_qat_checkpoint.pth.tar'
2022-07-27 13:06:44,193 - => loading checkpoint jupyter_logging/finetune_asl_base_ev1___2022.07.27-124454/aslclassifier_best.pth.tar
2022-07-27 13:06:44,198 - => Checkpoint contents:
+----------------------+-------------+---------------+
| Key                  | Type        | Value         |
|----------------------+-------------+---------------|
| arch                 | str         | aslclassifier |
| compression_sched    | dict        |               |
| epoch                | int         | 3             |
| extras               | dict        |               |
| optimizer_state_dict | dict        |               |
| optimizer_type       | type        | Adam          |
| state_dict           | OrderedDict |               |
+----------------------+-------------+---------------+

2022-07-27 13:06:44,199 - => Checkpoint['extras'] contents:
+--------------+--------+---------+
| Key          | Type   |   Value |
|--------------+--------+---------|
| best_epoch   | int    |  3      |
| best_top1    | float  | 97.6373 |
| current_top1 | float  | 97.6373 |
+--------------+--------+---------+

2022-07-27 13:06:44,200 - Loaded compression schedule from checkpoint (epoch 3)
2022-07-27 13:06:44,244 - => loaded 'state_dict' from checkpoint 'jupyter_logging/finetune_asl_base_ev1___2022.07.27-124454/aslclassifier_best.pth.tar'
2022-07-27 13:06:44,255 - 324 samples (64 per mini-batch)
2022-07-27 13:06:44,488 - Test: [    6/    6]    Loss 2.552679    Top1 47.916667    Top5 81.770833    
2022-07-27 13:06:44,589 - ==> Top1: 50.000    Top5: 82.407    Loss: 2.384

2022-07-27 13:06:44,590 - ==> Confusion:
[[ 3  0  0  1  0  0  0  0  1  0  0  0  0  1  0  0  0  0  0  3  0  1  1  0
   1  0  0  0  0]
 [ 0  2  0  0  0  0  0  0  2  0  4  0  0  0  0  0  0  0  0  0  0  0  3  0
   0  0  0  0  0]
 [ 0  0  0  7  0  0  0  0  0  0  0  3  0  0  1  0  0  0  0  0  0  0  0  0
   0  0  0  0  0]
 [ 0  0  0 11  0  0  0  0  0  0  2  1  0  0  0  0  0  0  0  0  0  0  0  0
   0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  1  0  0  0  1
   0  0  0  0  0]
 [ 0  0  0  0  0  4  0  0  1  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0
   0  0  0  0  0]
 [ 0  0  0  0  0  0  0  4  0  1  0  0  0  0  0  5  0  0  0  0  0  0  0  0
   0  0  0  0  0]
 [ 0  0  0  0  0  0  0  7  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0
   0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  5  3  0  0  0  0  1  0  0  0  0  0  0  0  0  0
   0  0  1  0  0]
 [ 0  0  0  0  0  0  0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0
   0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0 11  0  0  0  0  0  0  0  0  0  0  0  0  0
   0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0  1  2  0  0
   0  1  0  0  2]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0 11  0  0  0  0  0  0  0  0  0  0
   0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  1  0  0  0  9  0  0  0  0  0  0  0  0  0  0
   0  0  0  0  1]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0  0  1  0  1  0  0
   0  0  5  0  1]
 [ 0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  1  0  1  0  0  0  0  0
   3  0  2  0  3]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0  0  0  0  0  0
   0  0  3  0  1]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  2  0  0  0
   0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  3
   0  2  0  0  2]
 [ 0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  1  9  0  0  0  0
   1  0  1  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  6  0  0
   0  0  0  0  4]
 [ 0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  9  0  0
   0  0  0  0  1]
 [ 0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0 10  0
   0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  3  1  0  3
   0  1  0  0  2]
 [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
   5  0  0  0  6]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  1  0  1
   0  7  1  0  1]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
   0  0  9  0  1]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0
   0  0  0  8  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0
   0  0  0  0 12]]

2022-07-27 13:06:44,591 - ==> Test Set [Top1: 50.000   Top5: 82.407  on test set]
2022-07-27 13:08:32,834 - => loading checkpoint jupyter_logging/SSL___2022.07.15-171757/classifierbackbonenet_qat_checkpoint.pth.tar
2022-07-27 13:08:32,840 - => Checkpoint contents:
+----------------------+-------------+---------------------------+
| Key                  | Type        | Value                     |
|----------------------+-------------+---------------------------|
| arch                 | str         | classifierbackbonenet_qat |
| compression_sched    | dict        |                           |
| epoch                | int         | 99                        |
| extras               | dict        |                           |
| optimizer_state_dict | dict        |                           |
| optimizer_type       | type        | Adam                      |
| state_dict           | OrderedDict |                           |
+----------------------+-------------+---------------------------+

2022-07-27 13:08:32,841 - => Checkpoint['extras'] contents:
+-------+--------+---------+
| Key   | Type   |   Value |
|-------+--------+---------|
| epoch | int    |      99 |
+-------+--------+---------+

2022-07-27 13:08:32,842 - Loaded compression schedule from checkpoint (epoch 99)
2022-07-27 13:08:32,846 - => loaded 'state_dict' from checkpoint 'jupyter_logging/SSL___2022.07.15-171757/classifierbackbonenet_qat_checkpoint.pth.tar'
2022-07-27 13:09:42,545 - => loading checkpoint jupyter_logging/SSL___2022.07.15-171757/classifierbackbonenet_qat_checkpoint.pth.tar
2022-07-27 13:09:42,552 - => Checkpoint contents:
+----------------------+-------------+---------------------------+
| Key                  | Type        | Value                     |
|----------------------+-------------+---------------------------|
| arch                 | str         | classifierbackbonenet_qat |
| compression_sched    | dict        |                           |
| epoch                | int         | 99                        |
| extras               | dict        |                           |
| optimizer_state_dict | dict        |                           |
| optimizer_type       | type        | Adam                      |
| state_dict           | OrderedDict |                           |
+----------------------+-------------+---------------------------+

2022-07-27 13:09:42,552 - => Checkpoint['extras'] contents:
+-------+--------+---------+
| Key   | Type   |   Value |
|-------+--------+---------|
| epoch | int    |      99 |
+-------+--------+---------+

2022-07-27 13:09:42,553 - Loaded compression schedule from checkpoint (epoch 99)
2022-07-27 13:09:42,558 - => loaded 'state_dict' from checkpoint 'jupyter_logging/SSL___2022.07.15-171757/classifierbackbonenet_qat_checkpoint.pth.tar'
2022-07-27 13:09:42,584 - => loading checkpoint jupyter_logging/finetune_asl_base_ev1___2022.07.27-124454/aslclassifier_best.pth.tar
2022-07-27 13:09:42,589 - => Checkpoint contents:
+----------------------+-------------+---------------+
| Key                  | Type        | Value         |
|----------------------+-------------+---------------|
| arch                 | str         | aslclassifier |
| compression_sched    | dict        |               |
| epoch                | int         | 3             |
| extras               | dict        |               |
| optimizer_state_dict | dict        |               |
| optimizer_type       | type        | Adam          |
| state_dict           | OrderedDict |               |
+----------------------+-------------+---------------+

2022-07-27 13:09:42,589 - => Checkpoint['extras'] contents:
+--------------+--------+---------+
| Key          | Type   |   Value |
|--------------+--------+---------|
| best_epoch   | int    |  3      |
| best_top1    | float  | 97.6373 |
| current_top1 | float  | 97.6373 |
+--------------+--------+---------+

2022-07-27 13:09:42,590 - Loaded compression schedule from checkpoint (epoch 3)
2022-07-27 13:09:42,635 - => loaded 'state_dict' from checkpoint 'jupyter_logging/finetune_asl_base_ev1___2022.07.27-124454/aslclassifier_best.pth.tar'

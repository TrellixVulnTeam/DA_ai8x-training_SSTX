{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b261e12b",
   "metadata": {},
   "source": [
    "# Cats and Dogs Domain Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66236c4",
   "metadata": {},
   "source": [
    "This notebook builds on the notebook from the toy example for domain adaptation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197dc191",
   "metadata": {},
   "source": [
    "### Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "660421b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import os\n",
    "from matplotlib.image import imread\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import logging\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    import tensorboard  # pylint: disable=import-error\n",
    "    import tensorflow  # pylint: disable=import-error\n",
    "    tensorflow.io.gfile = tensorboard.compat.tensorflow_stub.io.gfile\n",
    "except (ModuleNotFoundError, AttributeError):\n",
    "    pass\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchnet.meter as tnt\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import nnplot\n",
    "import operator\n",
    "import distiller\n",
    "import distiller.apputils as apputils\n",
    "from distiller.data_loggers import PythonLogger, TensorBoardLogger\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'models/')\n",
    "sys.path.insert(1, 'distiller/')\n",
    "sys.path.insert(2, 'datasets/')\n",
    "\n",
    "from classification import *\n",
    "\n",
    "mod = importlib.import_module(\"minivgg\")\n",
    "\n",
    "import ai8x\n",
    "%matplotlib inline\n",
    "\n",
    "# Logger handle\n",
    "msglogger = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8fa465",
   "metadata": {},
   "source": [
    "## Define Training Configurations (args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e36ccb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"office5\"\n",
    "dataset_fn = office5_get_datasets\n",
    "num_classes = 5\n",
    "model_name = \"office5\"\n",
    "dimensions = (3,128,128)\n",
    "workers = 4\n",
    "batch_size = 16\n",
    "validation_split = 0.1\n",
    "log_prefix = \"office5_DCD\"\n",
    "log_dir = \"jupyter_logging\"\n",
    "data_path = \"../Datasets/office5_camera/\"\n",
    "deterministic = True\n",
    "print_freq = 100\n",
    "labels = ('back_pack', 'keyboard','laptop_computer','mug','pen')\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ebbe33",
   "metadata": {},
   "source": [
    "## Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fbec67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, act_mode_8bit):\n",
    "        self.act_mode_8bit = act_mode_8bit\n",
    "        self.truncate_testset = False\n",
    "\n",
    "def count_params(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f8575a",
   "metadata": {},
   "source": [
    "## Set up the logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c84b777e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log file for this run: /home/geffencooper/Model_Development/ai8x-training/jupyter_logging/office5_DCD___2022.06.29-124651/office5_DCD___2022.06.29-124651.log\n",
      "dataset_name:office5\n",
      "dataset_fn=<function office5_get_datasets at 0x7f8ec53ac3a0>\n",
      "num_classes=5\n",
      "model_name=office5\n",
      "dimensions=(3, 128, 128)\n",
      "batch_size=16\n",
      "validation_split=0.1\n",
      "lr=0.001000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------\n",
      "Logging to TensorBoard - remember to execute the server:\n",
      "> tensorboard --logdir='./logs'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "msglogger = apputils.config_pylogger('logging.conf', log_prefix,\n",
    "                                        log_dir)\n",
    "\n",
    "# Log various details about the execution environment.  It is sometimes useful\n",
    "# to refer to past experiment executions and this information may be useful.\n",
    "apputils.log_execution_env_state(None, msglogger.logdir)\n",
    "msglogger.debug(\"Distiller: %s\", distiller.__version__)\n",
    "\n",
    "\n",
    "pylogger = PythonLogger(msglogger, log_1d=True)\n",
    "all_loggers = [pylogger]\n",
    "\n",
    "# tensorboard\n",
    "tflogger = TensorBoardLogger(msglogger.logdir, log_1d=True, comment='_'+dataset_name)\n",
    "\n",
    "tflogger.tblogger.writer.add_text('Command line', \"args ---\")\n",
    "\n",
    "msglogger.info('dataset_name:%s\\ndataset_fn=%s\\nnum_classes=%d\\nmodel_name=%s\\ndimensions=%s\\nbatch_size=%d\\nvalidation_split=%s\\nlr=%f',\n",
    "                dataset_name,dataset_fn,num_classes,model_name,dimensions,batch_size,validation_split,lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eadd17",
   "metadata": {},
   "source": [
    "## Create and Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf6fde55",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args(act_mode_8bit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2047c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = office5_get_datasets((data_path, args), load_train=True, load_test=False,apply_transforms=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9cada0",
   "metadata": {},
   "source": [
    "## Visualize a batch of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6b5c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.visualize_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b63d2a",
   "metadata": {},
   "source": [
    "## Create the data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "196575ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "\ttraining=18000\n",
      "\tvalidation=2000\n",
      "\ttest=5000\n",
      "Augmentations:Compose(\n",
      "    Resize(size=(128, 128), interpolation=bilinear)\n",
      "    ColorJitter(brightness=(0.65, 1.35), contrast=(0.65, 1.35), saturation=(0.65, 1.35), hue=None)\n",
      "    RandomAffine(degrees=[-20.0, 20.0], translate=(0.25, 0.25))\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    RandomVerticalFlip(p=0.5)\n",
      "    ToTensor()\n",
      "    <ai8x.normalize object at 0x7f16dffd9040>\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Datasets/cats_and_dogs/train\n",
      "{'dogs': 0, 'cats': 1}\n",
      "../Datasets/cats_and_dogs/test\n",
      "{'dogs': 0, 'cats': 1}\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../Datasets/office5\"\n",
    "train_loader, val_loader, test_loader, _ = apputils.get_data_loaders(\n",
    "        dataset_fn, (data_path,args), batch_size,\n",
    "        workers, validation_split, deterministic,1, 1, 1)\n",
    "msglogger.info('Dataset sizes:\\n\\ttraining=%d\\n\\tvalidation=%d\\n\\ttest=%d',\n",
    "                   len(train_loader.sampler), len(val_loader.sampler), len(test_loader.sampler))\n",
    "msglogger.info('Augmentations:%s',train_loader.dataset.transform)\n",
    "# train_dataloader = DataLoader(train_set,batch_size=batch_size,shuffle=True)\n",
    "# test_dataloader = DataLoader(test_set,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c934995",
   "metadata": {},
   "source": [
    "## Set up the device, cuda or cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8dabe07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845e26d9",
   "metadata": {},
   "source": [
    "## Set up the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7db0732d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint jupyter_logging/imagenet10_long-train___2022.06.28-172810/imagenet10_qat_best.pth.tar\n",
      "=> Checkpoint contents:\n",
      "+----------------------+-------------+------------+\n",
      "| Key                  | Type        | Value      |\n",
      "|----------------------+-------------+------------|\n",
      "| arch                 | str         | imagenet10 |\n",
      "| compression_sched    | dict        |            |\n",
      "| epoch                | int         | 1855       |\n",
      "| extras               | dict        |            |\n",
      "| optimizer_state_dict | dict        |            |\n",
      "| optimizer_type       | type        | Adam       |\n",
      "| state_dict           | OrderedDict |            |\n",
      "+----------------------+-------------+------------+\n",
      "\n",
      "=> Checkpoint['extras'] contents:\n",
      "+--------------+--------+----------+\n",
      "| Key          | Type   |    Value |\n",
      "|--------------+--------+----------|\n",
      "| best_epoch   | int    | 1855     |\n",
      "| best_top1    | float  |   82.981 |\n",
      "| current_top1 | float  |   82.981 |\n",
      "+--------------+--------+----------+\n",
      "\n",
      "Loaded compression schedule from checkpoint (epoch 1855)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring device: MAX78000, simulate=False.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=> loaded 'state_dict' from checkpoint 'jupyter_logging/imagenet10_long-train___2022.06.28-172810/imagenet10_qat_best.pth.tar'\n",
      "/home/geffencooper/Model_Development/ai8x-training/models/minivgg.py:273: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(m.weight)\n",
      "\n",
      "model: OfficeClassifier(\n",
      "  (feature_extractor): Imagenet10Classifier(\n",
      "    (conv1): FusedConv2dReLU(\n",
      "      (activate): ReLU(inplace=True)\n",
      "      (op): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (calc_out_shift): OutputShift()\n",
      "      (calc_weight_scale): WeightScale()\n",
      "      (scale): Scaler()\n",
      "      (calc_out_scale): OutputScale()\n",
      "      (quantize_weight): Quantize()\n",
      "      (quantize_bias): Quantize()\n",
      "      (clamp_weight): Clamp()\n",
      "      (clamp_bias): Clamp()\n",
      "      (quantize): Quantize()\n",
      "      (clamp): Clamp()\n",
      "      (quantize_pool): Empty()\n",
      "      (clamp_pool): Empty()\n",
      "    )\n",
      "    (conv2): FusedConv2dReLU(\n",
      "      (activate): ReLU(inplace=True)\n",
      "      (op): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (calc_out_shift): OutputShift()\n",
      "      (calc_weight_scale): WeightScale()\n",
      "      (scale): Scaler()\n",
      "      (calc_out_scale): OutputScale()\n",
      "      (quantize_weight): Quantize()\n",
      "      (quantize_bias): Quantize()\n",
      "      (clamp_weight): Clamp()\n",
      "      (clamp_bias): Clamp()\n",
      "      (quantize): Quantize()\n",
      "      (clamp): Clamp()\n",
      "      (quantize_pool): Empty()\n",
      "      (clamp_pool): Empty()\n",
      "    )\n",
      "    (conv3): FusedMaxPoolConv2dReLU(\n",
      "      (activate): ReLU(inplace=True)\n",
      "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (op): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (calc_out_shift): OutputShift()\n",
      "      (calc_weight_scale): WeightScale()\n",
      "      (scale): Scaler()\n",
      "      (calc_out_scale): OutputScale()\n",
      "      (quantize_weight): Quantize()\n",
      "      (quantize_bias): Quantize()\n",
      "      (clamp_weight): Clamp()\n",
      "      (clamp_bias): Clamp()\n",
      "      (quantize): Quantize()\n",
      "      (clamp): Clamp()\n",
      "      (quantize_pool): Empty()\n",
      "      (clamp_pool): Empty()\n",
      "    )\n",
      "    (conv4): FusedConv2dBNReLU(\n",
      "      (activate): ReLU(inplace=True)\n",
      "      (op): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): None\n",
      "      (calc_out_shift): OutputShift()\n",
      "      (calc_weight_scale): WeightScale()\n",
      "      (scale): Scaler()\n",
      "      (calc_out_scale): OutputScale()\n",
      "      (quantize_weight): Quantize()\n",
      "      (quantize_bias): Quantize()\n",
      "      (clamp_weight): Clamp()\n",
      "      (clamp_bias): Clamp()\n",
      "      (quantize): Quantize()\n",
      "      (clamp): Clamp()\n",
      "      (quantize_pool): Empty()\n",
      "      (clamp_pool): Empty()\n",
      "    )\n",
      "    (conv5): FusedMaxPoolConv2dBNReLU(\n",
      "      (activate): ReLU(inplace=True)\n",
      "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (op): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): None\n",
      "      (calc_out_shift): OutputShift()\n",
      "      (calc_weight_scale): WeightScale()\n",
      "      (scale): Scaler()\n",
      "      (calc_out_scale): OutputScale()\n",
      "      (quantize_weight): Quantize()\n",
      "      (quantize_bias): Quantize()\n",
      "      (clamp_weight): Clamp()\n",
      "      (clamp_bias): Clamp()\n",
      "      (quantize): Quantize()\n",
      "      (clamp): Clamp()\n",
      "      (quantize_pool): Empty()\n",
      "      (clamp_pool): Empty()\n",
      "    )\n",
      "    (conv6): FusedConv2dBNReLU(\n",
      "      (activate): ReLU(inplace=True)\n",
      "      (op): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): None\n",
      "      (calc_out_shift): OutputShift()\n",
      "      (calc_weight_scale): WeightScale()\n",
      "      (scale): Scaler()\n",
      "      (calc_out_scale): OutputScale()\n",
      "      (quantize_weight): Quantize()\n",
      "      (quantize_bias): Quantize()\n",
      "      (clamp_weight): Clamp()\n",
      "      (clamp_bias): Clamp()\n",
      "      (quantize): Quantize()\n",
      "      (clamp): Clamp()\n",
      "      (quantize_pool): Empty()\n",
      "      (clamp_pool): Empty()\n",
      "    )\n",
      "    (conv7): FusedMaxPoolConv2dBNReLU(\n",
      "      (activate): ReLU(inplace=True)\n",
      "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (op): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): None\n",
      "      (calc_out_shift): OutputShift()\n",
      "      (calc_weight_scale): WeightScale()\n",
      "      (scale): Scaler()\n",
      "      (calc_out_scale): OutputScale()\n",
      "      (quantize_weight): Quantize()\n",
      "      (quantize_bias): Quantize()\n",
      "      (clamp_weight): Clamp()\n",
      "      (clamp_bias): Clamp()\n",
      "      (quantize): Quantize()\n",
      "      (clamp): Clamp()\n",
      "      (quantize_pool): Empty()\n",
      "      (clamp_pool): Empty()\n",
      "    )\n",
      "    (conv8): FusedConv2dBNReLU(\n",
      "      (activate): ReLU(inplace=True)\n",
      "      (op): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): None\n",
      "      (calc_out_shift): OutputShift()\n",
      "      (calc_weight_scale): WeightScale()\n",
      "      (scale): Scaler()\n",
      "      (calc_out_scale): OutputScale()\n",
      "      (quantize_weight): Quantize()\n",
      "      (quantize_bias): Quantize()\n",
      "      (clamp_weight): Clamp()\n",
      "      (clamp_bias): Clamp()\n",
      "      (quantize): Quantize()\n",
      "      (clamp): Clamp()\n",
      "      (quantize_pool): Empty()\n",
      "      (clamp_pool): Empty()\n",
      "    )\n",
      "    (conv9): FusedMaxPoolConv2dBNReLU(\n",
      "      (activate): ReLU(inplace=True)\n",
      "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (op): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): None\n",
      "      (calc_out_shift): OutputShift()\n",
      "      (calc_weight_scale): WeightScale()\n",
      "      (scale): Scaler()\n",
      "      (calc_out_scale): OutputScale()\n",
      "      (quantize_weight): Quantize()\n",
      "      (quantize_bias): Quantize()\n",
      "      (clamp_weight): Clamp()\n",
      "      (clamp_bias): Clamp()\n",
      "      (quantize): Quantize()\n",
      "      (clamp): Clamp()\n",
      "      (quantize_pool): Empty()\n",
      "      (clamp_pool): Empty()\n",
      "    )\n",
      "    (conv10): FusedMaxPoolConv2dBNReLU(\n",
      "      (activate): ReLU(inplace=True)\n",
      "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (op): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn): None\n",
      "      (calc_out_shift): OutputShift()\n",
      "      (calc_weight_scale): WeightScale()\n",
      "      (scale): Scaler()\n",
      "      (calc_out_scale): OutputScale()\n",
      "      (quantize_weight): Quantize()\n",
      "      (quantize_bias): Quantize()\n",
      "      (clamp_weight): Clamp()\n",
      "      (clamp_bias): Clamp()\n",
      "      (quantize): Quantize()\n",
      "      (clamp): Clamp()\n",
      "      (quantize_pool): Empty()\n",
      "      (clamp_pool): Empty()\n",
      "    )\n",
      "    (fc1): FusedLinearReLU(\n",
      "      (activate): ReLU(inplace=True)\n",
      "      (op): Linear(in_features=512, out_features=128, bias=True)\n",
      "      (calc_out_shift): OutputShift()\n",
      "      (calc_weight_scale): WeightScale()\n",
      "      (scale): Scaler()\n",
      "      (calc_out_scale): OutputScale()\n",
      "      (quantize_weight): Quantize()\n",
      "      (quantize_bias): Quantize()\n",
      "      (clamp_weight): Clamp()\n",
      "      (clamp_bias): Clamp()\n",
      "      (quantize): Quantize()\n",
      "      (clamp): Clamp()\n",
      "      (quantize_pool): Empty()\n",
      "      (clamp_pool): Empty()\n",
      "    )\n",
      "    (do1): Dropout(p=0.5, inplace=False)\n",
      "    (fc2): FusedLinearReLU(\n",
      "      (activate): ReLU(inplace=True)\n",
      "      (op): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (calc_out_shift): OutputShiftSqueeze()\n",
      "      (calc_weight_scale): One()\n",
      "      (scale): Scaler()\n",
      "      (calc_out_scale): OutputScale()\n",
      "      (quantize_weight): Empty()\n",
      "      (quantize_bias): Empty()\n",
      "      (clamp_weight): Empty()\n",
      "      (clamp_bias): Empty()\n",
      "      (quantize): Empty()\n",
      "      (clamp): Clamp()\n",
      "      (quantize_pool): Empty()\n",
      "      (clamp_pool): Empty()\n",
      "    )\n",
      "    (fc3): Linear(\n",
      "      (activate): Empty()\n",
      "      (op): Linear(in_features=64, out_features=5, bias=True)\n",
      "      (calc_out_shift): OutputShiftSqueeze()\n",
      "      (calc_weight_scale): One()\n",
      "      (scale): Scaler()\n",
      "      (calc_out_scale): OutputScale()\n",
      "      (quantize_weight): Empty()\n",
      "      (quantize_bias): Empty()\n",
      "      (clamp_weight): Empty()\n",
      "      (clamp_bias): Empty()\n",
      "      (quantize): Empty()\n",
      "      (clamp): Clamp()\n",
      "      (quantize_pool): Empty()\n",
      "      (clamp_pool): Empty()\n",
      "    )\n",
      "  )\n",
      "  (do1): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Number of Model Params: 129389\n"
     ]
    }
   ],
   "source": [
    "ai8x.set_device(device=85, simulate=False, round_avg=False)\n",
    "\n",
    "model = mod.OfficeClassifier(device=device)\n",
    "msglogger.info('model: %s',model)\n",
    "model = model.to(device)\n",
    "\n",
    "msglogger.info('Number of Model Params: %d',count_params(model))\n",
    "\n",
    "# configure tensorboard\n",
    "dummy_input = torch.randn((1, ) + dimensions)\n",
    "tflogger.tblogger.writer.add_graph(model.to('cpu'), (dummy_input, ), False)\n",
    "\n",
    "all_loggers.append(tflogger)\n",
    "all_tbloggers = [tflogger]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddad2c1",
   "metadata": {},
   "source": [
    "## Set up the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c00fd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epochs: 200\n",
      "Optimizer Type: <class 'torch.optim.adam.Adam'>\n",
      "lr_schedule:base: [0.001] milestones: Counter({10: 1, 35: 1, 100: 1}) gamma: 0.5\n",
      "qat policy: {'start_epoch': 10, 'weight_bits': 8}\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "msglogger.info('epochs: %d',num_epochs)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "msglogger.info('Optimizer Type: %s', type(optimizer))\n",
    "ms_lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 35,100], gamma=0.5)\n",
    "msglogger.info(\"lr_schedule:%s\",\"base: \"+str(ms_lr_scheduler.base_lrs)+\" milestones: \"+str(ms_lr_scheduler.milestones)+ \" gamma: \"+str(ms_lr_scheduler.gamma))\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "qat_policy = {'start_epoch':10,\n",
    "              'weight_bits':8}\n",
    "msglogger.info('qat policy: %s',qat_policy)\n",
    "compression_scheduler = distiller.CompressionScheduler(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80e446e",
   "metadata": {},
   "source": [
    "## Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba442fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(data_loader, model, criterion, loggers, epoch=-1, tflogger=None):\n",
    "    \"\"\"Execute the validation/test loop.\"\"\"\n",
    "\n",
    "    # keep track of incorrect predictions\n",
    "    wrong_samples = None\n",
    "    wrong_preds = None\n",
    "    actual_preds = None\n",
    "    img_names = None\n",
    "\n",
    "    # store loss stats\n",
    "    losses = {'objective_loss': tnt.AverageValueMeter()}\n",
    "    classerr = tnt.ClassErrorMeter(accuracy=True, topk=(1, min(num_classes, 5)))\n",
    "\n",
    "    # validation set info\n",
    "    batch_time = tnt.AverageValueMeter()\n",
    "    total_samples = len(data_loader.sampler)\n",
    "    batch_size = data_loader.batch_size\n",
    "    confusion = tnt.ConfusionMeter(num_classes)\n",
    "    total_steps = (total_samples + batch_size - 1) // batch_size\n",
    "    msglogger.info('%d samples (%d per mini-batch)', total_samples, batch_size)\n",
    "\n",
    "    # Switch to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    class_probs = []\n",
    "    class_preds = []\n",
    "\n",
    "    # iterate over the batches in the validation set\n",
    "    for validation_step, (inputs, target, names) in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "            # compute output from model\n",
    "            output = model(inputs)\n",
    "\n",
    "            # correct output for accurate loss calculation\n",
    "            if args.act_mode_8bit:\n",
    "                output /= 128.\n",
    "                for key in model.__dict__['_modules'].keys():\n",
    "                    if (hasattr(model.__dict__['_modules'][key], 'wide')\n",
    "                            and model.__dict__['_modules'][key].wide):\n",
    "                        output /= 256.\n",
    "\n",
    "            # # get the idxs of the max output\n",
    "            # pred = output.argmax(dim=1, keepdim=True)\n",
    "            \n",
    "            # wrong_idx = (pred != target.view_as(pred)).nonzero()[:, 0] # get wrong predictions\n",
    "            \n",
    "            # if validation_step == 0:\n",
    "            #     wrong_samples = inputs[wrong_idx]\n",
    "            #     wrong_preds = pred[wrong_idx]\n",
    "            #     actual_preds = target.view_as(pred)[wrong_idx]\n",
    "            #     img_names = [names[i.item()] for i in wrong_idx] #names[wrong_idx]\n",
    "            # else:\n",
    "            #     wrong_samples = torch.cat((wrong_samples,inputs[wrong_idx]),0)\n",
    "            #     wrong_preds = torch.cat((wrong_preds,pred[wrong_idx]),0)\n",
    "            #     actual_preds = torch.cat((actual_preds,target.view_as(pred)[wrong_idx]),0)\n",
    "            #     img_names.extend([names[i.item()] for i in wrong_idx])\n",
    "\n",
    "            # compute loss\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            losses['objective_loss'].add(loss.item())\n",
    "            if len(output.data.shape) <= 2:\n",
    "                classerr.add(output.data, target)\n",
    "            else:\n",
    "                classerr.add(output.data.permute(0, 2, 3, 1).flatten(start_dim=0, end_dim=2),\n",
    "                                target.flatten())\n",
    "            \n",
    "            confusion.add(output.data, target)\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.add(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            # store prediction stats\n",
    "            steps_completed = (validation_step+1)\n",
    "            if steps_completed % print_freq == 0 or steps_completed == total_steps:\n",
    "                class_probs_batch = [torch.nn.functional.softmax(el, dim=0) for el in output]\n",
    "                _, class_preds_batch = torch.max(output, 1)\n",
    "                class_probs.append(class_probs_batch)\n",
    "                class_preds.append(class_preds_batch)\n",
    "\n",
    "                stats = (\n",
    "                    '',\n",
    "                    OrderedDict([('Loss', losses['objective_loss'].mean),\n",
    "                                    ('Top1', classerr.value(1))])\n",
    "                )\n",
    "                if num_classes > 5:\n",
    "                    stats[1]['Top5'] = classerr.value(5)\n",
    "\n",
    "                distiller.log_training_progress(stats, None, epoch, steps_completed,\n",
    "                                                total_steps, print_freq, loggers)\n",
    "\n",
    "\n",
    "                # if tflogger is not None:\n",
    "                #     test_probs = torch.cat([torch.stack(batch) for batch in class_probs])\n",
    "                #     test_preds = torch.cat(class_preds)\n",
    "                #     for i in range(num_classes):\n",
    "                #         tb_preds = test_preds == i\n",
    "                #         tb_probs = test_probs[:, i]\n",
    "                #         tflogger.tblogger.writer.add_pr_curve(str(args.labels[i]), tb_preds,\n",
    "                #                                             tb_probs, global_step=epoch)\n",
    "\n",
    "                # if steps_completed == total_steps and tflogger is not None:\n",
    "                #     def select_n_random(data, labels, features, n=100):\n",
    "                #         \"\"\"Selects n random datapoints, their corresponding labels and features\"\"\"\n",
    "                #         assert len(data) == len(labels) == len(features)\n",
    "\n",
    "                #         perm = torch.randperm(len(data))\n",
    "                #         return data[perm][:n], labels[perm][:n], features[perm][:n]\n",
    "\n",
    "                #     # Select up to 100 random images and their target indices\n",
    "                #     images, labels, features = select_n_random(inputs, target, output,\n",
    "                #                                                n=min(100, len(inputs)))\n",
    "\n",
    "                #     # Get the class labels for each image\n",
    "                #     class_labels = [args.labels[lab] for lab in labels]\n",
    "\n",
    "                #     tflogger.tblogger.writer.add_embedding(\n",
    "                #         features,\n",
    "                #         metadata=class_labels,\n",
    "                #         label_img=args.visualize_fn(images, args),\n",
    "                #         global_step=epoch,\n",
    "                #         tag='verification/embedding'\n",
    "                #     )\n",
    "\n",
    "    if num_classes > 5:\n",
    "        msglogger.info('==> Top1: %.3f    Top5: %.3f    Loss: %.3f\\n',\n",
    "                        classerr.value()[0], classerr.value()[1],\n",
    "                        losses['objective_loss'].mean)\n",
    "    else:\n",
    "        msglogger.info('==> Top1: %.3f    Loss: %.3f\\n',\n",
    "                        classerr.value()[0], losses['objective_loss'].mean)\n",
    "\n",
    "    msglogger.info('==> Confusion:\\n%s\\n', str(confusion.value()))\n",
    "    if tflogger is not None:\n",
    "        cf = nnplot.confusion_matrix(confusion.value(), labels)\n",
    "        tflogger.tblogger.writer.add_image('Validation/ConfusionMatrix', cf, epoch,\n",
    "                                            dataformats='HWC')\n",
    "    # if epoch > 0:\n",
    "    #     data_loader.dataset.viz_mispredict(wrong_samples,wrong_preds,actual_preds,img_names)\n",
    "    return classerr.value(1), classerr.value(min(num_classes, 5)), losses['objective_loss'].mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92564625",
   "metadata": {},
   "source": [
    "## Run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1731f381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store model history across epochs\n",
    "perf_scores_history = []\n",
    "model = model.to(device)\n",
    "\n",
    "name = model_name\n",
    "\n",
    "# start the clock\n",
    "tic = datetime.now()\n",
    "\n",
    "# training loop\n",
    "for epoch in range(0, num_epochs):\n",
    "    # check if need to switch to QAT\n",
    "    if epoch > 0 and epoch == qat_policy['start_epoch']:\n",
    "        print('QAT is starting!')\n",
    "        # Fuse the BN parameters into conv layers before Quantization Aware Training (QAT)\n",
    "        ai8x.fuse_bn_layers(model)\n",
    "\n",
    "        # Switch model from unquantized to quantized for QAT\n",
    "        ai8x.initiate_qat(model, qat_policy)\n",
    "\n",
    "        # Model is re-transferred to GPU in case parameters were added\n",
    "        model.to(device)\n",
    "\n",
    "        # Empty the performance scores list for QAT operation\n",
    "        perf_scores_history = []\n",
    "        name = f'{model_name}_qat'\n",
    "\n",
    "    # store loss and training stats\n",
    "    losses = {'objective_loss': tnt.AverageValueMeter()}\n",
    "    classerr = tnt.ClassErrorMeter(accuracy=True, topk=(1, min(num_classes, 5)))\n",
    "    batch_time = tnt.AverageValueMeter()\n",
    "    data_time = tnt.AverageValueMeter()\n",
    "\n",
    "    # logging stats\n",
    "    total_samples = len(train_loader.sampler)\n",
    "    batch_size = train_loader.batch_size\n",
    "    steps_per_epoch = (total_samples + batch_size - 1) // batch_size\n",
    "    msglogger.info('Training epoch: %d samples (%d per mini-batch)', total_samples, batch_size)\n",
    "\n",
    "    # Switch to train mode\n",
    "    model.train()\n",
    "    acc_stats = []\n",
    "    end = time.time()\n",
    "\n",
    "    # iterate over all batches in the dataset\n",
    "    for train_step, (inputs, target, names) in enumerate(train_loader):\n",
    "        # Measure data loading time\n",
    "        data_time.add(time.time() - end)\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "\n",
    "        # forward pass and loss calculation\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # on the last batch store the stats for the epoch\n",
    "        if train_step >= len(train_loader)-2:\n",
    "            if len(output.data.shape) <= 2:\n",
    "                classerr.add(output.data, target)\n",
    "            else:\n",
    "                classerr.add(output.data.permute(0, 2, 3, 1).flatten(start_dim=0, end_dim=2),\n",
    "                                target.flatten())\n",
    "            acc_stats.append([classerr.value(1), classerr.value(min(num_classes, 5))])\n",
    "\n",
    "        # add the loss for each batch\n",
    "        losses[\"objective_loss\"].add(loss.item())\n",
    "\n",
    "        # reset the optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # backwards pass and parameter update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # track batch stats\n",
    "        batch_time.add(time.time() - end)\n",
    "        steps_completed = (train_step+1)\n",
    "\n",
    "        # log stats every 10 batches\n",
    "        if steps_completed % print_freq == 0 or steps_completed == steps_per_epoch:\n",
    "            # Log some statistics\n",
    "            errs = OrderedDict()\n",
    "            if classerr.n != 0:\n",
    "                errs['Top1'] = classerr.value(1)\n",
    "                if num_classes > 5:\n",
    "                    errs['Top5'] = classerr.value(5)\n",
    "            else:\n",
    "                errs['Top1'] = None\n",
    "                errs['Top5'] = None\n",
    "\n",
    "            stats_dict = OrderedDict()\n",
    "            for loss_name, meter in losses.items():\n",
    "                stats_dict[loss_name] = meter.mean\n",
    "            stats_dict.update(errs)\n",
    "            \n",
    "            stats_dict['LR'] = optimizer.param_groups[0]['lr']\n",
    "            stats_dict['Time'] = batch_time.mean\n",
    "            stats = ('Performance/Training/', stats_dict)\n",
    "            params = None\n",
    "            distiller.log_training_progress(stats,\n",
    "                                            params,\n",
    "                                            epoch, steps_completed,\n",
    "                                            steps_per_epoch, print_freq,\n",
    "                                            all_loggers)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "\n",
    "    # after a training epoch, do validation\n",
    "    msglogger.info('--- validate (epoch=%d)-----------', epoch)\n",
    "    top1, top5, vloss = validate(val_loader, model, criterion, [pylogger], epoch, tflogger)\n",
    "\n",
    "    # store validation stats\n",
    "    stats = ('Performance/Validation/', OrderedDict([('Loss', vloss), ('Top1', top1)]))\n",
    "    if num_classes > 5:\n",
    "        stats[1]['Top5'] = top5\n",
    "\n",
    "    distiller.log_training_progress(stats, None, epoch, steps_completed=0, total_steps=1,\n",
    "                                            log_freq=1, loggers=all_tbloggers)\n",
    "\n",
    "    perf_scores_history.append(distiller.MutableNamedTuple({'top1': top1, 'top5': top5,\n",
    "                                                            'epoch': epoch}))\n",
    "    # Keep perf_scores_history sorted from best to worst\n",
    "    # Sort by top1 as main sort key, then sort by top5 and epoch\n",
    "    perf_scores_history.sort(key=operator.attrgetter('top1', 'top5', 'epoch'),reverse=True)\n",
    "    for score in perf_scores_history[:1]:\n",
    "        if num_classes > 5:\n",
    "            msglogger.info('==> Best [Top1: %.3f   Top5: %.3f  on epoch: %d]',\n",
    "                            score.top1, score.top5,score.epoch)\n",
    "        else:\n",
    "            msglogger.info('==> Best [Top1: %.3f on epoch: %d]',\n",
    "                            score.top1, score.epoch)\n",
    "\n",
    "    # Save the checkpoint\n",
    "    is_best = epoch == perf_scores_history[0].epoch\n",
    "    checkpoint_extras = {'current_top1': top1,\n",
    "                        'best_top1': perf_scores_history[0].top1,\n",
    "                        'best_epoch': perf_scores_history[0].epoch}\n",
    "\n",
    "    apputils.save_checkpoint(epoch, model_name, model, optimizer=optimizer,\n",
    "                                scheduler=compression_scheduler, extras=checkpoint_extras,\n",
    "                                is_best=is_best, name=name,\n",
    "                                dir=msglogger.logdir)\n",
    "\n",
    "    ms_lr_scheduler.step()\n",
    "\n",
    "# Finally run results on the test set\n",
    "top1, top5, losses = validate(test_loader, model, criterion, [pylogger],epoch,None)\n",
    "msglogger.info('==> Test Set [Top1: %.3f   Top5: %.3f  on test set]', top1, top5)\n",
    "\n",
    "msglogger.info('Training time: %s', datetime.now() - tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4569b2ea",
   "metadata": {},
   "source": [
    "## Next train the Domain-Class Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c30def",
   "metadata": {},
   "source": [
    "### Set up the discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80274664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint jupyter_logging/imagenet10_long-train___2022.06.28-172810/imagenet10_qat_best.pth.tar\n",
      "=> Checkpoint contents:\n",
      "+----------------------+-------------+------------+\n",
      "| Key                  | Type        | Value      |\n",
      "|----------------------+-------------+------------|\n",
      "| arch                 | str         | imagenet10 |\n",
      "| compression_sched    | dict        |            |\n",
      "| epoch                | int         | 1855       |\n",
      "| extras               | dict        |            |\n",
      "| optimizer_state_dict | dict        |            |\n",
      "| optimizer_type       | type        | Adam       |\n",
      "| state_dict           | OrderedDict |            |\n",
      "+----------------------+-------------+------------+\n",
      "\n",
      "=> Checkpoint['extras'] contents:\n",
      "+--------------+--------+----------+\n",
      "| Key          | Type   |    Value |\n",
      "|--------------+--------+----------|\n",
      "| best_epoch   | int    | 1855     |\n",
      "| best_top1    | float  |   82.981 |\n",
      "| current_top1 | float  |   82.981 |\n",
      "+--------------+--------+----------+\n",
      "\n",
      "Loaded compression schedule from checkpoint (epoch 1855)\n",
      "=> loaded 'state_dict' from checkpoint 'jupyter_logging/imagenet10_long-train___2022.06.28-172810/imagenet10_qat_best.pth.tar'\n",
      "/home/geffencooper/Model_Development/ai8x-training/models/minivgg.py:273: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(m.weight)\n",
      "\n",
      "=> loading checkpoint jupyter_logging/office5-fine_tune___2022.06.29-104825/office5_qat_best.pth.tar\n",
      "=> Checkpoint contents:\n",
      "+----------------------+-------------+---------+\n",
      "| Key                  | Type        | Value   |\n",
      "|----------------------+-------------+---------|\n",
      "| arch                 | str         | office5 |\n",
      "| compression_sched    | dict        |         |\n",
      "| epoch                | int         | 42      |\n",
      "| extras               | dict        |         |\n",
      "| optimizer_state_dict | dict        |         |\n",
      "| optimizer_type       | type        | Adam    |\n",
      "| state_dict           | OrderedDict |         |\n",
      "+----------------------+-------------+---------+\n",
      "\n",
      "=> Checkpoint['extras'] contents:\n",
      "+--------------+--------+---------+\n",
      "| Key          | Type   |   Value |\n",
      "|--------------+--------+---------|\n",
      "| best_epoch   | int    | 42      |\n",
      "| best_top1    | float  | 95.3488 |\n",
      "| current_top1 | float  | 95.3488 |\n",
      "+--------------+--------+---------+\n",
      "\n",
      "Loaded compression schedule from checkpoint (epoch 42)\n",
      "=> loaded 'state_dict' from checkpoint 'jupyter_logging/office5-fine_tune___2022.06.29-104825/office5_qat_best.pth.tar'\n"
     ]
    }
   ],
   "source": [
    "# load the encoder-classifier model\n",
    "load_model_path = \"jupyter_logging/office5-fine_tune___2022.06.29-104825/office5_qat_best.pth.tar\"\n",
    "model = mod.OfficeClassifier(device=device)                       \n",
    "checkpoint = torch.load(load_model_path, map_location=lambda storage, loc: storage)\n",
    "ai8x.fuse_bn_layers(model)\n",
    "model = apputils.load_lean_checkpoint(model, load_model_path, model_device=device)\n",
    "ai8x.update_model(model)\n",
    "model = model.to(device)\n",
    "\n",
    "# register a forward hook to get the encoder output\n",
    "encoder_output = {}\n",
    "def get_embedding(name):\n",
    "    def hook(model, input, output):\n",
    "        encoder_output[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# get the activations\n",
    "model.feature_extractor.fc2.register_forward_hook(get_embedding('fc2'))\n",
    "\n",
    "# create the discriminator\n",
    "discriminator = mod.OfficeDCD()\n",
    "discriminator = discriminator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "899ada87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Datasets/office5/train\n",
      "{'keyboard': 0, 'pen': 1, 'back_pack': 2, 'mug': 3, 'laptop_computer': 4}\n",
      "../Datasets/office5_camera/train\n",
      "{'keyboard': 0, 'pen': 1, 'back_pack': 2, 'mug': 3, 'laptop_computer': 4}\n"
     ]
    }
   ],
   "source": [
    "X_s,Y_s = create_source_samples(\"../Datasets/office5/\",args)\n",
    "X_t,Y_t = create_target_samples(4,\"../Datasets/office5_camera/\",args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa5482dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------train DCD for step 2--------------------------------\n",
    "\n",
    "optimizer_D=torch.optim.Adam(discriminator.parameters(),lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64c7eba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step2----Epoch 1/200 loss:1.695\n",
      "step2----Epoch 2/200 loss:1.590\n",
      "step2----Epoch 3/200 loss:1.546\n",
      "step2----Epoch 4/200 loss:1.426\n",
      "step2----Epoch 5/200 loss:1.355\n",
      "step2----Epoch 6/200 loss:1.330\n",
      "step2----Epoch 7/200 loss:1.294\n",
      "step2----Epoch 8/200 loss:1.217\n",
      "step2----Epoch 9/200 loss:1.221\n",
      "step2----Epoch 10/200 loss:1.233\n",
      "step2----Epoch 11/200 loss:1.167\n",
      "step2----Epoch 12/200 loss:1.143\n",
      "step2----Epoch 13/200 loss:1.159\n",
      "step2----Epoch 14/200 loss:1.140\n",
      "step2----Epoch 15/200 loss:1.073\n",
      "step2----Epoch 16/200 loss:1.044\n",
      "step2----Epoch 17/200 loss:1.047\n",
      "step2----Epoch 18/200 loss:1.103\n",
      "step2----Epoch 19/200 loss:0.995\n",
      "step2----Epoch 20/200 loss:0.987\n",
      "step2----Epoch 21/200 loss:1.010\n",
      "step2----Epoch 22/200 loss:0.966\n",
      "step2----Epoch 23/200 loss:1.006\n",
      "step2----Epoch 24/200 loss:0.956\n",
      "step2----Epoch 25/200 loss:0.856\n",
      "step2----Epoch 26/200 loss:0.859\n",
      "step2----Epoch 27/200 loss:0.905\n",
      "step2----Epoch 28/200 loss:0.844\n",
      "step2----Epoch 29/200 loss:0.865\n",
      "step2----Epoch 30/200 loss:0.871\n",
      "step2----Epoch 31/200 loss:0.786\n",
      "step2----Epoch 32/200 loss:1.084\n",
      "step2----Epoch 33/200 loss:0.733\n",
      "step2----Epoch 34/200 loss:0.729\n",
      "step2----Epoch 35/200 loss:0.774\n",
      "step2----Epoch 36/200 loss:0.904\n",
      "step2----Epoch 37/200 loss:0.897\n",
      "step2----Epoch 38/200 loss:0.777\n",
      "step2----Epoch 39/200 loss:0.827\n",
      "step2----Epoch 40/200 loss:1.000\n",
      "step2----Epoch 41/200 loss:0.798\n",
      "step2----Epoch 42/200 loss:1.148\n",
      "step2----Epoch 43/200 loss:0.730\n",
      "step2----Epoch 44/200 loss:0.673\n",
      "step2----Epoch 45/200 loss:0.920\n",
      "step2----Epoch 46/200 loss:0.676\n",
      "step2----Epoch 47/200 loss:0.615\n",
      "step2----Epoch 48/200 loss:0.747\n",
      "step2----Epoch 49/200 loss:0.765\n",
      "step2----Epoch 50/200 loss:0.652\n",
      "step2----Epoch 51/200 loss:0.631\n",
      "step2----Epoch 52/200 loss:0.573\n",
      "step2----Epoch 53/200 loss:0.544\n",
      "step2----Epoch 54/200 loss:0.626\n",
      "step2----Epoch 55/200 loss:0.691\n",
      "step2----Epoch 56/200 loss:0.590\n",
      "step2----Epoch 57/200 loss:0.719\n",
      "step2----Epoch 58/200 loss:0.678\n",
      "step2----Epoch 59/200 loss:0.600\n",
      "step2----Epoch 60/200 loss:0.757\n",
      "step2----Epoch 61/200 loss:0.589\n",
      "step2----Epoch 62/200 loss:0.845\n",
      "step2----Epoch 63/200 loss:0.697\n",
      "step2----Epoch 64/200 loss:0.716\n",
      "step2----Epoch 65/200 loss:0.679\n",
      "step2----Epoch 66/200 loss:0.805\n",
      "step2----Epoch 67/200 loss:0.516\n",
      "step2----Epoch 68/200 loss:0.492\n",
      "step2----Epoch 69/200 loss:0.462\n",
      "step2----Epoch 70/200 loss:0.681\n",
      "step2----Epoch 71/200 loss:0.622\n",
      "step2----Epoch 72/200 loss:0.687\n",
      "step2----Epoch 73/200 loss:0.615\n",
      "step2----Epoch 74/200 loss:0.489\n",
      "step2----Epoch 75/200 loss:0.537\n",
      "step2----Epoch 76/200 loss:0.595\n",
      "step2----Epoch 77/200 loss:0.524\n",
      "step2----Epoch 78/200 loss:0.433\n",
      "step2----Epoch 79/200 loss:0.715\n",
      "step2----Epoch 80/200 loss:0.475\n",
      "step2----Epoch 81/200 loss:0.580\n",
      "step2----Epoch 82/200 loss:0.588\n",
      "step2----Epoch 83/200 loss:0.574\n",
      "step2----Epoch 84/200 loss:0.440\n",
      "step2----Epoch 85/200 loss:0.512\n",
      "step2----Epoch 86/200 loss:0.894\n",
      "step2----Epoch 87/200 loss:0.373\n",
      "step2----Epoch 88/200 loss:0.721\n",
      "step2----Epoch 89/200 loss:0.530\n",
      "step2----Epoch 90/200 loss:0.526\n",
      "step2----Epoch 91/200 loss:0.509\n",
      "step2----Epoch 92/200 loss:0.367\n",
      "step2----Epoch 93/200 loss:0.353\n",
      "step2----Epoch 94/200 loss:0.529\n",
      "step2----Epoch 95/200 loss:0.334\n",
      "step2----Epoch 96/200 loss:0.539\n",
      "step2----Epoch 97/200 loss:0.510\n",
      "step2----Epoch 98/200 loss:0.521\n",
      "step2----Epoch 99/200 loss:0.471\n",
      "step2----Epoch 100/200 loss:0.444\n",
      "step2----Epoch 101/200 loss:0.617\n",
      "step2----Epoch 102/200 loss:0.395\n",
      "step2----Epoch 103/200 loss:0.321\n",
      "step2----Epoch 104/200 loss:0.525\n",
      "step2----Epoch 105/200 loss:0.447\n",
      "step2----Epoch 106/200 loss:0.418\n",
      "step2----Epoch 107/200 loss:0.338\n",
      "step2----Epoch 108/200 loss:0.289\n",
      "step2----Epoch 109/200 loss:0.477\n",
      "step2----Epoch 110/200 loss:0.354\n",
      "step2----Epoch 111/200 loss:0.327\n",
      "step2----Epoch 112/200 loss:0.290\n",
      "step2----Epoch 113/200 loss:0.274\n",
      "step2----Epoch 114/200 loss:0.326\n",
      "step2----Epoch 115/200 loss:0.277\n",
      "step2----Epoch 116/200 loss:0.525\n",
      "step2----Epoch 117/200 loss:0.379\n",
      "step2----Epoch 118/200 loss:0.294\n",
      "step2----Epoch 119/200 loss:0.560\n",
      "step2----Epoch 120/200 loss:0.566\n",
      "step2----Epoch 121/200 loss:0.454\n",
      "step2----Epoch 122/200 loss:0.289\n",
      "step2----Epoch 123/200 loss:0.297\n",
      "step2----Epoch 124/200 loss:0.374\n",
      "step2----Epoch 125/200 loss:0.396\n",
      "step2----Epoch 126/200 loss:0.250\n",
      "step2----Epoch 127/200 loss:0.314\n",
      "step2----Epoch 128/200 loss:0.439\n",
      "step2----Epoch 129/200 loss:0.266\n",
      "step2----Epoch 130/200 loss:0.436\n",
      "step2----Epoch 131/200 loss:0.457\n",
      "step2----Epoch 132/200 loss:0.427\n",
      "step2----Epoch 133/200 loss:0.313\n",
      "step2----Epoch 134/200 loss:0.539\n",
      "step2----Epoch 135/200 loss:0.283\n",
      "step2----Epoch 136/200 loss:0.357\n",
      "step2----Epoch 137/200 loss:0.247\n",
      "step2----Epoch 138/200 loss:0.238\n",
      "step2----Epoch 139/200 loss:0.501\n",
      "step2----Epoch 140/200 loss:0.529\n",
      "step2----Epoch 141/200 loss:0.278\n",
      "step2----Epoch 142/200 loss:0.243\n",
      "step2----Epoch 143/200 loss:0.219\n",
      "step2----Epoch 144/200 loss:0.447\n",
      "step2----Epoch 145/200 loss:0.217\n",
      "step2----Epoch 146/200 loss:0.592\n",
      "step2----Epoch 147/200 loss:0.241\n",
      "step2----Epoch 148/200 loss:0.187\n",
      "step2----Epoch 149/200 loss:0.202\n",
      "step2----Epoch 150/200 loss:0.230\n",
      "step2----Epoch 151/200 loss:0.438\n",
      "step2----Epoch 152/200 loss:0.466\n",
      "step2----Epoch 153/200 loss:0.390\n",
      "step2----Epoch 154/200 loss:0.347\n",
      "step2----Epoch 155/200 loss:0.195\n",
      "step2----Epoch 156/200 loss:0.350\n",
      "step2----Epoch 157/200 loss:0.184\n",
      "step2----Epoch 158/200 loss:0.381\n",
      "step2----Epoch 159/200 loss:0.537\n",
      "step2----Epoch 160/200 loss:0.469\n",
      "step2----Epoch 161/200 loss:0.173\n",
      "step2----Epoch 162/200 loss:0.209\n",
      "step2----Epoch 163/200 loss:0.318\n",
      "step2----Epoch 164/200 loss:0.477\n",
      "step2----Epoch 165/200 loss:0.375\n",
      "step2----Epoch 166/200 loss:0.365\n",
      "step2----Epoch 167/200 loss:0.198\n",
      "step2----Epoch 168/200 loss:0.237\n",
      "step2----Epoch 169/200 loss:0.278\n",
      "step2----Epoch 170/200 loss:0.179\n",
      "step2----Epoch 171/200 loss:0.472\n",
      "step2----Epoch 172/200 loss:0.274\n",
      "step2----Epoch 173/200 loss:0.356\n",
      "step2----Epoch 174/200 loss:0.206\n",
      "step2----Epoch 175/200 loss:0.680\n",
      "step2----Epoch 176/200 loss:0.271\n",
      "step2----Epoch 177/200 loss:0.163\n",
      "step2----Epoch 178/200 loss:0.163\n",
      "step2----Epoch 179/200 loss:0.397\n",
      "step2----Epoch 180/200 loss:0.261\n",
      "step2----Epoch 181/200 loss:0.164\n",
      "step2----Epoch 182/200 loss:0.171\n",
      "step2----Epoch 183/200 loss:0.329\n",
      "step2----Epoch 184/200 loss:0.612\n",
      "step2----Epoch 185/200 loss:0.362\n",
      "step2----Epoch 186/200 loss:0.381\n",
      "step2----Epoch 187/200 loss:0.234\n",
      "step2----Epoch 188/200 loss:0.241\n",
      "step2----Epoch 189/200 loss:0.285\n",
      "step2----Epoch 190/200 loss:0.406\n",
      "step2----Epoch 191/200 loss:0.313\n",
      "step2----Epoch 192/200 loss:0.279\n",
      "step2----Epoch 193/200 loss:0.419\n",
      "step2----Epoch 194/200 loss:0.411\n",
      "step2----Epoch 195/200 loss:0.331\n",
      "step2----Epoch 196/200 loss:0.183\n",
      "step2----Epoch 197/200 loss:0.157\n",
      "step2----Epoch 198/200 loss:0.256\n",
      "step2----Epoch 199/200 loss:0.579\n",
      "step2----Epoch 200/200 loss:0.264\n",
      "step2----Epoch 201/200 loss:0.167\n",
      "step2----Epoch 202/200 loss:0.131\n",
      "step2----Epoch 203/200 loss:0.209\n",
      "step2----Epoch 204/200 loss:0.191\n",
      "step2----Epoch 205/200 loss:0.299\n",
      "step2----Epoch 206/200 loss:0.142\n",
      "step2----Epoch 207/200 loss:0.206\n",
      "step2----Epoch 208/200 loss:0.173\n",
      "step2----Epoch 209/200 loss:0.176\n",
      "step2----Epoch 210/200 loss:0.359\n",
      "step2----Epoch 211/200 loss:0.414\n",
      "step2----Epoch 212/200 loss:0.654\n",
      "step2----Epoch 213/200 loss:0.127\n",
      "step2----Epoch 214/200 loss:0.204\n",
      "step2----Epoch 215/200 loss:0.160\n",
      "step2----Epoch 216/200 loss:0.459\n",
      "step2----Epoch 217/200 loss:0.123\n",
      "step2----Epoch 218/200 loss:0.457\n",
      "step2----Epoch 219/200 loss:0.280\n",
      "step2----Epoch 220/200 loss:0.253\n",
      "step2----Epoch 221/200 loss:0.132\n",
      "step2----Epoch 222/200 loss:0.331\n",
      "step2----Epoch 223/200 loss:0.122\n",
      "step2----Epoch 224/200 loss:0.145\n",
      "step2----Epoch 225/200 loss:0.259\n",
      "step2----Epoch 226/200 loss:0.273\n",
      "step2----Epoch 227/200 loss:0.148\n",
      "step2----Epoch 228/200 loss:0.167\n",
      "step2----Epoch 229/200 loss:0.132\n",
      "step2----Epoch 230/200 loss:0.112\n",
      "step2----Epoch 231/200 loss:0.176\n",
      "step2----Epoch 232/200 loss:0.411\n",
      "step2----Epoch 233/200 loss:0.177\n",
      "step2----Epoch 234/200 loss:0.295\n",
      "step2----Epoch 235/200 loss:0.127\n",
      "step2----Epoch 236/200 loss:0.261\n",
      "step2----Epoch 237/200 loss:0.109\n",
      "step2----Epoch 238/200 loss:0.410\n",
      "step2----Epoch 239/200 loss:0.318\n",
      "step2----Epoch 240/200 loss:0.146\n",
      "step2----Epoch 241/200 loss:0.215\n",
      "step2----Epoch 242/200 loss:0.136\n",
      "step2----Epoch 243/200 loss:0.320\n",
      "step2----Epoch 244/200 loss:0.250\n",
      "step2----Epoch 245/200 loss:0.428\n",
      "step2----Epoch 246/200 loss:0.141\n",
      "step2----Epoch 247/200 loss:0.181\n",
      "step2----Epoch 248/200 loss:0.117\n",
      "step2----Epoch 249/200 loss:0.144\n",
      "step2----Epoch 250/200 loss:0.198\n",
      "step2----Epoch 251/200 loss:0.194\n",
      "step2----Epoch 252/200 loss:0.127\n",
      "step2----Epoch 253/200 loss:0.111\n",
      "step2----Epoch 254/200 loss:0.437\n",
      "step2----Epoch 255/200 loss:0.417\n",
      "step2----Epoch 256/200 loss:0.193\n",
      "step2----Epoch 257/200 loss:0.098\n",
      "step2----Epoch 258/200 loss:0.165\n",
      "step2----Epoch 259/200 loss:0.090\n",
      "step2----Epoch 260/200 loss:0.091\n",
      "step2----Epoch 261/200 loss:0.218\n",
      "step2----Epoch 262/200 loss:0.103\n",
      "step2----Epoch 263/200 loss:0.097\n",
      "step2----Epoch 264/200 loss:0.260\n",
      "step2----Epoch 265/200 loss:0.142\n",
      "step2----Epoch 266/200 loss:0.358\n",
      "step2----Epoch 267/200 loss:0.572\n",
      "step2----Epoch 268/200 loss:0.244\n",
      "step2----Epoch 269/200 loss:0.222\n",
      "step2----Epoch 270/200 loss:0.437\n",
      "step2----Epoch 271/200 loss:0.126\n",
      "step2----Epoch 272/200 loss:0.119\n",
      "step2----Epoch 273/200 loss:0.097\n",
      "step2----Epoch 274/200 loss:0.221\n",
      "step2----Epoch 275/200 loss:0.400\n",
      "step2----Epoch 276/200 loss:0.300\n",
      "step2----Epoch 277/200 loss:0.088\n",
      "step2----Epoch 278/200 loss:0.129\n",
      "step2----Epoch 279/200 loss:0.087\n",
      "step2----Epoch 280/200 loss:0.244\n",
      "step2----Epoch 281/200 loss:0.183\n",
      "step2----Epoch 282/200 loss:0.386\n",
      "step2----Epoch 283/200 loss:0.111\n",
      "step2----Epoch 284/200 loss:0.163\n",
      "step2----Epoch 285/200 loss:0.248\n",
      "step2----Epoch 286/200 loss:0.452\n",
      "step2----Epoch 287/200 loss:0.105\n",
      "step2----Epoch 288/200 loss:0.205\n",
      "step2----Epoch 289/200 loss:0.235\n",
      "step2----Epoch 290/200 loss:0.299\n",
      "step2----Epoch 291/200 loss:0.107\n",
      "step2----Epoch 292/200 loss:0.085\n",
      "step2----Epoch 293/200 loss:0.189\n",
      "step2----Epoch 294/200 loss:0.083\n",
      "step2----Epoch 295/200 loss:0.195\n",
      "step2----Epoch 296/200 loss:0.179\n",
      "step2----Epoch 297/200 loss:0.188\n",
      "step2----Epoch 298/200 loss:0.412\n",
      "step2----Epoch 299/200 loss:0.083\n",
      "step2----Epoch 300/200 loss:0.173\n",
      "step2----Epoch 301/200 loss:0.176\n",
      "step2----Epoch 302/200 loss:0.083\n",
      "step2----Epoch 303/200 loss:0.281\n",
      "step2----Epoch 304/200 loss:0.099\n",
      "step2----Epoch 305/200 loss:0.143\n",
      "step2----Epoch 306/200 loss:0.413\n",
      "step2----Epoch 307/200 loss:0.093\n",
      "step2----Epoch 308/200 loss:0.094\n",
      "step2----Epoch 309/200 loss:0.077\n",
      "step2----Epoch 310/200 loss:0.197\n",
      "step2----Epoch 311/200 loss:0.079\n",
      "step2----Epoch 312/200 loss:0.099\n",
      "step2----Epoch 313/200 loss:0.087\n",
      "step2----Epoch 314/200 loss:0.085\n",
      "step2----Epoch 315/200 loss:0.097\n",
      "step2----Epoch 316/200 loss:0.129\n",
      "step2----Epoch 317/200 loss:0.136\n",
      "step2----Epoch 318/200 loss:0.156\n",
      "step2----Epoch 319/200 loss:0.119\n",
      "step2----Epoch 320/200 loss:0.195\n",
      "step2----Epoch 321/200 loss:0.141\n",
      "step2----Epoch 322/200 loss:0.113\n",
      "step2----Epoch 323/200 loss:0.068\n",
      "step2----Epoch 324/200 loss:0.145\n",
      "step2----Epoch 325/200 loss:0.252\n",
      "step2----Epoch 326/200 loss:0.066\n",
      "step2----Epoch 327/200 loss:0.211\n",
      "step2----Epoch 328/200 loss:0.118\n",
      "step2----Epoch 329/200 loss:0.080\n",
      "step2----Epoch 330/200 loss:0.521\n",
      "step2----Epoch 331/200 loss:0.325\n",
      "step2----Epoch 332/200 loss:0.078\n",
      "step2----Epoch 333/200 loss:0.082\n",
      "step2----Epoch 334/200 loss:0.076\n",
      "step2----Epoch 335/200 loss:0.091\n",
      "step2----Epoch 336/200 loss:0.118\n",
      "step2----Epoch 337/200 loss:0.087\n",
      "step2----Epoch 338/200 loss:0.271\n",
      "step2----Epoch 339/200 loss:0.059\n",
      "step2----Epoch 340/200 loss:0.401\n",
      "step2----Epoch 341/200 loss:0.076\n",
      "step2----Epoch 342/200 loss:0.250\n",
      "step2----Epoch 343/200 loss:0.091\n",
      "step2----Epoch 344/200 loss:0.289\n",
      "step2----Epoch 345/200 loss:0.102\n",
      "step2----Epoch 346/200 loss:0.080\n",
      "step2----Epoch 347/200 loss:0.131\n",
      "step2----Epoch 348/200 loss:0.209\n",
      "step2----Epoch 349/200 loss:0.174\n",
      "step2----Epoch 350/200 loss:0.079\n",
      "step2----Epoch 351/200 loss:0.119\n",
      "step2----Epoch 352/200 loss:0.056\n",
      "step2----Epoch 353/200 loss:0.068\n",
      "step2----Epoch 354/200 loss:0.332\n",
      "step2----Epoch 355/200 loss:0.149\n",
      "step2----Epoch 356/200 loss:0.117\n",
      "step2----Epoch 357/200 loss:0.073\n",
      "step2----Epoch 358/200 loss:0.094\n",
      "step2----Epoch 359/200 loss:0.067\n",
      "step2----Epoch 360/200 loss:0.083\n",
      "step2----Epoch 361/200 loss:0.094\n",
      "step2----Epoch 362/200 loss:0.058\n",
      "step2----Epoch 363/200 loss:0.077\n",
      "step2----Epoch 364/200 loss:0.059\n",
      "step2----Epoch 365/200 loss:0.197\n",
      "step2----Epoch 366/200 loss:0.062\n",
      "step2----Epoch 367/200 loss:0.061\n",
      "step2----Epoch 368/200 loss:0.259\n",
      "step2----Epoch 369/200 loss:0.044\n",
      "step2----Epoch 370/200 loss:0.045\n",
      "step2----Epoch 371/200 loss:0.049\n",
      "step2----Epoch 372/200 loss:0.095\n",
      "step2----Epoch 373/200 loss:0.140\n",
      "step2----Epoch 374/200 loss:0.203\n",
      "step2----Epoch 375/200 loss:0.142\n",
      "step2----Epoch 376/200 loss:0.073\n",
      "step2----Epoch 377/200 loss:0.342\n",
      "step2----Epoch 378/200 loss:0.119\n",
      "step2----Epoch 379/200 loss:0.065\n",
      "step2----Epoch 380/200 loss:0.072\n",
      "step2----Epoch 381/200 loss:0.141\n",
      "step2----Epoch 382/200 loss:0.145\n",
      "step2----Epoch 383/200 loss:0.438\n",
      "step2----Epoch 384/200 loss:0.311\n",
      "step2----Epoch 385/200 loss:0.073\n",
      "step2----Epoch 386/200 loss:0.241\n",
      "step2----Epoch 387/200 loss:0.200\n",
      "step2----Epoch 388/200 loss:0.085\n",
      "step2----Epoch 389/200 loss:0.200\n",
      "step2----Epoch 390/200 loss:0.204\n",
      "step2----Epoch 391/200 loss:0.239\n",
      "step2----Epoch 392/200 loss:0.082\n",
      "step2----Epoch 393/200 loss:0.138\n",
      "step2----Epoch 394/200 loss:0.522\n",
      "step2----Epoch 395/200 loss:0.059\n",
      "step2----Epoch 396/200 loss:0.405\n",
      "step2----Epoch 397/200 loss:0.051\n",
      "step2----Epoch 398/200 loss:0.227\n",
      "step2----Epoch 399/200 loss:0.079\n",
      "step2----Epoch 400/200 loss:0.072\n"
     ]
    }
   ],
   "source": [
    "epoch_loss = []\n",
    "for epoch in range(400):\n",
    "    # data\n",
    "    groups,aa = create_groups(X_s,Y_s,X_t,Y_t,shot=4,seed=epoch)\n",
    "\n",
    "    n_iters = 4 * len(groups[1])\n",
    "    index_list = torch.randperm(n_iters)\n",
    "    mini_batch_size=32 #use mini_batch train can be more stable\n",
    "\n",
    "\n",
    "    loss_mean=[]\n",
    "\n",
    "    X1=[];X2=[];ground_truths=[]\n",
    "    for index in range(n_iters):\n",
    "                            #    0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15\n",
    "        # get the class idx --> [0,0,0,0,1,1,1,1,2,2,2,2,3,3,3,3]\n",
    "        ground_truth=index_list[index]//len(groups[1])\n",
    "        # get the pair by indexing into the groups\n",
    "        # the first index is the group, the second index is the relative location within the class\n",
    "        # e.g. idx 5 is the 1st element in class 1 for the example above\n",
    "        x1,x2=groups[ground_truth][index_list[index]-len(groups[1])*ground_truth]\n",
    "        X1.append(x1)\n",
    "        X2.append(x2)\n",
    "        ground_truths.append(ground_truth)\n",
    "\n",
    "        #select data for a mini-batch to train\n",
    "        if (index+1)%mini_batch_size==0:\n",
    "            X1=torch.stack(X1)\n",
    "            X2=torch.stack(X2)\n",
    "            ground_truths=torch.LongTensor(ground_truths)\n",
    "            X1=X1.to(device)\n",
    "            X2=X2.to(device)\n",
    "            ground_truths=ground_truths.to(device)\n",
    "\n",
    "            optimizer_D.zero_grad()\n",
    "            out1 = model(X1)\n",
    "            enc1 = encoder_output['fc2']\n",
    "            out2 = model(X2)\n",
    "            enc2 = encoder_output['fc2']\n",
    "            X_cat=torch.cat([enc1,enc2],1)\n",
    "            y_pred=discriminator(X_cat.detach())\n",
    "            #print(torch.sum(y_pred.argmax(dim=1)==ground_truths)/mini_batch_size)\n",
    "            loss=loss_fn(y_pred,ground_truths)\n",
    "            loss.backward()\n",
    "            optimizer_D.step()\n",
    "            loss_mean.append(loss.item())\n",
    "            X1 = []\n",
    "            X2 = []\n",
    "            ground_truths = []\n",
    "\n",
    "    print(\"step2----Epoch %d/%d loss:%.3f\"%(epoch+1,200,np.mean(loss_mean)))\n",
    "    epoch_loss.append(np.mean(loss_mean))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2b1eb82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8e7524d100>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cb04e5",
   "metadata": {},
   "source": [
    "## Now we do adversarial training to trick the discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d4a8719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Datasets/office5_camera/test\n",
      "{'keyboard': 0, 'pen': 1, 'back_pack': 2, 'mug': 3, 'laptop_computer': 4}\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = office5_get_datasets((\"../Datasets/office5_camera/\", args), load_train=False, load_test=True,apply_transforms=False)\n",
    "test_dataloader = DataLoader(test_set, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "820718be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling groups\n",
      "train 1\n",
      "train 2\n",
      "step3----Epoch 1/50  accuracy: 0.271 \n",
      "Sampling groups\n",
      "train 1\n",
      "train 2\n",
      "step3----Epoch 2/50  accuracy: 0.333 \n",
      "Sampling groups\n",
      "train 1\n",
      "train 2\n",
      "step3----Epoch 3/50  accuracy: 0.333 \n",
      "Sampling groups\n",
      "train 1\n",
      "train 2\n",
      "step3----Epoch 4/50  accuracy: 0.333 \n",
      "Sampling groups\n",
      "train 1\n",
      "train 2\n",
      "step3----Epoch 5/50  accuracy: 0.333 \n",
      "Sampling groups\n",
      "train 1\n",
      "train 2\n",
      "step3----Epoch 6/50  accuracy: 0.333 \n",
      "Sampling groups\n",
      "train 1\n",
      "train 2\n",
      "step3----Epoch 7/50  accuracy: 0.333 \n",
      "Sampling groups\n",
      "train 1\n",
      "train 2\n",
      "step3----Epoch 8/50  accuracy: 0.333 \n",
      "Sampling groups\n",
      "train 1\n",
      "train 2\n",
      "step3----Epoch 9/50  accuracy: 0.333 \n",
      "Sampling groups\n",
      "train 1\n",
      "train 2\n",
      "step3----Epoch 10/50  accuracy: 0.333 \n",
      "Sampling groups\n",
      "train 1\n",
      "train 2\n",
      "step3----Epoch 11/50  accuracy: 0.333 \n",
      "Sampling groups\n",
      "train 1\n",
      "train 2\n",
      "step3----Epoch 12/50  accuracy: 0.333 \n",
      "Sampling groups\n",
      "train 1\n",
      "train 2\n",
      "step3----Epoch 13/50  accuracy: 0.312 \n",
      "Sampling groups\n",
      "train 1\n",
      "train 2\n",
      "step3----Epoch 14/50  accuracy: 0.312 \n",
      "Sampling groups\n",
      "train 1\n",
      "train 2\n",
      "step3----Epoch 15/50  accuracy: 0.333 \n",
      "Sampling groups\n",
      "train 1\n",
      "train 2\n",
      "step3----Epoch 16/50  accuracy: 0.333 \n",
      "Sampling groups\n",
      "train 1\n",
      "train 2\n",
      "step3----Epoch 17/50  accuracy: 0.333 \n",
      "Sampling groups\n",
      "train 1\n",
      "train 2\n",
      "step3----Epoch 18/50  accuracy: 0.333 \n",
      "Sampling groups\n",
      "train 1\n",
      "train 2\n",
      "step3----Epoch 19/50  accuracy: 0.333 \n",
      "Sampling groups\n",
      "train 1\n",
      "train 2\n",
      "step3----Epoch 20/50  accuracy: 0.333 \n"
     ]
    }
   ],
   "source": [
    "#-------------------training for step 3-------------------\n",
    "optimizer_g_h=torch.optim.Adam(model.parameters(),lr=0.002)\n",
    "optimizer_d=torch.optim.Adam(discriminator.parameters(),lr=0.002)\n",
    "\n",
    "\n",
    "for epoch in range(20):\n",
    "    #---training g and h , DCD is frozen\n",
    "\n",
    "    groups, groups_y = sample_groups(X_s,Y_s,X_t,Y_t,shot=4,seed=200+epoch)\n",
    "    G1, G2, G3, G4 = groups\n",
    "    Y1, Y2, Y3, Y4 = groups_y\n",
    "    groups_2 = [G2, G4]\n",
    "    groups_y_2 = [Y2, Y4]\n",
    "\n",
    "    n_iters = 2 * len(G2)\n",
    "    index_list = torch.randperm(n_iters)\n",
    "\n",
    "    n_iters_dcd = 4 * len(G2)\n",
    "    index_list_dcd = torch.randperm(n_iters_dcd)\n",
    "\n",
    "    mini_batch_size_g_h = 16 #data only contains G2 and G4 ,so decrease mini_batch\n",
    "    mini_batch_size_dcd= 32 #data contains G1,G2,G3,G4 so use 40 as mini_batch\n",
    "    X1 = []\n",
    "    X2 = []\n",
    "    ground_truths_y1 = []\n",
    "    ground_truths_y2 = []\n",
    "    dcd_labels=[]\n",
    "    for index in range(n_iters):\n",
    "        ground_truth=index_list[index]//len(G2)\n",
    "        x1, x2 = groups_2[ground_truth][index_list[index] - len(G2) * ground_truth]\n",
    "        y1, y2 = groups_y_2[ground_truth][index_list[index] - len(G2) * ground_truth]\n",
    "        # y1=torch.LongTensor([y1.item()])\n",
    "        # y2=torch.LongTensor([y2.item()])\n",
    "        dcd_label=0 if ground_truth==0 else 2\n",
    "        X1.append(x1)\n",
    "        X2.append(x2)\n",
    "        ground_truths_y1.append(y1)\n",
    "        ground_truths_y2.append(y2)\n",
    "        dcd_labels.append(dcd_label)\n",
    "\n",
    "        if (index+1)%mini_batch_size_g_h==0:\n",
    "            print(\"train 1\")\n",
    "            X1=torch.stack(X1)\n",
    "            X2=torch.stack(X2)\n",
    "            ground_truths_y1=torch.LongTensor(ground_truths_y1)\n",
    "            ground_truths_y2 = torch.LongTensor(ground_truths_y2)\n",
    "            dcd_labels=torch.LongTensor(dcd_labels)\n",
    "            X1=X1.to(device)\n",
    "            X2=X2.to(device)\n",
    "            ground_truths_y1=ground_truths_y1.to(device)\n",
    "            ground_truths_y2 = ground_truths_y2.to(device)\n",
    "            dcd_labels=dcd_labels.to(device)\n",
    "\n",
    "            optimizer_g_h.zero_grad()\n",
    "\n",
    "            out1 = model(X1)\n",
    "            enc1 = encoder_output['fc2']\n",
    "            out2 = model(X2)\n",
    "            enc2 = encoder_output['fc2']\n",
    "\n",
    "            X_cat=torch.cat([enc1,enc2],1)\n",
    "            y_pred_X1=out1\n",
    "            y_pred_X2=out2\n",
    "            y_pred_dcd=discriminator(X_cat)\n",
    "\n",
    "            loss_X1=loss_fn(y_pred_X1,ground_truths_y1)\n",
    "            loss_X2=loss_fn(y_pred_X2,ground_truths_y2)\n",
    "            loss_dcd=loss_fn(y_pred_dcd,dcd_labels)\n",
    "\n",
    "            loss_sum = loss_X1 + loss_X2 + 0.2 * loss_dcd\n",
    "\n",
    "            loss_sum.backward()\n",
    "            optimizer_g_h.step()\n",
    "\n",
    "            X1 = []\n",
    "            X2 = []\n",
    "            ground_truths_y1 = []\n",
    "            ground_truths_y2 = []\n",
    "            dcd_labels = []\n",
    "\n",
    "\n",
    "    #----training dcd ,g and h frozen\n",
    "    X1 = []\n",
    "    X2 = []\n",
    "    ground_truths = []\n",
    "    for index in range(n_iters_dcd):\n",
    "\n",
    "        ground_truth=index_list_dcd[index]//len(groups[1])\n",
    "\n",
    "        x1, x2 = groups[ground_truth][index_list_dcd[index] - len(groups[1]) * ground_truth]\n",
    "        X1.append(x1)\n",
    "        X2.append(x2)\n",
    "        ground_truths.append(ground_truth)\n",
    "\n",
    "        if (index + 1) % mini_batch_size_dcd == 0:\n",
    "            print(\"train 2\")\n",
    "            X1 = torch.stack(X1)\n",
    "            X2 = torch.stack(X2)\n",
    "            ground_truths = torch.LongTensor(ground_truths)\n",
    "            X1 = X1.to(device)\n",
    "            X2 = X2.to(device)\n",
    "            ground_truths = ground_truths.to(device)\n",
    "\n",
    "            optimizer_d.zero_grad()\n",
    "\n",
    "            out1 = model(X1)\n",
    "            enc1 = encoder_output['fc2']\n",
    "            out2 = model(X2)\n",
    "            enc2 = encoder_output['fc2']\n",
    "            X_cat = torch.cat([enc1, enc2], 1)\n",
    "            y_pred = discriminator(X_cat.detach())\n",
    "            loss = loss_fn(y_pred, ground_truths)\n",
    "            loss.backward()\n",
    "            optimizer_d.step()\n",
    "            # loss_mean.append(loss.item())\n",
    "            X1 = []\n",
    "            X2 = []\n",
    "            ground_truths = []\n",
    "\n",
    "    #testing\n",
    "    acc = 0\n",
    "    for data, labels,names in test_dataloader:\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        y_test_pred = model(data)\n",
    "        acc += (torch.max(y_test_pred, 1)[1] == labels).float().mean().item()\n",
    "\n",
    "    accuracy = round(acc / float(len(test_dataloader)), 3)\n",
    "\n",
    "    print(\"step3----Epoch %d/%d  accuracy: %.3f \" % (epoch + 1, 50, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46e47828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Datasets/office5_both/test\n",
      "{'pen_camera': 0, 'keyboard_dataset': 1, 'backpack_camera': 2, 'laptop_dataset': 3, 'mug_dataset': 4, 'pen_dataset': 5, 'laptop_camera': 6, 'keyboard_camera': 7, 'backpack_dataset': 8, 'mug_camera': 9}\n",
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
     ]
    }
   ],
   "source": [
    "# grab the test batch\n",
    "test_batch_path = \"../Datasets/office5_both/\"\n",
    "#args = Args(act_mode_8bit=True)\n",
    "_, test_set = cats_and_dogs_get_datasets((test_batch_path, args), load_train=False, load_test=True)\n",
    "test_loader = DataLoader(test_set, batch_size=128, shuffle=True)\n",
    "\n",
    "# get a batch of data\n",
    "(batch_imgs, batch_labels, names) = next(iter(test_loader))\n",
    "batch_imgs,batch_labels = batch_imgs.to(device), batch_labels.to(device)\n",
    "\n",
    "# register forward hook to get embedding output\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# get the activations\n",
    "model.eval()\n",
    "model.feature_extractor.fc1.register_forward_hook(get_activation('fc1'))\n",
    "preds = model(batch_imgs)\n",
    "embds = activation['fc1']\n",
    "\n",
    "# get the labels\n",
    "batch_label_strings = []\n",
    "for idx,label in enumerate(batch_labels):\n",
    "    if label.item() == 0:\n",
    "        batch_label_strings.append(\"pen_camera\")\n",
    "    elif label.item() == 1:\n",
    "        batch_label_strings.append(\"keyboard_dataset\")\n",
    "    elif label.item() == 2:\n",
    "        batch_label_strings.append(\"backpack_camera\")\n",
    "    elif label.item() == 3:\n",
    "        batch_label_strings.append(\"laptop_dataset\")\n",
    "    elif label.item() == 4:\n",
    "        batch_label_strings.append(\"mug_dataset\")\n",
    "    elif label.item() == 5:\n",
    "        batch_label_strings.append(\"pen_dataset\")\n",
    "    elif label.item() == 6:\n",
    "        batch_label_strings.append(\"laptop_camera\")\n",
    "    elif label.item() == 7:\n",
    "        batch_label_strings.append(\"keyboard_camera\")\n",
    "    elif label.item() == 8:\n",
    "        batch_label_strings.append(\"backpack_dataset\")\n",
    "    elif label.item() == 9:\n",
    "        batch_label_strings.append(\"mug_camera\")\n",
    "\n",
    "# log to tensorboard\n",
    "tflogger.tblogger.writer.add_embedding(\n",
    "        embds.to('cpu'),\n",
    "        #metadata=batch_names,\n",
    "        metadata=batch_label_strings,\n",
    "        label_img=batch_imgs.to('cpu'),\n",
    "        global_step=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "622a8f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Datasets/test_batch/test\n",
      "{'dogs': 0, 'cats': 1}\n",
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
     ]
    }
   ],
   "source": [
    "# grab the test batch\n",
    "test_batch_path = \"../Datasets/test_batch/\"\n",
    "#args = Args(act_mode_8bit=True)\n",
    "_, test_set = cats_and_dogs_get_datasets((test_batch_path, args), load_train=False, load_test=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "# get a batch of data\n",
    "(batch_imgs, batch_labels, names) = next(iter(test_loader))\n",
    "batch_imgs,batch_labels = batch_imgs.to(device), batch_labels.to(device)\n",
    "\n",
    "# register forward hook to get embedding output\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# get the activations\n",
    "model.eval()\n",
    "model.fc1.register_forward_hook(get_activation('fc1'))\n",
    "preds = model(batch_imgs)\n",
    "embds = activation['fc1']\n",
    "\n",
    "# get the labels\n",
    "batch_label_strings = []\n",
    "for idx,label in enumerate(batch_labels):\n",
    "    if label.item() == 0:\n",
    "        batch_label_strings.append(\"dog\")\n",
    "    elif label.item() == 1:\n",
    "        batch_label_strings.append(\"cat\")\n",
    "\n",
    "# log to tensorboard\n",
    "tflogger.tblogger.writer.add_embedding(\n",
    "        embds.to('cpu'),\n",
    "        #metadata=batch_names,\n",
    "        metadata=batch_label_strings,\n",
    "        label_img=batch_imgs.to('cpu'),\n",
    "        global_step=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "12b0e3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8125"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.argmax(preds.to('cpu').detach().numpy(),axis=1) == batch_labels.to('cpu').detach().numpy())/64"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai8x-training",
   "language": "python",
   "name": "ai8x-training"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
